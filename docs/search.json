[
  {
    "objectID": "lecture_code.html",
    "href": "lecture_code.html",
    "title": "Lecture Code",
    "section": "",
    "text": "Here will be a link to the raw code used in class to generate the slides.\nNote: The code is in a .qmd file which you can open in R-Studio. It is a file called “Quarto” that is very similar to R-Markdown and R-Notebooks. Dr. Haraden is just trying to be fancy\nPre-Lab\nHypothesis & Power\nCategorical Data\nCategorical Data Part 2"
  },
  {
    "objectID": "lectures/06_OneSample.html#last-week",
    "href": "lectures/06_OneSample.html#last-week",
    "title": "Categorical & One-Sample",
    "section": "Last week",
    "text": "Last week\nNHST & p-values\nLab 2…feedback?\nFrequency of Labs check-in\n\nlibrary(here)\nlibrary(tidyverse)\nlibrary(psych)\nlibrary(psychTools)\nlibrary(knitr)\nlibrary(kableExtra)\nlibrary(ggpubr)\nlibrary(patchwork)\n\nset.seed(42)"
  },
  {
    "objectID": "lectures/06_OneSample.html#this-week",
    "href": "lectures/06_OneSample.html#this-week",
    "title": "Categorical & One-Sample",
    "section": "This Week…",
    "text": "This Week…\n\nThe chi-square goodness-of-fit test\nOne-sample t-tests"
  },
  {
    "objectID": "lectures/06_OneSample.html#what-are-the-steps-of-nhst",
    "href": "lectures/06_OneSample.html#what-are-the-steps-of-nhst",
    "title": "Categorical & One-Sample",
    "section": "What are the steps of NHST?",
    "text": "What are the steps of NHST?\n\n\n\nDefine null and alternative hypothesis.\nSet and justify alpha level.\nDetermine which sampling distribution ( \\(z\\), \\(t\\), or \\(\\chi^2\\) for now)\nCalculate parameters of your sampling distribution under the null.\n\n\nIf \\(z\\), calculate \\(\\mu\\) and \\(\\sigma_M\\)\n\n\n\nCalculate test statistic under the null.\n\n\nIf \\(z\\), \\(\\frac{\\bar{X} - \\mu}{\\sigma_M}\\)\n\n\nCalculate probability of that test statistic or more extreme under the null, and compare to alpha."
  },
  {
    "objectID": "lectures/06_OneSample.html#example---coffee-shop",
    "href": "lectures/06_OneSample.html#example---coffee-shop",
    "title": "Categorical & One-Sample",
    "section": "Example - Coffee Shop",
    "text": "Example - Coffee Shop\nLet’s say we collect data on customers of a coffee shop and we want to see if there is an equal number of folks that come into the shop across all days. Therefore, we record how many individuals came into the coffee shop over a weeks time.\nHow would we test this?\n\nFirst, setup the Null and alternative:\n\n\\(H_0\\): Customers will be equal across all days.\n\\(H_1\\): There will be more customers on one or multiple days than others and will not be equal"
  },
  {
    "objectID": "lectures/06_OneSample.html#distribution---degrees-of-freedom",
    "href": "lectures/06_OneSample.html#distribution---degrees-of-freedom",
    "title": "Categorical & One-Sample",
    "section": "Distribution - Degrees of freedom",
    "text": "Distribution - Degrees of freedom\nThe \\(\\chi^2\\) distribution is a single-parameter distribution defined by it’s degrees of freedom.\nIn the case of a goodness-of-fit test (like this one), the degrees of freedom are \\(\\textbf{k-1}\\), where k is the number of groups."
  },
  {
    "objectID": "lectures/06_OneSample.html#degrees-of-freedom",
    "href": "lectures/06_OneSample.html#degrees-of-freedom",
    "title": "Categorical & One-Sample",
    "section": "Degrees of freedom",
    "text": "Degrees of freedom\nThe Degrees of freedom are the number of genuinely independent things in a calculation. It’s specifically calculated as the number of quantities in a calculation minus the number of constraints.\nWhat it means in principle is that given a set number of categories (k) and a constraint (the proportions have to add up to 1), I can freely choose numbers for k-1 categories. But for the kth category, there’s only one number that will work."
  },
  {
    "objectID": "lectures/06_OneSample.html#but-what-if",
    "href": "lectures/06_OneSample.html#but-what-if",
    "title": "Categorical & One-Sample",
    "section": "But what if…",
    "text": "But what if…\nIn the example, we had a null distribution that was distributed uniformly\nWhat if that isn’t a super interesting research question?\nInstead we may want to compare the proportions in our sample to a larger population"
  },
  {
    "objectID": "lectures/06_OneSample.html#example-2---schools-super-powers",
    "href": "lectures/06_OneSample.html#example-2---schools-super-powers",
    "title": "Categorical & One-Sample",
    "section": "Example 2 - Schools & Super-powers",
    "text": "Example 2 - Schools & Super-powers\nThe data were obtained from Census at School, a website developed by the American Statistical Association tohelp students in the 4th through 12th grades understand statistical problem-solving.\n\n\nThe site sponsors a survey that students can complete and a database that students and instructors can use to illustrate principles in quantitative methods.\nThe database includes students from all 50 states, from grade levels 4 through 12, both boys and girls, who have completed the survey dating back to 2010."
  },
  {
    "objectID": "lectures/06_OneSample.html#write-up-practice",
    "href": "lectures/06_OneSample.html#write-up-practice",
    "title": "Categorical & One-Sample",
    "section": "Write-up Practice",
    "text": "Write-up Practice\nOpen up a word document and provide a 2-6 sentence write up of the analyses that we just completed\nDon’t forget:\n\nBefore the test, there are some descriptives\nInformation about the the null hypothesis\nIncluding a “stats block”\nBrief interpretation of the results"
  },
  {
    "objectID": "lectures/06_OneSample.html#the-usefulness-of-chi2",
    "href": "lectures/06_OneSample.html#the-usefulness-of-chi2",
    "title": "Categorical & One-Sample",
    "section": "The usefulness of \\(\\chi^2\\)",
    "text": "The usefulness of \\(\\chi^2\\)\nHow often will you conducted a \\(chi^2\\) goodness of fit test on raw data?\n\n(Probably) never\n\nHow often will you come across \\(\\chi^2\\) tests?\n\n(Probably) a lot!\n\nThe goodness of fit test is used to statistically test the how well a model fits data."
  },
  {
    "objectID": "lectures/06_OneSample.html#model-fit-with-chi2",
    "href": "lectures/06_OneSample.html#model-fit-with-chi2",
    "title": "Categorical & One-Sample",
    "section": "Model Fit with \\(\\chi^2\\)",
    "text": "Model Fit with \\(\\chi^2\\)\nTo calculate Goodness of Fit of a model to data, you build a statistical model of the process as you believe it is in the world.\n\n\nexample: depression ~ age + parental history of depression\n\n\n\nThen you estimate each subject’s predicted/expected value based on your model.\nYou compare each subject’s predicted value to their actual value – the difference is called the residual ( \\(\\varepsilon\\) )."
  },
  {
    "objectID": "lectures/05_Hypothesis-Power.html#recap",
    "href": "lectures/05_Hypothesis-Power.html#recap",
    "title": "Wk 5 - Hypothesis & Power",
    "section": "Recap",
    "text": "Recap\n\nlibrary(tidyverse)\nlibrary(gganimate)\nlibrary(pwr)\nlibrary(ggpubr)\nlibrary(transformr)\n\n\nSample statistics are biased estimates of the population\nCan construct confidence intervals around our sample statistics\n\nWe use (so far) the normal distribution & the t-distribution\n\nUp Next…Hypothesis testing!"
  },
  {
    "objectID": "lectures/05_Hypothesis-Power.html#hypothesis",
    "href": "lectures/05_Hypothesis-Power.html#hypothesis",
    "title": "Wk 5 - Hypothesis & Power",
    "section": "Hypothesis",
    "text": "Hypothesis\nWhat is a hypothesis?\nIn statistics, a hypothesis is a statement about the population. It is usually a prediction that a parameter describing some characteristic of a variable takes a particular numerical value, or falls into a certain range of values."
  },
  {
    "objectID": "lectures/05_Hypothesis-Power.html#hypothesis-1",
    "href": "lectures/05_Hypothesis-Power.html#hypothesis-1",
    "title": "Wk 5 - Hypothesis & Power",
    "section": "Hypothesis",
    "text": "Hypothesis\nFor example, dogs are characterized by their ability to read humans’ social cues, but it is (was) unknown whether that skill is biologically prepared. I might hypothesize that when a human points to a hidden treat, puppies do not understand that social cue and their performance on a related task is at-chance. We would call this a research hypothesis.\nThis could be represented numerically as, as a statistical hypothesis:\n\\[\\text{Proportion}_{\\text{Correct Performance}} = .50\\]"
  },
  {
    "objectID": "lectures/05_Hypothesis-Power.html#the-null-hypothesis",
    "href": "lectures/05_Hypothesis-Power.html#the-null-hypothesis",
    "title": "Wk 5 - Hypothesis & Power",
    "section": "The null hypothesis",
    "text": "The null hypothesis\nIn Null Hypothesis Significance Testing, we… test a null hypothesis.\nA null hypothesis ( \\(H_0\\) ) is a statement of no effect. The research hypothesis states that there is no relationship between X and Y, or our intervention has no effect on the outcome.\n\nThe statistical hypothesis is either that the population parameter is a single value, like 0, or that a range, like 0 or smaller."
  },
  {
    "objectID": "lectures/05_Hypothesis-Power.html#the-alternative-hypothesis",
    "href": "lectures/05_Hypothesis-Power.html#the-alternative-hypothesis",
    "title": "Wk 5 - Hypothesis & Power",
    "section": "The alternative hypothesis",
    "text": "The alternative hypothesis\nAccording to probability theory, our sample space must cover all possible elementary events. Therefore, we create an alternative hypothesis ( \\(H_1\\) ) that is every possible event not represented by our null hypothesis.\n\n\n\\[H_0: \\mu = 4\\] \\[H_1: \\mu \\neq 4\\]\n\n\\[H_0: \\mu \\leq -7\\] \\[H_1: \\mu &gt; -7\\]"
  },
  {
    "objectID": "lectures/05_Hypothesis-Power.html#the-tortured-logic-of-nhst",
    "href": "lectures/05_Hypothesis-Power.html#the-tortured-logic-of-nhst",
    "title": "Wk 5 - Hypothesis & Power",
    "section": "The tortured logic of NHST",
    "text": "The tortured logic of NHST\nWe create two hypotheses, \\(H_0\\) and \\(H_1\\). Usually, we care about \\(H_1\\), not \\(H_0\\). In fact, what we really want to know is how likely \\(H_1\\), given our data.\n\\[P(H_1|Data)\\] Instead, we’re going to test our null hypothesis. Well, not really. We’re going to assume our null hypothesis is true, and test how likely we would be to get these data.\n\\[P(Data|H_0)\\]"
  },
  {
    "objectID": "lectures/05_Hypothesis-Power.html#example-1",
    "href": "lectures/05_Hypothesis-Power.html#example-1",
    "title": "Wk 5 - Hypothesis & Power",
    "section": "Example #1",
    "text": "Example #1\nConsider the example of puppies’ abilities to read human social cues.\nLet \\(\\Pi\\) be the probability the puppy chooses the correct cup that a person points to.\nIn a task with two choices, an at-chance performance is \\(\\Pi = .5\\). This can be the null hypothesis because if this is true, than puppies would make the correct choice as often as they would make an incorrect choice.\nNote that the null hypothesis changes depending on the situation and research question."
  },
  {
    "objectID": "lectures/05_Hypothesis-Power.html#example-1---hypotheses",
    "href": "lectures/05_Hypothesis-Power.html#example-1---hypotheses",
    "title": "Wk 5 - Hypothesis & Power",
    "section": "Example #1 - Hypotheses",
    "text": "Example #1 - Hypotheses\nAs a dog-lover, you’re skeptical that reading human social cues is purely learned, and you have an alternative hypothesis that puppies will perform well over chance, thus having a probability of success on any given task greater than .5.\n\\[H_0: \\Pi = .5\\] \\[H_1: \\Pi \\neq .5\\]"
  },
  {
    "objectID": "lectures/05_Hypothesis-Power.html#example-1-1",
    "href": "lectures/05_Hypothesis-Power.html#example-1-1",
    "title": "Wk 5 - Hypothesis & Power",
    "section": "Example #1",
    "text": "Example #1\nTo test the null hypothesis, you a single puppy and test them 12 times on a pointing task. The puppy makes the correct choice 10 times.\nThe question you’re going to ask is:\n\n\n“How likely is it that the puppy is successful 10 times out of 12, if the probability of success is .5?”\n\n\nThis is the essence of NHST.\nYou can already test this using what you know about the binomial distribution."
  },
  {
    "objectID": "lectures/05_Hypothesis-Power.html#complications-with-the-binomial",
    "href": "lectures/05_Hypothesis-Power.html#complications-with-the-binomial",
    "title": "Wk 5 - Hypothesis & Power",
    "section": "Complications with the binomial",
    "text": "Complications with the binomial\nThe likelihood of the puppy being successful 10 times out of 12 if the true probability of success is .5 is 0.02. That’s pretty low! That’s so low that we might begin to suspect that the true probability is not .5.\nBut there’s a problem with this example. The real study used a sample of many puppies (&gt;300), and the average number of correct trials per puppy was about 8.33. But the binomial won’t allow us to calculate the probability of fractional successes!\nWhat we really want is not to assess 10 out of 12 times, but a proportion, like .694. How many different proportions could result puppy to puppy?"
  },
  {
    "objectID": "lectures/05_Hypothesis-Power.html#our-statistic-is-usually-continuous",
    "href": "lectures/05_Hypothesis-Power.html#our-statistic-is-usually-continuous",
    "title": "Wk 5 - Hypothesis & Power",
    "section": "Our statistic is usually continuous",
    "text": "Our statistic is usually continuous\nWhen we estimate a statistic for our sample – like the proportion of puppy success, or the average IQ score, or the relationship between age in months and second attending to a new object – that statistic is nearly always continuous. So we have to assess the probability of that statistic using a probability distribution for continuous variables, like the normal distribution. (Or t, or F, or \\(\\chi^2\\) ).\nWhat is the probability of any value in a continuous distribution?"
  },
  {
    "objectID": "lectures/05_Hypothesis-Power.html#quick-recap",
    "href": "lectures/05_Hypothesis-Power.html#quick-recap",
    "title": "Wk 5 - Hypothesis & Power",
    "section": "Quick recap",
    "text": "Quick recap\nFor any NHST test, we:\n\nIdentify the null hypothesis ( \\(H_0\\) ), which is usually the opposite of what we think to be true.\nCollect data.\nDetermine how likely we are to get these data or more extreme if the null is true."
  },
  {
    "objectID": "lectures/05_Hypothesis-Power.html#enter-sampling-distributions",
    "href": "lectures/05_Hypothesis-Power.html#enter-sampling-distributions",
    "title": "Wk 5 - Hypothesis & Power",
    "section": "Enter sampling distributions",
    "text": "Enter sampling distributions\n\n\n\n\nCode\ndata.frame(trials = trial, d = dbinom(trial, size = 12, prob = .5), \n           color = ifelse(trial %in% c(0,1,2, 10,11,12), \"1\", \"2\")) %&gt;%\n  ggplot(aes(x = trials, y = d, fill = color)) +\n  geom_bar(stat = \"identity\") + \n  guides(fill = \"none\")+\n  scale_x_continuous(\"Number of successes\", breaks = c(0:12))+\n  scale_y_continuous(\"Probability of X successes\") +\n  theme(text = element_text(size = 20))\n\n\n\n\n\n\nWhen we were analyzing the puppy problem, we built the distribution under the null using the binomial.\nThis is our sampling distribution."
  },
  {
    "objectID": "lectures/05_Hypothesis-Power.html#example-2",
    "href": "lectures/05_Hypothesis-Power.html#example-2",
    "title": "Wk 5 - Hypothesis & Power",
    "section": "Example #2",
    "text": "Example #2\nBray and colleagues (2020) test a sample of 10* puppies on multiple cognitive tasks, including their ability to correctly find a treat hidden under one of two cups based on human pointing. The average success rate was 69.41% (SD = 18.88).\nHow do you generate the sampling distribution around the null?\n\nNull: distribution of successes – you know this population, trying to see if ratings of female applicants come from the same distribution of scores"
  },
  {
    "objectID": "lectures/05_Hypothesis-Power.html#a-p-value-does-not",
    "href": "lectures/05_Hypothesis-Power.html#a-p-value-does-not",
    "title": "Wk 5 - Hypothesis & Power",
    "section": "A p-value DOES NOT:",
    "text": "A p-value DOES NOT:\n\nTell you that the probability that the null hypothesis is true.\nProve that the alternative hypothesis is true.\nTell you anything about the size or magnitude of any observed difference in your data."
  },
  {
    "objectID": "lectures/05_Hypothesis-Power.html#p-values",
    "href": "lectures/05_Hypothesis-Power.html#p-values",
    "title": "Wk 5 - Hypothesis & Power",
    "section": "\\(p\\)-values",
    "text": "\\(p\\)-values\nFisher established (rather arbitrarily) the sanctity of the .05 and .01 significance levels during his work in agriculture, including work on the effectiveness of fertilizer. A common source of fertilizer is cow manure. Male cattle are called bulls.\nA common misinterpretation of the \\(p\\)-value ( \\(\\alpha\\) ) is that it is the probability of the null hypothesis being wrong.\nAnother common misunderstanding is that \\(1-\\alpha\\) is the probability that results will replicate."
  },
  {
    "objectID": "lectures/05_Hypothesis-Power.html#p-values-1",
    "href": "lectures/05_Hypothesis-Power.html#p-values-1",
    "title": "Wk 5 - Hypothesis & Power",
    "section": "\\(p\\)-values",
    "text": "\\(p\\)-values\n\nIn most research, the probability that the null hypothesis is true is very small.\nIf the null hypothesis is false, then the only mistake to be made is a failure to detect a real effect."
  },
  {
    "objectID": "lectures/05_Hypothesis-Power.html#errors",
    "href": "lectures/05_Hypothesis-Power.html#errors",
    "title": "Wk 5 - Hypothesis & Power",
    "section": "Errors",
    "text": "Errors\nIn hypothesis testing, we can make two kinds of errors.\n\n\n\n\nReject \\(H_0\\)\nDo not reject\n\n\n\n\n\\(H_0\\) True\nType I Error\nCorrect decision\n\n\n\\(H_0\\) False\nCorrect decision\nType II Error\n\n\n\nFalsely rejecting the null hypothesis is a Type I error. Traditionally, this has been viewed as particularly important to control at a low level (akin to avoiding false conviction of an innocent defendant)."
  },
  {
    "objectID": "lectures/05_Hypothesis-Power.html#errors-1",
    "href": "lectures/05_Hypothesis-Power.html#errors-1",
    "title": "Wk 5 - Hypothesis & Power",
    "section": "Errors",
    "text": "Errors\nIn hypothesis testing, we can make two kinds of errors.\n\n\n\n\nReject \\(H_0\\)\nDo not reject\n\n\n\n\n\\(H_0\\) True\nType I Error\nCorrect decision\n\n\n\\(H_0\\) False\nCorrect decision\nType II Error\n\n\n\nFailing to reject the null hypothesis when it is false is a Type II error. This is sometimes viewed as a failure in signal detection."
  },
  {
    "objectID": "lectures/05_Hypothesis-Power.html#errors-2",
    "href": "lectures/05_Hypothesis-Power.html#errors-2",
    "title": "Wk 5 - Hypothesis & Power",
    "section": "Errors",
    "text": "Errors\nIn hypothesis testing, we can make two kinds of errors.\n\n\n\n\nReject \\(H_0\\)\nDo not reject\n\n\n\n\n\\(H_0\\) True\nType I Error\nCorrect decision\n\n\n\\(H_0\\) False\nCorrect decision\nType II Error\n\n\n\nNull hypothesis testing is designed to make it easy to control Type I errors. We set a minimum proportion of such errors that we would be willing to tolerate in the long run. This is the significance level ( \\(\\alpha\\) ). By tradition this is no greater than .05."
  },
  {
    "objectID": "lectures/05_Hypothesis-Power.html#errors-3",
    "href": "lectures/05_Hypothesis-Power.html#errors-3",
    "title": "Wk 5 - Hypothesis & Power",
    "section": "Errors",
    "text": "Errors\nIn hypothesis testing, we can make two kinds of errors.\n\n\n\n\nReject \\(H_0\\)\nDo not reject\n\n\n\n\n\\(H_0\\) True\nType I Error\nCorrect decision\n\n\n\\(H_0\\) False\nCorrect decision\nType II Error\n\n\n\nControlling Type II errors is more challenging because it depends on several factors. But, we usually DO want to control these errors. Some argue that the null hypothesis is usually false, so the only error we can make is a Type II error – a failure to detect a signal that is present. Power is the probability of correctly rejecting a false null hypothesis."
  },
  {
    "objectID": "lectures/05_Hypothesis-Power.html#example-errors",
    "href": "lectures/05_Hypothesis-Power.html#example-errors",
    "title": "Wk 5 - Hypothesis & Power",
    "section": "Example Errors",
    "text": "Example Errors\n\nIn the long run, if psychology samples have a mean of 110 (\\(\\sigma\\) = 20, \\(N\\) = 25), we will correctly reject the null with probability of 0.71 (power; 1 - \\(\\beta\\)). We will incorrectly fail to reject the null with probability of 0.29 ( \\(\\beta\\) )\n\n\n\n\n\n\n\n\nReject \\(H_0\\)\nDo not reject\n\n\n\n\n\\(H_0\\) True\nType I Error \\(\\alpha\\) = 0.05\nCorrect decision\n\n\n\\(H_0\\) False\nCorrect decision\nType II Error \\(\\beta\\) = 0.29"
  },
  {
    "objectID": "lectures/05_Hypothesis-Power.html#what-if-these-factors-change",
    "href": "lectures/05_Hypothesis-Power.html#what-if-these-factors-change",
    "title": "Wk 5 - Hypothesis & Power",
    "section": "What if these factors change?",
    "text": "What if these factors change?\n\n\nSample size ( \\(N\\) )\nEffect size ( Right now it is difference between means )\n\nSignificance level ( \\(\\alpha\\) )\nPower ( \\(\\beta\\) )"
  },
  {
    "objectID": "lectures/05_Hypothesis-Power.html#calculating-power-in-r",
    "href": "lectures/05_Hypothesis-Power.html#calculating-power-in-r",
    "title": "Wk 5 - Hypothesis & Power",
    "section": "Calculating Power in R",
    "text": "Calculating Power in R\n\n\n\nlibrary(pwr)\n\nWe will use the pwr package and a few tutorials you can look at are as follows:\n\n\nReproducible Medical Research with R\nStat Methods\nVignettes from pwr package\n\n\n\nThe four components are interrelated and by knowing three, we can determine the fourth:\n\n\nSample Size\nEffect Size\nSignificance Level \\(\\alpha\\)\nPower \\(\\beta\\)"
  },
  {
    "objectID": "lectures/05_Hypothesis-Power.html#summary",
    "href": "lectures/05_Hypothesis-Power.html#summary",
    "title": "Wk 5 - Hypothesis & Power",
    "section": "Summary",
    "text": "Summary\n\nConducting a study we tend to have null \\(H_0\\) and alternative \\(H_1\\) hypotheses\nTested through Null Hypothesis Significance Testing\n\\(p-values\\) are the probability of getting this score or higher if the null distribution were true\nImportant to consider power in all studies we do"
  },
  {
    "objectID": "lectures/05_Hypothesis-Power.html#reminders",
    "href": "lectures/05_Hypothesis-Power.html#reminders",
    "title": "Wk 5 - Hypothesis & Power",
    "section": "Reminders",
    "text": "Reminders\n\nLab 2 is due at 11:59pm on Sunday (10/1)"
  },
  {
    "objectID": "lectures/05_Hypothesis-Power.html#additional-slides",
    "href": "lectures/05_Hypothesis-Power.html#additional-slides",
    "title": "Wk 5 - Hypothesis & Power",
    "section": "Additional Slides",
    "text": "Additional Slides"
  },
  {
    "objectID": "lectures/05_Hypothesis-Power.html#alpha",
    "href": "lectures/05_Hypothesis-Power.html#alpha",
    "title": "Wk 5 - Hypothesis & Power",
    "section": "alpha",
    "text": "alpha\nHistorically, psychologists have chosen to set their \\(\\alpha\\) level at .05, meaning any p-value less than .05 is considered “statistically significant” or the null is rejected.\nThis means that, among the times we examine a relationship that is truly null, we will reject the null 1 in 20 times.\nSome have argued that this is not conservative enough and we should use \\(\\alpha &lt; .005\\) (Benjamin et al., 2018)."
  },
  {
    "objectID": "lectures/05_Hypothesis-Power.html#check-in-and-review",
    "href": "lectures/05_Hypothesis-Power.html#check-in-and-review",
    "title": "Wk 5 - Hypothesis & Power",
    "section": "Check-in and Review",
    "text": "Check-in and Review\n\nThe null hypothesis ( \\(H_0\\) ) is a claim about the particular value that a population parameter takes.\nThe alternative hypothesis ( \\(H_1\\) ) states an alternative range of values for the population parameter.\nWe test the null hypothesis by determining if we have sufficient evidence that contradicts or nullifies it.\nWe reject the null hypothesis if the data in hand are rare, unusual, or atypical if the null were true. The alternative hypothesis gains support when the null is rejected, but \\(H_1\\) is not proven."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "PSYC 640",
    "section": "",
    "text": "This is a website for PSYC 640 - Introduction to Graduate Statistics at Rochester Institute of Technology.\nDr. Haraden is trying to put materials up here to share with everyone."
  },
  {
    "objectID": "lectures/05_Pre-Lab.html#lessons-from-lab-1",
    "href": "lectures/05_Pre-Lab.html#lessons-from-lab-1",
    "title": "Wk 5 - Hypothesis & Power",
    "section": "Lessons from Lab 1",
    "text": "Lessons from Lab 1\n\nGetting data into R is surprisingly hard\nThe console doesn’t come with you\nWork together\nProfessor gets too excited about R\n\n\nlibrary(tidyverse) #plotting\nlibrary(ggpubr) #prettier figures"
  },
  {
    "objectID": "lectures/05_Pre-Lab.html#sampling-revisited",
    "href": "lectures/05_Pre-Lab.html#sampling-revisited",
    "title": "Wk 5 - Hypothesis & Power",
    "section": "Sampling Revisited",
    "text": "Sampling Revisited\nWe use features of the sample (statistics) to inform us about features of the population (parameters). The quality of this information goes up as sample size goes up – the Law of Large Numbers. The quality of this information is easier to defend with random samples.\nAll sample statistics are wrong (they do not match the population parameters exactly) but they become more useful (better matches) as sample size increases."
  },
  {
    "objectID": "lectures/05_Pre-Lab.html#some-terminology",
    "href": "lectures/05_Pre-Lab.html#some-terminology",
    "title": "Wk 5 - Hypothesis & Power",
    "section": "Some Terminology",
    "text": "Some Terminology\n\n\n\n\n\n\n\nPopulation\nSample\n\n\n\n\n\\(\\mu\\) (mu) = Population Mean\n\\(\\bar{X}\\) (x bar) = Sample Mean\n\n\n\\(\\sigma\\) (sigma) = Population Standard Deviation\n\\(s\\) = \\(\\hat{\\sigma}\\) = Sample Standard Deviation\n\n\n\\(\\sigma^2\\) (sigma squared) = Population Variance\n\\(s^2\\) = \\(\\hat{\\sigma^2}\\) = Sample Variance"
  },
  {
    "objectID": "lectures/06_OneSample2.html#last-week",
    "href": "lectures/06_OneSample2.html#last-week",
    "title": "Categorical Data & Comparing Means",
    "section": "Last week",
    "text": "Last week\n\n\nReview of the NHST\nCategorical Data analysis with the \\(\\chi^2\\) distribution\n\nGoodness of Fit test\n\n\n\n\n# File management\nlibrary(here)\n# for dplyr, ggplot\nlibrary(tidyverse)\n# Descriptives\nlibrary(psych)\n# Making things look nice\nlibrary(knitr)\n# Presenting nice tables\nlibrary(kableExtra)\n# Making things look nice\nlibrary(ggpubr)\n# animate things\nlibrary(gganimate)"
  },
  {
    "objectID": "lectures/06_OneSample2.html#today",
    "href": "lectures/06_OneSample2.html#today",
    "title": "Categorical Data & Comparing Means",
    "section": "Today…",
    "text": "Today…\n\nThe chi-square test of independence (Book Chapter 12.2)\nReview Assumptions of chi-square test\nIntroduction to Comparing Means"
  },
  {
    "objectID": "lectures/06_OneSample2.html#pop-quizjk",
    "href": "lectures/06_OneSample2.html#pop-quizjk",
    "title": "Categorical Data & Comparing Means",
    "section": "Pop Quiz…jk",
    "text": "Pop Quiz…jk\n\nWhat do we mean when we say a study was powered to an effect of 0.34?\nWhat does a p-value tell us?\n\nScientists get it wrong"
  },
  {
    "objectID": "lectures/06_OneSample2.html#chapek-9-data",
    "href": "lectures/06_OneSample2.html#chapek-9-data",
    "title": "Categorical Data & Comparing Means",
    "section": "Chapek 9 Data",
    "text": "Chapek 9 Data\nTake a peek at the data:\n\nhead(chapek9)\n\n  species choice\n1   robot flower\n2   human   data\n3   human   data\n4   human   data\n5   robot   data\n6   human flower\n\n# or \n#glimpse(chapek9)"
  },
  {
    "objectID": "lectures/06_OneSample2.html#constructing-hypotheses",
    "href": "lectures/06_OneSample2.html#constructing-hypotheses",
    "title": "Categorical Data & Comparing Means",
    "section": "Constructing Hypotheses",
    "text": "Constructing Hypotheses\nResearch hypothesis states that “humans and robots answer the question in different ways”\nNow our notation has two subscript values?? What torture is this??"
  },
  {
    "objectID": "lectures/06_OneSample2.html#constructing-hypotheses-1",
    "href": "lectures/06_OneSample2.html#constructing-hypotheses-1",
    "title": "Categorical Data & Comparing Means",
    "section": "Constructing Hypotheses",
    "text": "Constructing Hypotheses\nOnce we have this established, we can take a look at the null\n\n\nClaiming now that the true choice probabilities don’t depend on the species making the choice ( \\(P_i\\) )\nHowever, we don’t know what the expected probability would be for each answer choice\n\nWe have to calculate the totals of each row/column"
  },
  {
    "objectID": "lectures/06_OneSample2.html#writing-it-up",
    "href": "lectures/06_OneSample2.html#writing-it-up",
    "title": "Categorical Data & Comparing Means",
    "section": "Writing it up",
    "text": "Writing it up\n\nPearson's \\(\\chi^2\\) revealed a significant association between species and choice ( \\(\\chi^2 (2) =\\) 10.7, \\(p\\) &lt; .01), such that robots appeared to be more likely to say that they prefer flowers, but the humans were more likely to say they prefer data."
  },
  {
    "objectID": "lectures/06_OneSample2.html#assumptions-of-the-test",
    "href": "lectures/06_OneSample2.html#assumptions-of-the-test",
    "title": "Categorical Data & Comparing Means",
    "section": "Assumptions of the test",
    "text": "Assumptions of the test\n\nThe expected frequencies are rather large\nData are independent of one another"
  },
  {
    "objectID": "lectures/06_OneSample2.html#comparing-means",
    "href": "lectures/06_OneSample2.html#comparing-means",
    "title": "Categorical Data & Comparing Means",
    "section": "Comparing Means",
    "text": "Comparing Means\nWhen we move from categorical outcomes to variables measured on an interval or ratio scale, we become interested in means rather than frequencies. Comparing means is usually done with the t-test, of which there are several forms.\nThe one-sample t-test is appropriate when a single sample mean is compared to a population mean but the population standard deviation is unknown. A sample estimate of the population standard deviation is used instead. The appropriate sampling distribution is the t-distribution, with N-1 degrees of freedom.\n\\[t_{df=N-1} = \\frac{\\bar{X}-\\mu}{\\frac{\\hat{\\sigma}}{\\sqrt{N}}}\\]\nThe heavier tails of the t-distribution, especially for small N, are the penalty we pay for having to estimate the population standard deviation from the sample."
  },
  {
    "objectID": "lectures/06_OneSample2.html#one-sample-t-tests",
    "href": "lectures/06_OneSample2.html#one-sample-t-tests",
    "title": "Categorical Data & Comparing Means",
    "section": "One-sample t-tests",
    "text": "One-sample t-tests\nt-tests were developed by William Sealy Gosset, who was a chemist studying the grains used in making beer. (He worked for Guinness.)\n\nSpecifically, he wanted to know whether particular strains of grain made better or worse beer than the standard.\nHe developed the t-test, to test small samples of beer against a population with an unknown standard deviation.\n\nProbably had input from Karl Pearson and Ronald Fisher\n\nPublished this as “Student” because Guinness didn’t want these tests tied to the production of beer."
  },
  {
    "objectID": "lectures/06_OneSample2.html#cohens-d",
    "href": "lectures/06_OneSample2.html#cohens-d",
    "title": "Categorical Data & Comparing Means",
    "section": "Cohen’s D",
    "text": "Cohen’s D\nCohen suggested one of the most common effect size estimates—the standardized mean difference—useful when comparing a group mean to a population mean or two group means to each other.\n\\[\\delta = \\frac{\\mu_1 - \\mu_0}{\\sigma} \\approx d = \\frac{\\bar{X}-\\mu}{\\hat{\\sigma}}\\]\nCohen’s d is in the standard deviation (Z) metric."
  },
  {
    "objectID": "lectures/06_OneSample2.html#next-time",
    "href": "lectures/06_OneSample2.html#next-time",
    "title": "Categorical Data & Comparing Means",
    "section": "Next time…",
    "text": "Next time…\nMore comparing means!"
  },
  {
    "objectID": "lectures/07_ComparingMeans.html#last-week",
    "href": "lectures/07_ComparingMeans.html#last-week",
    "title": "t-tests",
    "section": "Last week",
    "text": "Last week\n\n\nCategorical Data analysis with the \\(\\chi^2\\) distribution\n\nTest of Independence\nGoodness of Fit test\n\nSingle Sample \\(t\\)-test\n\n\n\n# File management\nlibrary(here)\n# for dplyr, ggplot\nlibrary(tidyverse)\n# Making things look nice\nlibrary(ggpubr)"
  },
  {
    "objectID": "lectures/07_ComparingMeans.html#today",
    "href": "lectures/07_ComparingMeans.html#today",
    "title": "t-tests",
    "section": "Today…",
    "text": "Today…\n\nComparing Means with the \\(t\\)-test\n\nIndependent samples\nPaired Samples"
  },
  {
    "objectID": "lectures/07_ComparingMeans.html#comparing-means",
    "href": "lectures/07_ComparingMeans.html#comparing-means",
    "title": "t-tests",
    "section": "Comparing Means",
    "text": "Comparing Means\nCalculated using a t-test. To calculate the t-statistic, you will use this formula:\n\\[t_{df=N-1} = \\frac{\\bar{X}-\\mu}{\\frac{\\hat{\\sigma}}{\\sqrt{N}}}\\]\nThe heavier tails of the t-distribution, especially for small N, are the penalty we pay for having to estimate the population standard deviation from the sample."
  },
  {
    "objectID": "lectures/07_ComparingMeans.html#one-sample-t-tests",
    "href": "lectures/07_ComparingMeans.html#one-sample-t-tests",
    "title": "t-tests",
    "section": "One-sample t-tests",
    "text": "One-sample t-tests\nt-tests were developed by William Sealy Gosset, who was a chemist studying the grains used in making beer. (He worked for Guinness.)\n\nSpecifically, he wanted to know whether particular strains of grain made better or worse beer than the standard.\nHe developed the t-test, to test small samples of beer against a population with an unknown standard deviation.\n\nProbably had input from Karl Pearson and Ronald Fisher\n\nPublished this as “Student” because Guinness didn’t want these tests tied to the production of beer."
  },
  {
    "objectID": "lectures/07_ComparingMeans.html#cohens-d",
    "href": "lectures/07_ComparingMeans.html#cohens-d",
    "title": "t-tests",
    "section": "Cohen’s D",
    "text": "Cohen’s D\nCohen suggested one of the most common effect size estimates—the standardized mean difference—useful when comparing a group mean to a population mean or two group means to each other.\n\\[\\delta = \\frac{\\mu_1 - \\mu_0}{\\sigma} \\approx d = \\frac{\\bar{X}-\\mu}{\\hat{\\sigma}}\\]\nCohen’s d is in the standard deviation (Z) metric."
  },
  {
    "objectID": "lectures/07_ComparingMeans.html#independent-samples-t-test",
    "href": "lectures/07_ComparingMeans.html#independent-samples-t-test",
    "title": "t-tests",
    "section": "Independent Samples t-Test",
    "text": "Independent Samples t-Test\n\nAssumptions:\n\nRandom sampling\nIndependence of observations\nApproximately normally distributed data\nHomogeneity of variances\n\nFormula: Explain the mathematical formula for the t-test.\nInterpretation of Results: Discuss how to interpret the t-statistic, degrees of freedom, and p-value.\nPractical Example: Walk through a hypothetical example of comparing test scores of two groups (e.g., a control group and an experimental group) in R."
  },
  {
    "objectID": "lectures/07_ComparingMeans.html#paired-samples-t-test",
    "href": "lectures/07_ComparingMeans.html#paired-samples-t-test",
    "title": "t-tests",
    "section": "Paired Samples t-Test",
    "text": "Paired Samples t-Test\n\nAssumptions:\n\nDifferences between paired observations are approximately normally distributed\nHomogeneity of differences\n\nFormula: Explain the formula for the paired samples t-test.\nInterpretation of Results: Discuss how to interpret the t-statistic, degrees of freedom, and p-value in the context of paired samples.\nPractical Example: Demonstrate a paired samples t-test using data where each observation is paired with another (e.g., before and after measurements) in R."
  },
  {
    "objectID": "lectures/07_ComparingMeans.html#normality-assumption",
    "href": "lectures/07_ComparingMeans.html#normality-assumption",
    "title": "t-tests",
    "section": "Normality Assumption",
    "text": "Normality Assumption\n\nHow to Check for Normality: Explain methods like histograms, Q-Q plots, and statistical tests (Shapiro-Wilk, Anderson-Darling) to assess normality.\nRemedies for Violations: Discuss options like data transformation or non-parametric alternatives when data is not normally distributed."
  },
  {
    "objectID": "lectures/07_ComparingMeans.html#homogeneity-of-variance",
    "href": "lectures/07_ComparingMeans.html#homogeneity-of-variance",
    "title": "t-tests",
    "section": "Homogeneity of Variance",
    "text": "Homogeneity of Variance\n\nLevene’s Test for Equality of Variances: Show how to use Levene’s test in R to assess if variances are equal between groups.\nRemedies for Violations: Explain techniques like Welch’s t-test for unequal variances."
  },
  {
    "objectID": "lectures/07_ComparingMeans.html#data-preparation",
    "href": "lectures/07_ComparingMeans.html#data-preparation",
    "title": "t-tests",
    "section": "Data Preparation",
    "text": "Data Preparation\n\nLoading Data: Use the read.csv() or similar functions to load datasets in R.\nData Exploration: Perform summary statistics, explore data distribution, and visualize data using plots (e.g., histograms, boxplots).\nData Visualization: Use ggplot2 or other packages for data visualization to get a sense of the data."
  },
  {
    "objectID": "lectures/07_ComparingMeans.html#independent-samples-t-test-in-r",
    "href": "lectures/07_ComparingMeans.html#independent-samples-t-test-in-r",
    "title": "t-tests",
    "section": "Independent Samples t-Test in R",
    "text": "Independent Samples t-Test in R\n\nUsing t.test() Function: Demonstrate how to use the t.test() function in R to conduct an independent samples t-test.\nInterpreting Output: Explain how to interpret the results of the t-test, including the t-statistic, degrees of freedom, and p-value.\nPractical Example in R: Walk through a real example using R, including data preparation, t-test, and result interpretation."
  },
  {
    "objectID": "lectures/07_ComparingMeans.html#paired-samples-t-test-in-r",
    "href": "lectures/07_ComparingMeans.html#paired-samples-t-test-in-r",
    "title": "t-tests",
    "section": "Paired Samples t-Test in R",
    "text": "Paired Samples t-Test in R\n\nUsing t.test() Function with Paired Data: Show how to use the t.test() function with paired data and explain the paired argument.\nInterpreting Output: Discuss how to interpret the results of the paired samples t-test.\nPractical Example in R: Provide a hands-on example with paired data, demonstrating data preparation, t-test, and result interpretation."
  },
  {
    "objectID": "lectures/07_ComparingMeans.html#next-time",
    "href": "lectures/07_ComparingMeans.html#next-time",
    "title": "t-tests",
    "section": "Next time…",
    "text": "Next time…"
  }
]