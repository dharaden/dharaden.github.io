{"title":"Categorical & One-Sample","markdown":{"yaml":{"title":"Categorical & One-Sample","subtitle":"PSYC 640 - Fall 2023","author":"Dustin Haraden, PhD","format":{"revealjs":{"multiplex":true,"slide-number":true,"incremental":true,"touch":true,"code-overflow":"wrap","theme":"night"}},"execute":{"echo":true},"editor":"visual"},"headingText":"Last week","containsRefs":false,"markdown":"\n\n```{r, include = F}\nknitr::opts_chunk$set(message = FALSE, warning = FALSE)\n```\n\n\nNHST & *p-*values\n\nLab 2...feedback?\n\nFrequency of Labs check-in\n\n```{r, results = 'hide', message = F, warning = F}\nlibrary(here)\nlibrary(tidyverse)\nlibrary(psych)\nlibrary(psychTools)\nlibrary(knitr)\nlibrary(kableExtra)\nlibrary(ggpubr)\nlibrary(patchwork)\n\nset.seed(42)\n```\n\n------------------------------------------------------------------------\n\n## This Week...\n\n-   The chi-square goodness-of-fit test\n-   One-sample *t*-tests\n\n------------------------------------------------------------------------\n\n### Key questions:\n\n-   How do we know if category frequencies are consistent with null hypothesis expectations?\n\n-   How do we handle categories with very low frequencies?\n\n-   How do we compare one sample to a population mean?\n\n------------------------------------------------------------------------\n\n## What are the steps of NHST?\n\n::: columns\n::: {.column width=\"50%\"}\n1.  Define null and alternative hypothesis.\n\n2.  Set and justify alpha level.\n\n3.  Determine which sampling distribution ( $z$, $t$, or $\\chi^2$ for now)\n\n4.  Calculate parameters of your sampling distribution under the null.\n\n-   If $z$, calculate $\\mu$ and $\\sigma_M$\n:::\n\n::: {.column width=\"50%\"}\n5.  Calculate test statistic under the null.\n\n-   If $z$, $\\frac{\\bar{X} - \\mu}{\\sigma_M}$\n\n6.  Calculate probability of that test statistic or more extreme under the null, and compare to alpha.\n:::\n:::\n\n------------------------------------------------------------------------\n\nOne-sample tests compare your given sample with a \"known\" population.\n\nResearch question: does this sample come from this population?\n\n**Hypotheses**\n\n-   $H_0$: Yes, this sample comes from this population.\n\n-   $H_1$: No, this sample comes from a different population.\n\n------------------------------------------------------------------------\n\n## Example - Coffee Shop\n\nLet's say we collect data on customers of a coffee shop and we want to see if there is an **equal number of folks** that come into the shop across all days. Therefore, we record how many individuals came into the coffee shop over a weeks time.\n\n***How would we test this?***\n\n-   First, setup the Null and alternative:\n\n    -   $H_0$: Customers will be equal across all days.\n\n    -   $H_1$: There will be more customers on one or multiple days than others and will not be equal\n\n------------------------------------------------------------------------\n\n### Example - Distribution\n\n```{r}\n#| code-fold: true\ndays <- c(\"M\", \"T\", \"W\", \"R\", \"F\")\nexpected <- rep(50, length(days))\ndf <- data.frame(days, expected)\norder <- c(\"M\", \"T\", \"W\", \"R\", \"F\")\n\nunif <- df %>% \n  ggplot(aes(x = days,\n             y = expected)) + \n  geom_col() + \n  scale_x_discrete(limits = order) + \n  labs(title = \"Uniform - Null\") + \n  ylim(0, 65)\n\nobserved <- round(rnorm(5, mean = 55, sd = 4),2)\ndf2 <- data.frame(days, observed)\n\nrand <- df %>% \n  ggplot(aes(x = days, \n             y = observed)) + \n  geom_col() + \n  scale_x_discrete(limits = order) + \n  labs(title = \"Alternative\") + \n  ylim(0, 65)\n\n(unif + rand)\n```\n\n------------------------------------------------------------------------\n\n### Example - Set Alpha\n\nAfter determining the Null and Alternative Hypotheses, we set our Alpha level.\n\nLet's keep things simple and keep it at convention to set it for $\\alpha$ = 0.05\n\n------------------------------------------------------------------------\n\n### Example - Distribution\n\nNow we determine the type of distribution that we will be working with\n\nIn the past we have used:\n\n-   Normal Distribution ( $z$-scores )\n\n-   t-distribution ( $t$-scores )\n\n------------------------------------------------------------------------\n\nNow we are going to be using a distribution that works with ***categorical (nominal)*** **data**.\n\n**The** $\\chi^2$ **- distribution**\n\n::: nonincremental\n-   We use this distribution because we are dealing with (1) one sample, and (2) a categorical outcome\n\n-   **Note**: this test will provide statistical evidence of an association or relationship between two categorical variables\n:::\n\n***???*** The way you measure the variable determines whether it is categorical or continuous. We can create summary statistics from categorical variables by counting or calculating proportions -- but that makes the summary statistics continuous, *not the outcome variable itself*.\n\n------------------------------------------------------------------------\n\n## Distribution - Degrees of freedom\n\nThe $\\chi^2$ distribution is a single-parameter distribution defined by it's degrees of freedom.\n\nIn the case of a **goodness-of-fit test** (like this one), the degrees of freedom are $\\textbf{k-1}$, where k is the number of groups.\n\n------------------------------------------------------------------------\n\n## Degrees of freedom\n\nThe **Degrees of freedom** are the number of genuinely independent things in a calculation. It's specifically calculated as the number of quantities in a calculation minus the number of constraints.\n\nWhat it means in principle is that given a set number of categories (k) and a constraint (the proportions have to add up to 1), I can freely choose numbers for k-1 categories. But for the kth category, there's only one number that will work.\n\n------------------------------------------------------------------------\n\n::: columns\n::: {.column width=\"30%\"}\n[The degrees of freedom are the number of categories (k) minus 1. Given that the category frequencies must sum to the total sample size, k-1 category frequencies are free to vary; the last is determined.]{style=\"font-size:30px;\"}\n:::\n\n::: {.column width=\"70%\"}\n```{r}\n(critical_val = qchisq(p = 0.95, df = 5-1))\n```\n\n```{r}\n#| code-fold: true\n#| \n\ndata.frame(x = seq(0,20)) %>%\n  ggplot(aes(x = x)) +\n  stat_function(fun = function(x) dchisq(x, df = 5-1), geom = \"line\") +\n  stat_function(fun = function(x) dchisq(x, df = 5-1), geom = \"area\", fill = \"purple\", \n                xlim =c(critical_val, 20)) +\n  geom_vline(aes(xintercept = critical_val), linetype = 2, color = \"purple\")+\n  geom_text(aes(x = critical_val+2, y = dchisq(critical_val, 5-1) + .05, \n                label = paste(\"Critical Value =\", round (critical_val,2))), angle = 90)+\n  labs(x = \"Chi-Square\", y = \"Density\", title = \"Area under the curve\") +\n  theme_pubr(base_size = 20)\n```\n:::\n:::\n\n------------------------------------------------------------------------\n\n### Calculating the $\\chi^2$ test statistic\n\nLet's first take a look at the observed data that we have as well as the expected data under the Null\n\n```{r}\nobserved <- df2\nexpected <- df\n```\n\n|              |       |                   |                   |                   |                   |                   |\n|-----------|-----------|-----------|-----------|-----------|-----------|-----------|\n|              |       | **Monday**        | **Tuesday**       | **Wednesday**     | **Thursday**      | **Friday**        |\n| **Observed** | $O_i$ | `r observed[1,2]` | `r observed[2,2]` | `r observed[3,2]` | `r observed[4,2]` | `r observed[5,2]` |\n| **Expected** | $E_i$ | `r expected[1,2]` | `r expected[2,2]` | `r expected[3,2]` | `r expected[4,2]` | `r expected[5,2]` |\n\nNow what? We need some way to index differences between these frequencies so that we can sensibly determine how rare or unusual the observed data are compared to the null distribution.\n\n------------------------------------------------------------------------\n\n$$\\chi^2_{df = k-1} = \\sum^k_{i=1}\\frac{(O_i-E_i)^2}{E_i}$$\n\nThe chi-square goodness of fit (GOF) statistic compares observed and expected frequencies. It is small when the observed frequencies closely match the expected frequencies under the null hypothesis. The chi-square distribution can be used to determine the particular $\\chi^2$ value that corresponds to a rare or unusual profile of observed frequencies.\n\n------------------------------------------------------------------------\n\n|                |             |                                   |                                   |                                   |                                   |                                   |\n|-----------|-----------|-----------|-----------|-----------|-----------|-----------|\n|                |             | **Monday**                        | **Tuesday**                       | **Wednesday**                     | **Thursday**                      | **Friday**                        |\n| **Observed**   | $O_i$       | `r observed[1,2]`                 | `r observed[2,2]`                 | `r observed[3,2]`                 | `r observed[4,2]`                 | `r observed[5,2]`                 |\n| **Expected**   | $E_i$       | `r expected[1,2]`                 | `r expected[2,2]`                 | `r expected[3,2]`                 | `r expected[4,2]`                 | `r expected[5,2]`                 |\n| **Difference** | $O_i - E_i$ | `r observed[1,2] - expected[1,2]` | `r observed[2,2] - expected[2,2]` | `r observed[3,2] - expected[3,2]` | `r observed[4,2] - expected[4,2]` | `r observed[5,2] - expected[5,2]` |\n\n------------------------------------------------------------------------\n\n```{r, results = 'asis', message=F}\n#| code-fold: true\nchisq_tab <- full_join(expected, observed) %>% \n  mutate(diff = (observed - expected))\nchisq_tab %>% \n  kable(., format = \"html\", digits = 2, \n        align = c(\"c\", \"c\", \"c\", \"c\"), \n        col.names = c(\"Days\", \"Expected\", \"Observed\", \"Difference\"))\n\n```\n\n------------------------------------------------------------------------\n\nWe now have the differences between the expected and observed, but there are likely going to be some negative numbers. To fix this, we can square the differences\n\n```{r, results = 'asis', message=F}\n#| code-fold: true\nchisq_tab %>% \n  mutate(sq_diff=(diff^2)) %>% \n  kable(., format = \"html\", digits = 2, \n        align = c(\"c\", \"c\", \"c\", \"c\", \"c\"), \n        col.names = c(\"Days\", \"Expected\", \"Observed\", \"Difference\", \"Sq. Diff\"))\n```\n\n------------------------------------------------------------------------\n\n::: columns\n::: {.column width=\"50%\"}\nNow we have a collection of numbers that are large whenever the null hypothesis makes a bad prediction and small when it makes a good prediction.\n\nNext, we need to divide all numbers by the expected frequency as a way to put our estimate into perspective\n:::\n\n::: {.column width=\"50%\"}\n```{r, results = 'asis', message=F}\n#| code-fold: true\nchisq_tab <- chisq_tab %>% \n  mutate(sq_diff=(diff^2)) %>%\n  mutate(error = sq_diff/50) \n\nchisq_tab %>% \n  kable(., format = \"html\", digits = 2, \n        align = c(\"c\", \"c\", \"c\", \"c\", \"c\", \"c\"), \n        col.names = c(\"Days\", \"Expected\", \"Observed\", \"Difference\", \"Sq. Diff\", \"Error Score\"))\n\n```\n:::\n:::\n\n------------------------------------------------------------------------\n\nWe can finish this off by taking each of our scores related to \"error\" and adding them up\n\nThe result is the **goodness of fit** statistic (GOF) or $\\chi^2$\n\n::: columns\n::: {.column width=\"50%\"}\n```{r}\nchi_square <- sum(chisq_tab$error)\nchi_square\n```\n\nLet's compare that to the critical $\\chi^2$ value given the df of the sample\n\n```{r}\ncritical_val <- qchisq(p = 0.95, df = length(chisq_tab$days)-1)\ncritical_val\n```\n:::\n\n::: {.column width=\"50%\"}\nCalculate the probability of getting our sample statistic or greater ***if the null were true***\n\n```{r}\np_val <- pchisq(q = chi_square, df = length(chisq_tab$days)-1, lower.tail = F)\np_val\n```\n\n**What can we conclude?**\n:::\n:::\n\n------------------------------------------------------------------------\n\n::: columns\n::: {.column width=\"30%\"}\n[The degrees of freedom are the number of categories (k) minus 1. Given that the category frequencies must sum to the total sample size, k-1 category frequencies are free to vary; the last is determined.]{style=\"font-size:30px;\"}\n:::\n\n::: {.column width=\"70%\"}\n```{r}\n#| code-fold: true\n#| \ndata.frame(x = seq(0,20)) %>%\n  ggplot(aes(x = x)) +\n  stat_function(fun = function(x) dchisq(x, df = length(chisq_tab$days)-1), geom = \"line\") +\n  stat_function(fun = function(x) dchisq(x, df = length(chisq_tab$days)-1), geom = \"area\", fill = \"purple\", \n                xlim =c(critical_val, 20)) +\n  geom_vline(aes(xintercept = critical_val), linetype = 2, color = \"purple\")+\n    geom_vline(aes(xintercept = chi_square), linetype = 2, color = \"black\")+\n  geom_text(aes(x = critical_val+2, y = dchisq(critical_val, length(chisq_tab$days)-1) + .05, \n                label = paste(\"Critical Value =\", round (critical_val,2))), angle = 90)+\n    geom_text(aes(x = chi_square+2, y = dchisq(critical_val, length(chisq_tab$days)-1) + .05, \n                label = paste(\"Test statistic =\", round (chi_square,2))), angle = 90)+\n  labs(x = \"Chi-Square\", y = \"Density\", title = \"Area under the curve\") +\n  theme_pubr(base_size = 20)\n```\n:::\n:::\n\n------------------------------------------------------------------------\n\n::: columns\n::: {.column width=\"30%\"}\n[The degrees of freedom are the number of categories (k) minus 1. Given that the category frequencies must sum to the total sample size, k-1 category frequencies are free to vary; the last is determined.]{style=\"font-size:30px;\"}\n:::\n\n::: {.column width=\"70%\"}\n```{r}\n#| code-fold: true\n#| \ndata.frame(x = seq(0,20)) %>%\n  ggplot(aes(x = x)) +\n  stat_function(fun = function(x) dchisq(x, df = length(chisq_tab$days)-1), geom = \"line\") +\n  stat_function(fun = function(x) dchisq(x, df = length(chisq_tab$days)-1), geom = \"area\", fill = \"purple\", \n                xlim =c(chi_square, 20)) +\n  geom_vline(aes(xintercept = critical_val), linetype = 2, color = \"black\")+\n  geom_vline(aes(xintercept = chi_square), linetype = 2, color = \"purple\")+\n  geom_text(aes(x = critical_val+2, y = dchisq(critical_val, length(chisq_tab$days)-1) + .05, \n                label = paste(\"Critical Value =\", round (critical_val,2))), angle = 90)+\n  geom_text(aes(x = chi_square+2, y = dchisq(critical_val, length(chisq_tab$days)-1) + .05, \n                label = paste(\"Test statistic =\", round (chi_square,2))), angle = 90)+\n  labs(x = \"Chi-Square\", y = \"Density\", title = \"Area under the curve\") +\n  theme_pubr(base_size = 20)\n```\n:::\n:::\n\n------------------------------------------------------------------------\n\n### Recap of the Steps\n\n1.  Define null and alternative hypothesis.\n\n-   $H_0$: No difference across days\n\n-   $H_1$: Days will be different\n\n2.  Set and justify alpha level $\\alpha$ = 0.05\n\n3.  Determine which sampling distribution ( $\\chi^2$ )\n\n4.  Calculate parameters of your sampling distribution under the null.\n\n-   Calculate $\\chi^2$-critical: `r critical_val`\n\n### Recap of the Steps (con't.)\n\n5.  Calculate test statistic under the null.\n\n-   $\\chi^2_{df = k-1} = \\sum^k_{i=1}\\frac{(O_i-E_i)^2}{E_i}$\n-   `r chi_square`\n\n6.  Calculate probability of that test statistic or more extreme under the null, and compare to alpha.\n\n    ```{r}\n    pchisq(q = chi_square, df = length(chisq_tab$days)-1, lower.tail = F)\n    ```\n\n------------------------------------------------------------------------\n\nNow that we know how to do the test using R, we now need to be able to ***write up the analyses*** that we just performed.\n\nIf we wanted to write these results from our example into a paper or something similar (maybe telling our pet who prefers scientific writing), we could do this:\n\n> Across all days of the average workweek, we observed, `r observed[1,2]` patrons on Monday, `r observed[2,2]` customers on Tuesday, `r observed[3,2]` on Wednesday, `r observed[4,2]` for Thursday, and finally `r observed[5,2]` patrons on Friday. A chi-square goodness of fit test was conducted to test whether these data followed a uniform distribution with 50 visits per day. The results of the test ( $\\chi^2$(4) = `r round(chi_square, 2)`, $p$ = `r round(p_val, 2)`) indicate no differences from the uniform distribution.\n\n------------------------------------------------------------------------\n\n> Across all days of the average workweek, we observed, `r observed[1,2]` patrons on Monday, `r observed[2,2]` customers on Tuesday, `r observed[3,2]` on Wednesday, `r observed[4,2]` for Thursday, and finally `r observed[5,2]` patrons on Friday. A chi-square goodness of fit test was conducted to test whether these data followed a uniform distribution with 50 visits per day. The results of the test ( $\\chi^2$(4) = `r round(chi_square, 2)`, $p$ = `r round(p_val, 2)`) indicate no differences from the uniform distribution.\n\nA couple things to notice about the write up:\n\n::: columns\n::: {.column width=\"50%\"}\n1.  Before the test, there are some descriptives\n\n2.  Informs you of the null hypothesis\n:::\n\n::: {.column width=\"50%\"}\n3.  Inclusion of a \"stats block\"\n4.  Results are interpreted\n:::\n:::\n\n## But what if...\n\nIn the example, we had a null distribution that was distributed uniformly\n\nWhat if that isn't a super interesting research question?\n\nInstead we may want to compare the proportions in our sample to a larger population\n\n## Example 2 - Schools & Super-powers\n\nThe data were obtained from [Census at School](https://ww2.amstat.org/censusatschool/), a website developed by the American Statistical Association tohelp students in the 4th through 12th grades understand statistical problem-solving.\n\n::: nonincremental\n-   The site sponsors a survey that students can complete and a database that students and instructors can use to illustrate principles in quantitative methods.\n\n-   The database includes students from all 50 states, from grade levels 4 through 12, both boys and girls, who have completed the survey dating back to 2010.\n:::\n\n------------------------------------------------------------------------\n\nLet's focus on a single question:\n\n*Which of the following superpowers would you most like to have? Select one.*\n\n::: nonincremental\n::: columns\n::: {.column width=\"50%\"}\n-   Invisibility\n-   Telepathy (read minds)\n-   Freeze time\n:::\n\n::: {.column width=\"50%\"}\n-   Super strength\n-   Fly\n:::\n:::\n:::\n\nThe responses from 250 randomly selected New York students were obtained from the Census at School database.\n\n```{r, message=FALSE, warning = F}\n\nschool <- read_csv(\"https://raw.githubusercontent.com/dharaden/dharaden.github.io/main/courses/PSYC640_fall23/data/example2-chisq.csv\") \nschool <- school %>% \n  filter(!is.na(Superpower))\n```\n\n------------------------------------------------------------------------\n\n::: columns\n::: {.column width=\"50%\"}\n```{r frequency table, results = 'asis'}\n#| code-fold: true\nschool %>%\n  group_by(Superpower) %>%\n  summarize(Frequency = n()) %>%\n  mutate(Proportion = Frequency/sum(Frequency)) %>%\n  kable(., \n        format = \"html\", \n        digits = 2) %>% \n  kable_classic(font_size = 25)\n```\n:::\n\n::: {.column width=\"50%\"}\nDescriptively this is interesting. But, are the responses unusual or atypical in any way? To answer that question, we need some basis for comparison---a null hypothesis <br> One option would be to ask if the New York preferences are different compared to students from the general population.\n:::\n:::\n\n------------------------------------------------------------------------\n\n```{r, message=FALSE, warning = F}\n#| code-fold: true\nschool_usa <-  read_csv(\"https://raw.githubusercontent.com/uopsych/psy611/master/data/census_at_school_usa.csv\")\n\nschool_usa$Region = \"USA\"\nschool %>%\n  full_join(select(school_usa, Region, Superpower)) %>%\n  filter(!is.na(Superpower)) %>%\n  group_by(Region, Superpower) %>%\n  summarize(Frequency = n()) %>%\n  mutate(Proportion = Frequency/sum(Frequency)) %>%\n  ggplot(aes(x = Region, y = Proportion, fill = Region)) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  coord_flip() +\n  labs(\n    x = NULL,\n    title = \"Category Proportion as a function of Source\") +\n  guides(fill = \"none\") +\n  facet_wrap(~Superpower) +\n  theme_bw(base_size = 20) + \n  theme(plot.title.position = \"plot\")\n```\n\n------------------------------------------------------------------------\n\n::: columns\n::: {.column width=\"50%\"}\n$H_0$: New York student superpower preferences are similar to the preferences of typical students in the United States.\n\n$H_1$: New York student superpower preferences are different from the preferences of typical students in the United States.\n:::\n\n::: {.column width=\"50%\"}\n```{r, results = 'asis', message=F}\n#| code-fold: true\nschool %>%\n  full_join(select(school_usa, Region, Superpower)) %>%\n  mutate(Region = ifelse(Region == \"NY\", \"NY\", \"USA\")) %>%\n  filter(!is.na(Superpower)) %>%\n  group_by(Region, Superpower) %>%\n  summarize(Frequency = n()) %>%\n  mutate(Proportion = Frequency/sum(Frequency)) %>%\n  select(-Frequency) %>%\n  spread(Region, Proportion) %>%\n  kable(., \n        col.names = c(\"Superpower\", \"NY Observed\\nProportion\", \n                      \"USA\\nProportion\"), \n        format = \"html\", \n        digits = 2)%>% \n  kable_classic(font_size = 25)\n```\n:::\n:::\n\n------------------------------------------------------------------------\n\n### Calculating the $\\chi^2$ test statistic\n\nTo compare the New York observed frequencies to the US data, we need to calculate the frequencies that would have been expected if New York was just like all of the other states.\n\nThe expected frequencies under this null model can be obtained by taking each preference category proportion from the US data (the null expectation) and multiplying it by the sample size for New York:\n\n$$E_i = P_iN_{NY}$$\n\n------------------------------------------------------------------------\n\n```{r, echo = F}\nusa_freq = table(school_usa$Superpower)\nusa_prop = usa_freq/sum(usa_freq)\n\nschool %>%\n  filter(!is.na(Superpower)) %>%\n  group_by(Superpower) %>%\n  summarize(Frequency = n()) %>%\n  mutate(Expected = usa_prop*200,\n         Expected = round(Expected, 2)) %>%\n  kable(., format = \"html\", digits = 2, align = c(\"l\", \"c\", \"c\"),\n        col.names = c(\"Superpower\", \"Observed\\nFreq\", \"Expected Freq\"))\n```\n\nNow what? We need some way to index differences between these frequencies, preferably one that translates easily into a sampling distribution so that we can sensibly determine how rare or unusual the New York data are compared to the US (null) distribution.\n\n------------------------------------------------------------------------\n\n$$\\chi^2_{df = k-1} = \\sum^k_{i=1}\\frac{(O_i-E_i)^2}{E_i}$$\n\nThe chi-square goodness of fit (GOF) statistic compares observed and expected frequencies. It is small when the observed frequencies closely match the expected frequencies under the null hypothesis. The chi-square distribution can be used to determine the particular $\\chi^2$ value that corresponds to a rare or unusual profile of observed frequencies.\n\n------------------------------------------------------------------------\n\n```{r create obs, echo = 3:4}\nny_observed = table(school$Superpower)\nny_expected = (table(school_usa$Superpower)/sum(table(school_usa$Superpower)))*200\nny_observed\nny_expected\n```\n\n------------------------------------------------------------------------\n\n```{r, ref.label=\"create obs\", echo=3:4}\n\n```\n\n```{r}\n(chi_square = sum((ny_observed - ny_expected)^2/ny_expected))\n```\n\n------------------------------------------------------------------------\n\n```{r, ref.label=\"create obs\", echo=3:4}\n\n```\n\n```{r, highlight=2}\n(chi_square = sum((ny_observed - ny_expected)^2/ny_expected))\n(critical_val = qchisq(p = 0.95, df = length(ny_expected)-1))\n```\n\n------------------------------------------------------------------------\n\n```{r, ref.label=\"create obs\", echo=3:4}\n\n```\n\n```{r, highlight=2}\n(chi_square = sum((ny_observed - ny_expected)^2/ny_expected))\n(critical_val = qchisq(p = 0.95, df = length(ny_expected)-1))\n(p_val = pchisq(q = chi_square, df = length(ny_expected)-1, lower.tail = F))\n```\n\n------------------------------------------------------------------------\n\n::: columns\n::: {.column width=\"30%\"}\n[The degrees of freedom are the number of categories (k) minus 1. Given that the category frequencies must sum to the total sample size, k-1 category frequencies are free to vary; the last is determined.]{style=\"font-size:30px;\"}\n:::\n\n::: {.column width=\"70%\"}\n```{r}\n#| code-fold: true\n#| \ndata.frame(x = seq(0,20)) %>%\n  ggplot(aes(x = x)) +\n  stat_function(fun = function(x) dchisq(x, df = length(ny_expected)-1), geom = \"line\") +\n  stat_function(fun = function(x) dchisq(x, df = length(ny_expected)-1), geom = \"area\", fill = \"purple\", \n                xlim =c(critical_val, 20)) +\n  geom_vline(aes(xintercept = critical_val), linetype = 2, color = \"purple\")+\n    geom_vline(aes(xintercept = chi_square), linetype = 2, color = \"black\")+\n  geom_text(aes(x = critical_val+2, y = dchisq(critical_val, length(ny_expected)-1) + .05, \n                label = paste(\"Critical Value =\", round (critical_val,2))), angle = 90)+\n    geom_text(aes(x = chi_square+2, y = dchisq(critical_val, length(ny_expected)-1) + .05, \n                label = paste(\"Test statistic =\", round (chi_square,2))), angle = 90)+\n  labs(x = \"Chi-Square\", y = \"Density\", title = \"Area under the curve\") +\n  theme_pubr(base_size = 20)\n```\n:::\n:::\n\n------------------------------------------------------------------------\n\n::: columns\n::: {.column width=\"30%\"}\n[The degrees of freedom are the number of categories (k) minus 1. Given that the category frequencies must sum to the total sample size, k-1 category frequencies are free to vary; the last is determined.]{style=\"font-size:30px;\"}\n:::\n\n::: {.column width=\"70%\"}\n```{r}\n#| code-fold: true\n#| \ndata.frame(x = seq(0,20)) %>%\n  ggplot(aes(x = x)) +\n  stat_function(fun = function(x) dchisq(x, df = length(ny_expected)-1), geom = \"line\") +\n  stat_function(fun = function(x) dchisq(x, df = length(ny_expected)-1), geom = \"area\", fill = \"purple\", \n                xlim =c(chi_square, 20)) +\n  geom_vline(aes(xintercept = critical_val), linetype = 2, color = \"black\")+\n  geom_vline(aes(xintercept = chi_square), linetype = 2, color = \"purple\")+\n  geom_text(aes(x = critical_val+2, y = dchisq(critical_val, length(ny_expected)-1) + .05, \n                label = paste(\"Critical Value =\", round (critical_val,2))), angle = 90)+\n  geom_text(aes(x = chi_square+2, y = dchisq(critical_val, length(ny_expected)-1) + .05, \n                label = paste(\"Test statistic =\", round (chi_square,2))), angle = 90)+\n  labs(x = \"Chi-Square\", y = \"Density\", title = \"Area under the curve\") +\n  theme_pubr(base_size = 20)\n```\n:::\n:::\n\n------------------------------------------------------------------------\n\n```{r}\np.usa = (table(school_usa$Superpower)/sum(table(school_usa$Superpower)))\np.usa\nchisq.test(x = ny_observed, p = p.usa)\n```\n\nThe New York student preferences are unusual under the null hypothesis (USA preferences).\n\nNote that the `chisq.test` function takes for x a vector of the counts. In other words, to use this function, you need to calculate the summary statisttic of counts and feed that into the function.\n\n------------------------------------------------------------------------\n\n```{r}\nc.test = chisq.test(x = ny_observed, p = p.usa)\nstr(c.test)\n```\n\n------------------------------------------------------------------------\n\n```{r}\nc.test$residuals\n```\n\n------------------------------------------------------------------------\n\n```{r, echo = 2}\np.usa = as.data.frame(p.usa)[,\"Freq\"]\nlsr::goodnessOfFitTest(x = as.factor(school$Superpower), p = p.usa)\n```\n\n(Note that this function, `goodnessOfFitTest`, takes the raw data, not the vector of counts.)\n\n------------------------------------------------------------------------\n\nWhat if we had used the equal proportions null hypothesis?\n\n```{r}\nlsr::goodnessOfFitTest(x = as.factor(school$Superpower))\n```\n\nWhy might this be a sensible or useful test?\n\n## Write-up Practice\n\nOpen up a word document and provide a 2-6 sentence write up of the analyses that we just completed\n\nDon't forget:\n\n1.  Before the test, there are some descriptives\n\n2.  Information about the the null hypothesis\n\n3.  Including a \"stats block\"\n\n4.  Brief interpretation of the results\n\n## The usefulness of $\\chi^2$\n\nHow often will you conducted a $chi^2$ goodness of fit test on raw data?\n\n-   (Probably) never\n\nHow often will you come across $\\chi^2$ tests?\n\n-   (Probably) a lot!\n\nThe goodness of fit test is used to statistically test the how well a model fits data.\n\n------------------------------------------------------------------------\n\n## Model Fit with $\\chi^2$\n\nTo calculate Goodness of Fit of a model to data, you build a statistical model of the process as you believe it is in the world.\n\n::: nonincremental\n-   example: depression \\~ age + parental history of depression\n:::\n\n-   Then you estimate each subject's predicted/expected value based on your model.\n\n-   You compare each subject's predicted value to their actual value -- the difference is called the **residual** ( $\\varepsilon$ ).\n\n------------------------------------------------------------------------\n\nIf your model is a good fit, then\n\n$$\\Sigma_1^N\\varepsilon^2 = \\chi^2$$\n\n-   We would then compare that to the distribution of the Null: $\\chi^2_{N-p}$ .\n\n-   Significant chi-square tests suggest the model does not fit -- the data have values that are far away from \"expected.\"\n\n------------------------------------------------------------------------\n\nWhen we move from categorical outcomes to variables measured on an interval or ratio scale, we become interested in means rather than frequencies. Comparing means is usually done with the *t*-test, of which there are several forms.\n\nThe one-sample *t*-test is appropriate when a single sample mean is compared to a population mean but the population standard deviation is unknown. A sample estimate of the population standard deviation is used instead. The appropriate sampling distribution is the t-distribution, with N-1 degrees of freedom.\n\n$$t_{df=N-1} = \\frac{\\bar{X}-\\mu}{\\frac{\\hat{\\sigma}}{\\sqrt{N}}}$$\n\nThe heavier tails of the t-distribution, especially for small N, are the penalty we pay for having to estimate the population standard deviation from the sample.\n\n------------------------------------------------------------------------\n\n# Next up...\n\nFurther dive into categorical data analysis\n\nStarting with t-tests\n","srcMarkdownNoYaml":"\n\n```{r, include = F}\nknitr::opts_chunk$set(message = FALSE, warning = FALSE)\n```\n\n## Last week\n\nNHST & *p-*values\n\nLab 2...feedback?\n\nFrequency of Labs check-in\n\n```{r, results = 'hide', message = F, warning = F}\nlibrary(here)\nlibrary(tidyverse)\nlibrary(psych)\nlibrary(psychTools)\nlibrary(knitr)\nlibrary(kableExtra)\nlibrary(ggpubr)\nlibrary(patchwork)\n\nset.seed(42)\n```\n\n------------------------------------------------------------------------\n\n## This Week...\n\n-   The chi-square goodness-of-fit test\n-   One-sample *t*-tests\n\n------------------------------------------------------------------------\n\n### Key questions:\n\n-   How do we know if category frequencies are consistent with null hypothesis expectations?\n\n-   How do we handle categories with very low frequencies?\n\n-   How do we compare one sample to a population mean?\n\n------------------------------------------------------------------------\n\n## What are the steps of NHST?\n\n::: columns\n::: {.column width=\"50%\"}\n1.  Define null and alternative hypothesis.\n\n2.  Set and justify alpha level.\n\n3.  Determine which sampling distribution ( $z$, $t$, or $\\chi^2$ for now)\n\n4.  Calculate parameters of your sampling distribution under the null.\n\n-   If $z$, calculate $\\mu$ and $\\sigma_M$\n:::\n\n::: {.column width=\"50%\"}\n5.  Calculate test statistic under the null.\n\n-   If $z$, $\\frac{\\bar{X} - \\mu}{\\sigma_M}$\n\n6.  Calculate probability of that test statistic or more extreme under the null, and compare to alpha.\n:::\n:::\n\n------------------------------------------------------------------------\n\nOne-sample tests compare your given sample with a \"known\" population.\n\nResearch question: does this sample come from this population?\n\n**Hypotheses**\n\n-   $H_0$: Yes, this sample comes from this population.\n\n-   $H_1$: No, this sample comes from a different population.\n\n------------------------------------------------------------------------\n\n## Example - Coffee Shop\n\nLet's say we collect data on customers of a coffee shop and we want to see if there is an **equal number of folks** that come into the shop across all days. Therefore, we record how many individuals came into the coffee shop over a weeks time.\n\n***How would we test this?***\n\n-   First, setup the Null and alternative:\n\n    -   $H_0$: Customers will be equal across all days.\n\n    -   $H_1$: There will be more customers on one or multiple days than others and will not be equal\n\n------------------------------------------------------------------------\n\n### Example - Distribution\n\n```{r}\n#| code-fold: true\ndays <- c(\"M\", \"T\", \"W\", \"R\", \"F\")\nexpected <- rep(50, length(days))\ndf <- data.frame(days, expected)\norder <- c(\"M\", \"T\", \"W\", \"R\", \"F\")\n\nunif <- df %>% \n  ggplot(aes(x = days,\n             y = expected)) + \n  geom_col() + \n  scale_x_discrete(limits = order) + \n  labs(title = \"Uniform - Null\") + \n  ylim(0, 65)\n\nobserved <- round(rnorm(5, mean = 55, sd = 4),2)\ndf2 <- data.frame(days, observed)\n\nrand <- df %>% \n  ggplot(aes(x = days, \n             y = observed)) + \n  geom_col() + \n  scale_x_discrete(limits = order) + \n  labs(title = \"Alternative\") + \n  ylim(0, 65)\n\n(unif + rand)\n```\n\n------------------------------------------------------------------------\n\n### Example - Set Alpha\n\nAfter determining the Null and Alternative Hypotheses, we set our Alpha level.\n\nLet's keep things simple and keep it at convention to set it for $\\alpha$ = 0.05\n\n------------------------------------------------------------------------\n\n### Example - Distribution\n\nNow we determine the type of distribution that we will be working with\n\nIn the past we have used:\n\n-   Normal Distribution ( $z$-scores )\n\n-   t-distribution ( $t$-scores )\n\n------------------------------------------------------------------------\n\nNow we are going to be using a distribution that works with ***categorical (nominal)*** **data**.\n\n**The** $\\chi^2$ **- distribution**\n\n::: nonincremental\n-   We use this distribution because we are dealing with (1) one sample, and (2) a categorical outcome\n\n-   **Note**: this test will provide statistical evidence of an association or relationship between two categorical variables\n:::\n\n***???*** The way you measure the variable determines whether it is categorical or continuous. We can create summary statistics from categorical variables by counting or calculating proportions -- but that makes the summary statistics continuous, *not the outcome variable itself*.\n\n------------------------------------------------------------------------\n\n## Distribution - Degrees of freedom\n\nThe $\\chi^2$ distribution is a single-parameter distribution defined by it's degrees of freedom.\n\nIn the case of a **goodness-of-fit test** (like this one), the degrees of freedom are $\\textbf{k-1}$, where k is the number of groups.\n\n------------------------------------------------------------------------\n\n## Degrees of freedom\n\nThe **Degrees of freedom** are the number of genuinely independent things in a calculation. It's specifically calculated as the number of quantities in a calculation minus the number of constraints.\n\nWhat it means in principle is that given a set number of categories (k) and a constraint (the proportions have to add up to 1), I can freely choose numbers for k-1 categories. But for the kth category, there's only one number that will work.\n\n------------------------------------------------------------------------\n\n::: columns\n::: {.column width=\"30%\"}\n[The degrees of freedom are the number of categories (k) minus 1. Given that the category frequencies must sum to the total sample size, k-1 category frequencies are free to vary; the last is determined.]{style=\"font-size:30px;\"}\n:::\n\n::: {.column width=\"70%\"}\n```{r}\n(critical_val = qchisq(p = 0.95, df = 5-1))\n```\n\n```{r}\n#| code-fold: true\n#| \n\ndata.frame(x = seq(0,20)) %>%\n  ggplot(aes(x = x)) +\n  stat_function(fun = function(x) dchisq(x, df = 5-1), geom = \"line\") +\n  stat_function(fun = function(x) dchisq(x, df = 5-1), geom = \"area\", fill = \"purple\", \n                xlim =c(critical_val, 20)) +\n  geom_vline(aes(xintercept = critical_val), linetype = 2, color = \"purple\")+\n  geom_text(aes(x = critical_val+2, y = dchisq(critical_val, 5-1) + .05, \n                label = paste(\"Critical Value =\", round (critical_val,2))), angle = 90)+\n  labs(x = \"Chi-Square\", y = \"Density\", title = \"Area under the curve\") +\n  theme_pubr(base_size = 20)\n```\n:::\n:::\n\n------------------------------------------------------------------------\n\n### Calculating the $\\chi^2$ test statistic\n\nLet's first take a look at the observed data that we have as well as the expected data under the Null\n\n```{r}\nobserved <- df2\nexpected <- df\n```\n\n|              |       |                   |                   |                   |                   |                   |\n|-----------|-----------|-----------|-----------|-----------|-----------|-----------|\n|              |       | **Monday**        | **Tuesday**       | **Wednesday**     | **Thursday**      | **Friday**        |\n| **Observed** | $O_i$ | `r observed[1,2]` | `r observed[2,2]` | `r observed[3,2]` | `r observed[4,2]` | `r observed[5,2]` |\n| **Expected** | $E_i$ | `r expected[1,2]` | `r expected[2,2]` | `r expected[3,2]` | `r expected[4,2]` | `r expected[5,2]` |\n\nNow what? We need some way to index differences between these frequencies so that we can sensibly determine how rare or unusual the observed data are compared to the null distribution.\n\n------------------------------------------------------------------------\n\n$$\\chi^2_{df = k-1} = \\sum^k_{i=1}\\frac{(O_i-E_i)^2}{E_i}$$\n\nThe chi-square goodness of fit (GOF) statistic compares observed and expected frequencies. It is small when the observed frequencies closely match the expected frequencies under the null hypothesis. The chi-square distribution can be used to determine the particular $\\chi^2$ value that corresponds to a rare or unusual profile of observed frequencies.\n\n------------------------------------------------------------------------\n\n|                |             |                                   |                                   |                                   |                                   |                                   |\n|-----------|-----------|-----------|-----------|-----------|-----------|-----------|\n|                |             | **Monday**                        | **Tuesday**                       | **Wednesday**                     | **Thursday**                      | **Friday**                        |\n| **Observed**   | $O_i$       | `r observed[1,2]`                 | `r observed[2,2]`                 | `r observed[3,2]`                 | `r observed[4,2]`                 | `r observed[5,2]`                 |\n| **Expected**   | $E_i$       | `r expected[1,2]`                 | `r expected[2,2]`                 | `r expected[3,2]`                 | `r expected[4,2]`                 | `r expected[5,2]`                 |\n| **Difference** | $O_i - E_i$ | `r observed[1,2] - expected[1,2]` | `r observed[2,2] - expected[2,2]` | `r observed[3,2] - expected[3,2]` | `r observed[4,2] - expected[4,2]` | `r observed[5,2] - expected[5,2]` |\n\n------------------------------------------------------------------------\n\n```{r, results = 'asis', message=F}\n#| code-fold: true\nchisq_tab <- full_join(expected, observed) %>% \n  mutate(diff = (observed - expected))\nchisq_tab %>% \n  kable(., format = \"html\", digits = 2, \n        align = c(\"c\", \"c\", \"c\", \"c\"), \n        col.names = c(\"Days\", \"Expected\", \"Observed\", \"Difference\"))\n\n```\n\n------------------------------------------------------------------------\n\nWe now have the differences between the expected and observed, but there are likely going to be some negative numbers. To fix this, we can square the differences\n\n```{r, results = 'asis', message=F}\n#| code-fold: true\nchisq_tab %>% \n  mutate(sq_diff=(diff^2)) %>% \n  kable(., format = \"html\", digits = 2, \n        align = c(\"c\", \"c\", \"c\", \"c\", \"c\"), \n        col.names = c(\"Days\", \"Expected\", \"Observed\", \"Difference\", \"Sq. Diff\"))\n```\n\n------------------------------------------------------------------------\n\n::: columns\n::: {.column width=\"50%\"}\nNow we have a collection of numbers that are large whenever the null hypothesis makes a bad prediction and small when it makes a good prediction.\n\nNext, we need to divide all numbers by the expected frequency as a way to put our estimate into perspective\n:::\n\n::: {.column width=\"50%\"}\n```{r, results = 'asis', message=F}\n#| code-fold: true\nchisq_tab <- chisq_tab %>% \n  mutate(sq_diff=(diff^2)) %>%\n  mutate(error = sq_diff/50) \n\nchisq_tab %>% \n  kable(., format = \"html\", digits = 2, \n        align = c(\"c\", \"c\", \"c\", \"c\", \"c\", \"c\"), \n        col.names = c(\"Days\", \"Expected\", \"Observed\", \"Difference\", \"Sq. Diff\", \"Error Score\"))\n\n```\n:::\n:::\n\n------------------------------------------------------------------------\n\nWe can finish this off by taking each of our scores related to \"error\" and adding them up\n\nThe result is the **goodness of fit** statistic (GOF) or $\\chi^2$\n\n::: columns\n::: {.column width=\"50%\"}\n```{r}\nchi_square <- sum(chisq_tab$error)\nchi_square\n```\n\nLet's compare that to the critical $\\chi^2$ value given the df of the sample\n\n```{r}\ncritical_val <- qchisq(p = 0.95, df = length(chisq_tab$days)-1)\ncritical_val\n```\n:::\n\n::: {.column width=\"50%\"}\nCalculate the probability of getting our sample statistic or greater ***if the null were true***\n\n```{r}\np_val <- pchisq(q = chi_square, df = length(chisq_tab$days)-1, lower.tail = F)\np_val\n```\n\n**What can we conclude?**\n:::\n:::\n\n------------------------------------------------------------------------\n\n::: columns\n::: {.column width=\"30%\"}\n[The degrees of freedom are the number of categories (k) minus 1. Given that the category frequencies must sum to the total sample size, k-1 category frequencies are free to vary; the last is determined.]{style=\"font-size:30px;\"}\n:::\n\n::: {.column width=\"70%\"}\n```{r}\n#| code-fold: true\n#| \ndata.frame(x = seq(0,20)) %>%\n  ggplot(aes(x = x)) +\n  stat_function(fun = function(x) dchisq(x, df = length(chisq_tab$days)-1), geom = \"line\") +\n  stat_function(fun = function(x) dchisq(x, df = length(chisq_tab$days)-1), geom = \"area\", fill = \"purple\", \n                xlim =c(critical_val, 20)) +\n  geom_vline(aes(xintercept = critical_val), linetype = 2, color = \"purple\")+\n    geom_vline(aes(xintercept = chi_square), linetype = 2, color = \"black\")+\n  geom_text(aes(x = critical_val+2, y = dchisq(critical_val, length(chisq_tab$days)-1) + .05, \n                label = paste(\"Critical Value =\", round (critical_val,2))), angle = 90)+\n    geom_text(aes(x = chi_square+2, y = dchisq(critical_val, length(chisq_tab$days)-1) + .05, \n                label = paste(\"Test statistic =\", round (chi_square,2))), angle = 90)+\n  labs(x = \"Chi-Square\", y = \"Density\", title = \"Area under the curve\") +\n  theme_pubr(base_size = 20)\n```\n:::\n:::\n\n------------------------------------------------------------------------\n\n::: columns\n::: {.column width=\"30%\"}\n[The degrees of freedom are the number of categories (k) minus 1. Given that the category frequencies must sum to the total sample size, k-1 category frequencies are free to vary; the last is determined.]{style=\"font-size:30px;\"}\n:::\n\n::: {.column width=\"70%\"}\n```{r}\n#| code-fold: true\n#| \ndata.frame(x = seq(0,20)) %>%\n  ggplot(aes(x = x)) +\n  stat_function(fun = function(x) dchisq(x, df = length(chisq_tab$days)-1), geom = \"line\") +\n  stat_function(fun = function(x) dchisq(x, df = length(chisq_tab$days)-1), geom = \"area\", fill = \"purple\", \n                xlim =c(chi_square, 20)) +\n  geom_vline(aes(xintercept = critical_val), linetype = 2, color = \"black\")+\n  geom_vline(aes(xintercept = chi_square), linetype = 2, color = \"purple\")+\n  geom_text(aes(x = critical_val+2, y = dchisq(critical_val, length(chisq_tab$days)-1) + .05, \n                label = paste(\"Critical Value =\", round (critical_val,2))), angle = 90)+\n  geom_text(aes(x = chi_square+2, y = dchisq(critical_val, length(chisq_tab$days)-1) + .05, \n                label = paste(\"Test statistic =\", round (chi_square,2))), angle = 90)+\n  labs(x = \"Chi-Square\", y = \"Density\", title = \"Area under the curve\") +\n  theme_pubr(base_size = 20)\n```\n:::\n:::\n\n------------------------------------------------------------------------\n\n### Recap of the Steps\n\n1.  Define null and alternative hypothesis.\n\n-   $H_0$: No difference across days\n\n-   $H_1$: Days will be different\n\n2.  Set and justify alpha level $\\alpha$ = 0.05\n\n3.  Determine which sampling distribution ( $\\chi^2$ )\n\n4.  Calculate parameters of your sampling distribution under the null.\n\n-   Calculate $\\chi^2$-critical: `r critical_val`\n\n### Recap of the Steps (con't.)\n\n5.  Calculate test statistic under the null.\n\n-   $\\chi^2_{df = k-1} = \\sum^k_{i=1}\\frac{(O_i-E_i)^2}{E_i}$\n-   `r chi_square`\n\n6.  Calculate probability of that test statistic or more extreme under the null, and compare to alpha.\n\n    ```{r}\n    pchisq(q = chi_square, df = length(chisq_tab$days)-1, lower.tail = F)\n    ```\n\n------------------------------------------------------------------------\n\nNow that we know how to do the test using R, we now need to be able to ***write up the analyses*** that we just performed.\n\nIf we wanted to write these results from our example into a paper or something similar (maybe telling our pet who prefers scientific writing), we could do this:\n\n> Across all days of the average workweek, we observed, `r observed[1,2]` patrons on Monday, `r observed[2,2]` customers on Tuesday, `r observed[3,2]` on Wednesday, `r observed[4,2]` for Thursday, and finally `r observed[5,2]` patrons on Friday. A chi-square goodness of fit test was conducted to test whether these data followed a uniform distribution with 50 visits per day. The results of the test ( $\\chi^2$(4) = `r round(chi_square, 2)`, $p$ = `r round(p_val, 2)`) indicate no differences from the uniform distribution.\n\n------------------------------------------------------------------------\n\n> Across all days of the average workweek, we observed, `r observed[1,2]` patrons on Monday, `r observed[2,2]` customers on Tuesday, `r observed[3,2]` on Wednesday, `r observed[4,2]` for Thursday, and finally `r observed[5,2]` patrons on Friday. A chi-square goodness of fit test was conducted to test whether these data followed a uniform distribution with 50 visits per day. The results of the test ( $\\chi^2$(4) = `r round(chi_square, 2)`, $p$ = `r round(p_val, 2)`) indicate no differences from the uniform distribution.\n\nA couple things to notice about the write up:\n\n::: columns\n::: {.column width=\"50%\"}\n1.  Before the test, there are some descriptives\n\n2.  Informs you of the null hypothesis\n:::\n\n::: {.column width=\"50%\"}\n3.  Inclusion of a \"stats block\"\n4.  Results are interpreted\n:::\n:::\n\n## But what if...\n\nIn the example, we had a null distribution that was distributed uniformly\n\nWhat if that isn't a super interesting research question?\n\nInstead we may want to compare the proportions in our sample to a larger population\n\n## Example 2 - Schools & Super-powers\n\nThe data were obtained from [Census at School](https://ww2.amstat.org/censusatschool/), a website developed by the American Statistical Association tohelp students in the 4th through 12th grades understand statistical problem-solving.\n\n::: nonincremental\n-   The site sponsors a survey that students can complete and a database that students and instructors can use to illustrate principles in quantitative methods.\n\n-   The database includes students from all 50 states, from grade levels 4 through 12, both boys and girls, who have completed the survey dating back to 2010.\n:::\n\n------------------------------------------------------------------------\n\nLet's focus on a single question:\n\n*Which of the following superpowers would you most like to have? Select one.*\n\n::: nonincremental\n::: columns\n::: {.column width=\"50%\"}\n-   Invisibility\n-   Telepathy (read minds)\n-   Freeze time\n:::\n\n::: {.column width=\"50%\"}\n-   Super strength\n-   Fly\n:::\n:::\n:::\n\nThe responses from 250 randomly selected New York students were obtained from the Census at School database.\n\n```{r, message=FALSE, warning = F}\n\nschool <- read_csv(\"https://raw.githubusercontent.com/dharaden/dharaden.github.io/main/courses/PSYC640_fall23/data/example2-chisq.csv\") \nschool <- school %>% \n  filter(!is.na(Superpower))\n```\n\n------------------------------------------------------------------------\n\n::: columns\n::: {.column width=\"50%\"}\n```{r frequency table, results = 'asis'}\n#| code-fold: true\nschool %>%\n  group_by(Superpower) %>%\n  summarize(Frequency = n()) %>%\n  mutate(Proportion = Frequency/sum(Frequency)) %>%\n  kable(., \n        format = \"html\", \n        digits = 2) %>% \n  kable_classic(font_size = 25)\n```\n:::\n\n::: {.column width=\"50%\"}\nDescriptively this is interesting. But, are the responses unusual or atypical in any way? To answer that question, we need some basis for comparison---a null hypothesis <br> One option would be to ask if the New York preferences are different compared to students from the general population.\n:::\n:::\n\n------------------------------------------------------------------------\n\n```{r, message=FALSE, warning = F}\n#| code-fold: true\nschool_usa <-  read_csv(\"https://raw.githubusercontent.com/uopsych/psy611/master/data/census_at_school_usa.csv\")\n\nschool_usa$Region = \"USA\"\nschool %>%\n  full_join(select(school_usa, Region, Superpower)) %>%\n  filter(!is.na(Superpower)) %>%\n  group_by(Region, Superpower) %>%\n  summarize(Frequency = n()) %>%\n  mutate(Proportion = Frequency/sum(Frequency)) %>%\n  ggplot(aes(x = Region, y = Proportion, fill = Region)) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  coord_flip() +\n  labs(\n    x = NULL,\n    title = \"Category Proportion as a function of Source\") +\n  guides(fill = \"none\") +\n  facet_wrap(~Superpower) +\n  theme_bw(base_size = 20) + \n  theme(plot.title.position = \"plot\")\n```\n\n------------------------------------------------------------------------\n\n::: columns\n::: {.column width=\"50%\"}\n$H_0$: New York student superpower preferences are similar to the preferences of typical students in the United States.\n\n$H_1$: New York student superpower preferences are different from the preferences of typical students in the United States.\n:::\n\n::: {.column width=\"50%\"}\n```{r, results = 'asis', message=F}\n#| code-fold: true\nschool %>%\n  full_join(select(school_usa, Region, Superpower)) %>%\n  mutate(Region = ifelse(Region == \"NY\", \"NY\", \"USA\")) %>%\n  filter(!is.na(Superpower)) %>%\n  group_by(Region, Superpower) %>%\n  summarize(Frequency = n()) %>%\n  mutate(Proportion = Frequency/sum(Frequency)) %>%\n  select(-Frequency) %>%\n  spread(Region, Proportion) %>%\n  kable(., \n        col.names = c(\"Superpower\", \"NY Observed\\nProportion\", \n                      \"USA\\nProportion\"), \n        format = \"html\", \n        digits = 2)%>% \n  kable_classic(font_size = 25)\n```\n:::\n:::\n\n------------------------------------------------------------------------\n\n### Calculating the $\\chi^2$ test statistic\n\nTo compare the New York observed frequencies to the US data, we need to calculate the frequencies that would have been expected if New York was just like all of the other states.\n\nThe expected frequencies under this null model can be obtained by taking each preference category proportion from the US data (the null expectation) and multiplying it by the sample size for New York:\n\n$$E_i = P_iN_{NY}$$\n\n------------------------------------------------------------------------\n\n```{r, echo = F}\nusa_freq = table(school_usa$Superpower)\nusa_prop = usa_freq/sum(usa_freq)\n\nschool %>%\n  filter(!is.na(Superpower)) %>%\n  group_by(Superpower) %>%\n  summarize(Frequency = n()) %>%\n  mutate(Expected = usa_prop*200,\n         Expected = round(Expected, 2)) %>%\n  kable(., format = \"html\", digits = 2, align = c(\"l\", \"c\", \"c\"),\n        col.names = c(\"Superpower\", \"Observed\\nFreq\", \"Expected Freq\"))\n```\n\nNow what? We need some way to index differences between these frequencies, preferably one that translates easily into a sampling distribution so that we can sensibly determine how rare or unusual the New York data are compared to the US (null) distribution.\n\n------------------------------------------------------------------------\n\n$$\\chi^2_{df = k-1} = \\sum^k_{i=1}\\frac{(O_i-E_i)^2}{E_i}$$\n\nThe chi-square goodness of fit (GOF) statistic compares observed and expected frequencies. It is small when the observed frequencies closely match the expected frequencies under the null hypothesis. The chi-square distribution can be used to determine the particular $\\chi^2$ value that corresponds to a rare or unusual profile of observed frequencies.\n\n------------------------------------------------------------------------\n\n```{r create obs, echo = 3:4}\nny_observed = table(school$Superpower)\nny_expected = (table(school_usa$Superpower)/sum(table(school_usa$Superpower)))*200\nny_observed\nny_expected\n```\n\n------------------------------------------------------------------------\n\n```{r, ref.label=\"create obs\", echo=3:4}\n\n```\n\n```{r}\n(chi_square = sum((ny_observed - ny_expected)^2/ny_expected))\n```\n\n------------------------------------------------------------------------\n\n```{r, ref.label=\"create obs\", echo=3:4}\n\n```\n\n```{r, highlight=2}\n(chi_square = sum((ny_observed - ny_expected)^2/ny_expected))\n(critical_val = qchisq(p = 0.95, df = length(ny_expected)-1))\n```\n\n------------------------------------------------------------------------\n\n```{r, ref.label=\"create obs\", echo=3:4}\n\n```\n\n```{r, highlight=2}\n(chi_square = sum((ny_observed - ny_expected)^2/ny_expected))\n(critical_val = qchisq(p = 0.95, df = length(ny_expected)-1))\n(p_val = pchisq(q = chi_square, df = length(ny_expected)-1, lower.tail = F))\n```\n\n------------------------------------------------------------------------\n\n::: columns\n::: {.column width=\"30%\"}\n[The degrees of freedom are the number of categories (k) minus 1. Given that the category frequencies must sum to the total sample size, k-1 category frequencies are free to vary; the last is determined.]{style=\"font-size:30px;\"}\n:::\n\n::: {.column width=\"70%\"}\n```{r}\n#| code-fold: true\n#| \ndata.frame(x = seq(0,20)) %>%\n  ggplot(aes(x = x)) +\n  stat_function(fun = function(x) dchisq(x, df = length(ny_expected)-1), geom = \"line\") +\n  stat_function(fun = function(x) dchisq(x, df = length(ny_expected)-1), geom = \"area\", fill = \"purple\", \n                xlim =c(critical_val, 20)) +\n  geom_vline(aes(xintercept = critical_val), linetype = 2, color = \"purple\")+\n    geom_vline(aes(xintercept = chi_square), linetype = 2, color = \"black\")+\n  geom_text(aes(x = critical_val+2, y = dchisq(critical_val, length(ny_expected)-1) + .05, \n                label = paste(\"Critical Value =\", round (critical_val,2))), angle = 90)+\n    geom_text(aes(x = chi_square+2, y = dchisq(critical_val, length(ny_expected)-1) + .05, \n                label = paste(\"Test statistic =\", round (chi_square,2))), angle = 90)+\n  labs(x = \"Chi-Square\", y = \"Density\", title = \"Area under the curve\") +\n  theme_pubr(base_size = 20)\n```\n:::\n:::\n\n------------------------------------------------------------------------\n\n::: columns\n::: {.column width=\"30%\"}\n[The degrees of freedom are the number of categories (k) minus 1. Given that the category frequencies must sum to the total sample size, k-1 category frequencies are free to vary; the last is determined.]{style=\"font-size:30px;\"}\n:::\n\n::: {.column width=\"70%\"}\n```{r}\n#| code-fold: true\n#| \ndata.frame(x = seq(0,20)) %>%\n  ggplot(aes(x = x)) +\n  stat_function(fun = function(x) dchisq(x, df = length(ny_expected)-1), geom = \"line\") +\n  stat_function(fun = function(x) dchisq(x, df = length(ny_expected)-1), geom = \"area\", fill = \"purple\", \n                xlim =c(chi_square, 20)) +\n  geom_vline(aes(xintercept = critical_val), linetype = 2, color = \"black\")+\n  geom_vline(aes(xintercept = chi_square), linetype = 2, color = \"purple\")+\n  geom_text(aes(x = critical_val+2, y = dchisq(critical_val, length(ny_expected)-1) + .05, \n                label = paste(\"Critical Value =\", round (critical_val,2))), angle = 90)+\n  geom_text(aes(x = chi_square+2, y = dchisq(critical_val, length(ny_expected)-1) + .05, \n                label = paste(\"Test statistic =\", round (chi_square,2))), angle = 90)+\n  labs(x = \"Chi-Square\", y = \"Density\", title = \"Area under the curve\") +\n  theme_pubr(base_size = 20)\n```\n:::\n:::\n\n------------------------------------------------------------------------\n\n```{r}\np.usa = (table(school_usa$Superpower)/sum(table(school_usa$Superpower)))\np.usa\nchisq.test(x = ny_observed, p = p.usa)\n```\n\nThe New York student preferences are unusual under the null hypothesis (USA preferences).\n\nNote that the `chisq.test` function takes for x a vector of the counts. In other words, to use this function, you need to calculate the summary statisttic of counts and feed that into the function.\n\n------------------------------------------------------------------------\n\n```{r}\nc.test = chisq.test(x = ny_observed, p = p.usa)\nstr(c.test)\n```\n\n------------------------------------------------------------------------\n\n```{r}\nc.test$residuals\n```\n\n------------------------------------------------------------------------\n\n```{r, echo = 2}\np.usa = as.data.frame(p.usa)[,\"Freq\"]\nlsr::goodnessOfFitTest(x = as.factor(school$Superpower), p = p.usa)\n```\n\n(Note that this function, `goodnessOfFitTest`, takes the raw data, not the vector of counts.)\n\n------------------------------------------------------------------------\n\nWhat if we had used the equal proportions null hypothesis?\n\n```{r}\nlsr::goodnessOfFitTest(x = as.factor(school$Superpower))\n```\n\nWhy might this be a sensible or useful test?\n\n## Write-up Practice\n\nOpen up a word document and provide a 2-6 sentence write up of the analyses that we just completed\n\nDon't forget:\n\n1.  Before the test, there are some descriptives\n\n2.  Information about the the null hypothesis\n\n3.  Including a \"stats block\"\n\n4.  Brief interpretation of the results\n\n## The usefulness of $\\chi^2$\n\nHow often will you conducted a $chi^2$ goodness of fit test on raw data?\n\n-   (Probably) never\n\nHow often will you come across $\\chi^2$ tests?\n\n-   (Probably) a lot!\n\nThe goodness of fit test is used to statistically test the how well a model fits data.\n\n------------------------------------------------------------------------\n\n## Model Fit with $\\chi^2$\n\nTo calculate Goodness of Fit of a model to data, you build a statistical model of the process as you believe it is in the world.\n\n::: nonincremental\n-   example: depression \\~ age + parental history of depression\n:::\n\n-   Then you estimate each subject's predicted/expected value based on your model.\n\n-   You compare each subject's predicted value to their actual value -- the difference is called the **residual** ( $\\varepsilon$ ).\n\n------------------------------------------------------------------------\n\nIf your model is a good fit, then\n\n$$\\Sigma_1^N\\varepsilon^2 = \\chi^2$$\n\n-   We would then compare that to the distribution of the Null: $\\chi^2_{N-p}$ .\n\n-   Significant chi-square tests suggest the model does not fit -- the data have values that are far away from \"expected.\"\n\n------------------------------------------------------------------------\n\nWhen we move from categorical outcomes to variables measured on an interval or ratio scale, we become interested in means rather than frequencies. Comparing means is usually done with the *t*-test, of which there are several forms.\n\nThe one-sample *t*-test is appropriate when a single sample mean is compared to a population mean but the population standard deviation is unknown. A sample estimate of the population standard deviation is used instead. The appropriate sampling distribution is the t-distribution, with N-1 degrees of freedom.\n\n$$t_{df=N-1} = \\frac{\\bar{X}-\\mu}{\\frac{\\hat{\\sigma}}{\\sqrt{N}}}$$\n\nThe heavier tails of the t-distribution, especially for small N, are the penalty we pay for having to estimate the population standard deviation from the sample.\n\n------------------------------------------------------------------------\n\n# Next up...\n\nFurther dive into categorical data analysis\n\nStarting with t-tests\n"},"formats":{"revealjs":{"identifier":{"display-name":"RevealJS","target-format":"revealjs","base-format":"revealjs"},"execute":{"fig-width":10,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":false,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"wrap","code-link":false,"code-line-numbers":true,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","html-math-method":{"method":"mathjax","url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML-full"},"slide-level":2,"to":"revealjs","incremental":true,"output-file":"06_OneSample.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":false,"quarto-version":"1.3.433","auto-stretch":true,"editor":"visual","title":"Categorical & One-Sample","subtitle":"PSYC 640 - Fall 2023","author":"Dustin Haraden, PhD","multiplex":true,"slideNumber":true,"touch":true,"theme":"night"}}},"projectFormats":["html"]}