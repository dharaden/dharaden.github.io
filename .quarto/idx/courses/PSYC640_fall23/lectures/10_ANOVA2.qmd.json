{"title":"Two-Way ANOVA","markdown":{"yaml":{"title":"Two-Way ANOVA","subtitle":"PSYC 640 - Fall 2023","author":"Dustin Haraden, PhD","format":{"revealjs":{"multiplex":true,"scrollable":true,"slide-number":true,"incremental":false,"touch":true,"code-overflow":"wrap","theme":"dark"}},"execute":{"echo":true},"editor":"visual","editor_options":{"chunk_output_type":"console"}},"headingText":"Last Class","containsRefs":false,"markdown":"\n\n```{r, include = F}\nknitr::opts_chunk$set(message = FALSE, warning = FALSE)\n```\n\n\n::: nonincremental\n-   One Way ANOVA\n    -   Comparing means across multiple groups/levels\n:::\n\n------------------------------------------------------------------------\n\n## Looking Ahead\n\n-   R-Workshop! ([Link to Sign up](https://docs.google.com/forms/d/e/1FAIpQLSd2Gjcb88kWHBS7LXNDaLCZ9Sb7aroRceMjrLl5K36epewdtQ/viewform?usp=sf_link))\n    -   11/3 & 12/1 from 2-3pm\n-   Professor will put together guidelines for the final project and have dates to complete various components\n-   Lab 3 - Starting Today will be due before class on 11/6\n    -   Will be able to resubmit for any missed items up to 1 week from when I post the initial feedback\n-   Correlation & Regression\n\n------------------------------------------------------------------------\n\n## Today...\n\nOne-Way ANOVA\n\n-   Contrasts/post-hoc tests\n\nTwo-Way ANOVA\n\n```{r, results = 'hide', message = F, warning = F}\n# File management\nlibrary(here)\n# for dplyr, ggplot2\nlibrary(tidyverse)\n#Loading data\nlibrary(rio)\n# Estimating Marginal Means\nlibrary(emmeans)\n# Pretty Tables\nlibrary(kableExtra)\n\n#Remove Scientific Notation \noptions(scipen=999)\n```\n\n# Review of One-Way ANOVA\n\n## One-Way ANOVA\n\nGoal: Inform of differences among the levels of our variable of interest (Omnibus Test)\n\nUsing the between and within group variance to create the $F$-statistic/ratio\n\nHypotheses:\n\n$$\nH_0: it\\: is\\: true\\: that\\: \\mu_1 = \\mu_2 = \\mu_3 =\\: ...\\mu_k \n\\\\ \nH_1: it\\: is\\: \\boldsymbol{not}\\: true\\: that\\: \\mu_1 = \\mu_2 = \\mu_3 =\\: ...\\mu_k\n$$\n\n------------------------------------------------------------------------\n\n![](/images/ANOVA.png){width=\"729\"}\n\n::: columns\n::: {.column width=\"50%\"}\n$F = \\frac{MS_{between}}{MS_{within}} = \\frac{small}{large} < 1$\n:::\n\n::: {.column width=\"50%\"}\n$F = \\frac{MS_{between}}{MS_{within}} = \\frac{large}{small} > 1$\n:::\n:::\n\n## Review of the NHST process\n\n1.  Collect Sample and define hypotheses\n\n2.  Set alpha level\n\n3.  Determine the sampling distribution (will be using $F$-distribution now)\n\n4.  Identify the critical value\n\n5.  Calculate test statistic for sample collected\n\n6.  Inspect & compare statistic to critical value; Calculate probability\n\n## Steps to calculating F-ratio\n\n1.  Variance to Sum of Squares (Between & Within)\n2.  Degrees of Freedom\n3.  Mean squares values\n4.  F-Statistic\n\n## Calculating the F-Statistic\n\n$$F = \\frac{MS_b}{MS_w}$$\n\nIf the null hypothesis is true, $F$ has an expected value close to 1 (numerator and denominator are estimates of the same variability)\n\nIf it is false, the numerator will likely be larger, because systematic, between-group differences contribute to the variance of the means, but not to variance within group.\n\n------------------------------------------------------------------------\n\n```{r fig.width = 10, fig.height = 6}\n#| code-fold: true\n#| \ndata.frame(F = c(0,8)) %>%\n  ggplot(aes(x = F)) +\n  stat_function(fun = function(x) df(x, df1 = 3, df2 = 196), \n                geom = \"line\") +\n  stat_function(fun = function(x) df(x, df1 = 3, df2 = 196), \n                geom = \"area\", xlim = c(2.65, 8), fill = \"purple\") +\n  geom_vline(aes(xintercept = 2.65), color = \"purple\") +\n  scale_y_continuous(\"Density\") + scale_x_continuous(\"F statistic\", breaks = NULL) +\n  theme_bw(base_size = 20)\n```\n\nIf data are normally distributed, then the variance is $\\chi^2$ distributed\n\n$F$-distributions are one-tailed tests. Recall that we're interested in how far away our test statistic from the null $(F = 1).$\n\n------------------------------------------------------------------------\n\n## ANOVA table\n\n+---------------------+----------+----------------+----------------------------+-------------------------+----------+\n| Source of Variation | df       | Sum of Squares | Mean Squares               | F-statistic             | p-value  |\n+:===================:+:========:+:==============:+:==========================:+:=======================:+:========:+\n| Group               | $G-1$    | $SS_b$         | $MS_b = \\frac{SS_b}{df_b}$ | $F = \\frac{MS_b}{MS_w}$ | $p$      |\n+---------------------+----------+----------------+----------------------------+-------------------------+----------+\n| Residual            | $N-G$    | $SS_w$         | $MS_w = \\frac{SS_w}{df_w}$ |                         |          |\n+---------------------+----------+----------------+----------------------------+-------------------------+----------+\n| Total               | $N-1$    | $SS_{total}$   |                            |                         |          |\n+---------------------+----------+----------------+----------------------------+-------------------------+----------+\n\n# Contrasts/Post-Hoc\n\n------------------------------------------------------------------------\n\n## Contrasts/Post-Hoc Tests\n\nPerformed when there is a significant difference among the groups to examine which groups are different\n\n1.  **Contrasts**: When we have *a priori* hypotheses\n2.  **Post-hoc Tests**: When we want to test everything\n\n------------------------------------------------------------------------\n\nThese comparisons take the general form of t-tests, but note some extensions:\n\n-   the pooled variance estimate comes from $SS_{\\text{residual}}$, meaning it pulls information from all groups\n\n-   the degrees of freedom for the $t$-test is $N-k$, so using all data\n\n------------------------------------------------------------------------\n\n## Previous Spooky Data\n\nEMF rating across multiple locations\n\n```{r}\nspooky <- import(\"https://raw.githubusercontent.com/dharaden/dharaden.github.io/main/courses/PSYC640_fall23/data/SS%20Calculations.csv\") %>% \n  select(Location, EMF) #There were some extra empty variables in there that we don't care about\n\nfit_1 <- aov(EMF ~ Location, data = spooky)\nsummary(fit_1)\n\n```\n\n------------------------------------------------------------------------\n\n## Post-hoc tests\n\n```{r}\n#library(emmeans)\nemmeans(fit_1, pairwise ~ Location, adjust = \"none\")\n```\n\n------------------------------------------------------------------------\n\n## Family-wise error\n\nThese pairwise comparisons can quickly grow in number as the number of Groups increases. With 8 (k) Groups, we have k(k-1)/2 = 28 possible pairwise comparisons.\n\nAs the number of groups in the ANOVA grows, the number of possible pairwise comparisons increases dramatically.\n\n```{r,echo = F, fig.width = 10, fig.height = 5.5}\ndata.frame(g = 2:15) %>%\n  mutate(num = (g*(g-1))/2) %>%\n  ggplot(aes(x = g, y = num)) +\n  geom_line(size = 1.5) +\n  geom_point(size = 3)+\n  scale_x_continuous(\"Number of Groups in the ANOVA\") +\n  scale_y_continuous(\"Number of Pairwise Comparisons\") +\n  theme_bw()\n```\n\n------------------------------------------------------------------------\n\nAs the number of tests grows, and assuming the null hypothesis is true, the probability that we will make one or more Type I errors increases. To approximate the magnitude of the problem, we can assume that the multiple pairwise comparisons are independent. The probability that we **don't** make a Type I error for [**one**]{.underline} test is:\n\n$$P(\\text{No Type I}, 1 \\text{ test}) = 1-\\alpha$$\n\n------------------------------------------------------------------------\n\nThe probability that we don't make a Type I error for [**two**]{.underline} tests is:\n\n$$P(\\text{No Type I}, 2 \\text{ test}) = (1-\\alpha)(1-\\alpha)$$\n\nFor C tests, the probability that we make **no** Type I errors is\n\n$$P(\\text{No Type I}, C \\text{ tests}) = (1-\\alpha)^C$$\n\nWe can then use the following to calculate the probability that we make one or more Type I errors in a collection of C independent tests.\n\n$$P(\\text{At least one Type I}, C \\text{ tests}) = 1-(1-\\alpha)^C$$\n\n------------------------------------------------------------------------\n\nThe Type I error inflation that accompanies multiple comparisons motivates the large number of \"correction\" procedures that have been developed.\n\n```{r,echo = F, fig.width = 10, fight.height = 6}\ndata.frame(g = 2:15) %>%\n  mutate(num = (g*(g-1))/2,\n         p_notype1 = (1-.05)^num,\n         p_type1 = 1-p_notype1) %>%\n  ggplot(aes(x = g, y = p_type1)) +\n  geom_line(size = 1.5) +\n  geom_point(size = 3)+\n  scale_x_continuous(\"Number of Groups in the ANOVA\") +\n  scale_y_continuous(\"Probability of a Type I Error\") +\n  theme_bw()\n```\n\n------------------------------------------------------------------------\n\nMultiple comparisons, each tested with $\\alpha_{per-test}$, increases the family-wise $\\alpha$ level.\n\n$$\\large \\alpha_{family-wise} = 1 - (1-\\alpha_{per-test})^C$$ Šidák showed that the family-wise a could be controlled to a desired level (e.g., .05) by changing the $\\alpha_{per-test}$ to:\n\n$$\\large \\alpha_{per-wise} = 1 - (1-\\alpha_{family-wise})^{\\frac{1}{C}}$$\n\n------------------------------------------------------------------------\n\n### Bonferroni\n\nBonferroni (and Dunn, and others) suggested this simple approximation:\n\n$$\\large \\alpha_{per-test} = \\frac{\\alpha_{family-wise}}{C}$$\n\nThis is typically called the Bonferroni correction and is very often used even though better alternatives are available.\n\n------------------------------------------------------------------------\n\n```{r}\nemmeans(fit_1, pairwise ~ Location, adjust = \"bonferroni\")\n```\n\n------------------------------------------------------------------------\n\nThe Bonferroni procedure is conservative. Other correction procedures have been developed that control family-wise Type I error at .05 but that are more powerful than the Bonferroni procedure. The most common one is the Holm procedure.\n\nThe Holm procedure does not make a constant adjustment to each per-test $\\alpha$. Instead it makes adjustments in stages depending on the relative size of each pairwise p-value.\n\n------------------------------------------------------------------------\n\n### Holm correction\n\n1.  Rank order the p-values from largest to smallest.\n2.  Start with the smallest p-value. Multiply it by its rank.\n3.  Go to the next smallest p-value. Multiply it by its rank. If the result is larger than the adjusted p-value of next smallest rank, keep it. Otherwise replace with the previous step adjusted p-value.\n4.  Repeat Step 3 for the remaining p-values.\n5.  Judge significance of each new p-value against $\\alpha = .05$.\n\n------------------------------------------------------------------------\n\n```{r,echo = F, results = 'asis'}\n\ndata.frame(or = c(.0012, .0023, .0450, .0470, .0530, .2100),\n           rank = c(6:1)) %>%\n  mutate(rankp = or*rank,\n         holm = p.adjust(or, method = \"holm\"),\n         bon = p.adjust(or, method = \"bonferroni\")) %>%\n  kable(., \n        col.names = c(\"Original p value\", \"Rank\", \n                      \"Rank x p\", \"Holm\", \"Bonferroni\")) \n```\n\n------------------------------------------------------------------------\n\n```{r}\nemmeans(fit_1, pairwise ~ Location, adjust = \"holm\")\n```\n\n# ANOVA is regression\n\nIn regression, we can accommodate categorical predictors. How does this compare to ANOVA?\n\n-   Same omnibus test of the model!\n\n\\*(Really the same model, but packaged differently.)\n\n-   When would you use one versus the other?\n\n------------------------------------------------------------------------\n\n::: columns\n::: {.column width=\"50%\"}\n**ANOVA**\n\n-   More traditional for 3+ groups\n\n-   Comparing/controlling multiple categorical variables\n:::\n\n::: {.column width=\"50%\"}\n**Regression**\n\n-   Best for two groups\n\n-   Incorporating continuous predictors too\n\n-   Good for 3+ groups when you have more specific hypotheses (contrasts)\n:::\n:::\n\n# Two-Way ANOVA\n\n------------------------------------------------------------------------\n\n## What is a Two-Way ANOVA?\n\nExamines the impact of ***2*** nominal/categorical variables on a continuous outcome\n\nWe can now examine:\n\n-   The impact of variable 1 on the outcome (Main Effect)\n\n-   The impact of variable 2 on the outcome (Main Effect)\n\n-   The *interaction* of variable 1 & 2 on the outcome (Interaction Effect)\n\n    ::: incremental\n    -   The effect of variable 1 ***depends*** on the level of variable 2\n    :::\n\n------------------------------------------------------------------------\n\n## Two-Way ANOVA: Assumptions\n\nSame as what we've examined previously, plus a couple more:\n\n::: columns\n::: {.column width=\"50%\"}\n-   Independence\n\n-   Normality of Residuals\n\n-   Homoscedasticity (Homogeneity of Variance)\n:::\n\n::: {.column width=\"50%\" style=\"font-size: 30px\"}\n::: incremental\n-   Additivity\n\n    -   The effects of each factor are consistent across all levels of the other factor\n\n-   Multicollinearity\n\n    -   Correlations between factors. This can make it difficult to separate unique contributions to the outcome\n\n-   Equal Cell Sizes (N)\n:::\n:::\n:::\n\n------------------------------------------------------------------------\n\n## Main Effect & Interactions\n\nMain Effect: Basically a one-way ANOVA\n\n-   The effect of variable 1 is the same across all levels of variable 2\n\nInteraction:\n\n-   Able to examine the effect of variable 1 across different levels of variable 2\n\n-   Basically speaking, **the effect of variable 1 on our outcome *DEPENDS on the levels of variable 2***\n\n# Example Data\n\n------------------------------------------------------------------------\n\n## Data\n\n::: {style=\"font-size: 30px\"}\nWe are interested on the impact of phone usage on overall sleep quality\n\nWe include 2 variables of interest: 1) Phone Type (iOS vs. Android) and 2) Amount of usage (High, Medium & Low) to examine if there are differences in terms of sleep quality\n\n*Note: It is important to consider [HOW]{.underline} we operationalize constructs as some things we have as factors could easily be continuous variables*\n:::\n\n```{r}\n#| code-fold: true\n#Generate some data\n\n# Set random seed for reproducibility\nset.seed(42)\n\nn <- 500  # Set number of observations\n\n# Generate Type of Phone data\nphone_type <- sample(c(\"Android\", \"iOS\"), \n                     n, \n                     replace = TRUE)\n\n# Generate Phone Usage data\nphone_usage <- factor(sample(c(\"Low\", \"Medium\", \"High\"), \n                             n, \n                             replace = TRUE), \n                      levels= c(\"Low\", \"Medium\", \"High\"))\n\n# Generate Sleep Quality data (with some variation)\n# Intentionally inflating to highlight main effects\nsleep_quality <- round(\n  rnorm(n, mean = ifelse(phone_type == \"Android\", 5, 7) + ifelse(phone_usage == \"High\", 1, -1), sd = 1),\n  1\n)\n\n# Generate Sleep Quality data (with some variation)\nsleep_quality2 <- round(rnorm(n, mean = 6, sd = 3),1)\n\n# Create a data frame\nsleep_data <- data.frame(phone_type, \n                         phone_usage, \n                         sleep_quality,\n                         sleep_quality2)\n\nhead(sleep_data)\n```\n\n------------------------------------------------------------------------\n\n## Test Statistics\n\nWe've gone too far today without me showing some equations\n\nWith one way anova, we calculated the $SS_{between}$ and the $SS_{within}$ and were able to use those to capture the F-statistic\n\nNow we have another variable to take into account. Therefore, we need to calculate:\n\n$$\nSS_{between Group1}, \\: SS_{between Group2}\n$$\n\n$$\nSS_{within}, \\: SS_{total}\n$$\n\n------------------------------------------------------------------------\n\n## $F$-Statistic/Ratio\n\nHelpful site for hand calculations: ([link](https://www.statology.org/two-way-anova-by-hand/))\n\n-   Since we have these various sum of squares, we can then fill out the ANOVA table\n\n-   This also means that we will have *multiple* F-Statistics\n\n------------------------------------------------------------------------\n\n+---------------------+-------------+----------------------------------+-------------------------------------+---------------------------------+----------+\n| Source of Variation | df          | Sum of Squares                   | Mean Squares                        | F-statistic                     | p-value  |\n+:===================:+:===========:+:================================:+:===================================:+:===============================:+:========:+\n| Group1              | $j-1$       | $SS_{b1}$                        | $MS_{b1} = \\frac{SS_{b1}}{df_{b1}}$ | $F_1 = \\frac{MS_{b1}}{MS_{w1}}$ | $p_1$    |\n+---------------------+-------------+----------------------------------+-------------------------------------+---------------------------------+----------+\n| Group2              | $k-1$       | $SS_{b2}$                        | $MS_{b2} = \\frac{SS_{b2}}{df_{b2}}$ | $F_2 = \\frac{MS_{b2}}{MS_{w2}}$ | $p_2$    |\n+---------------------+-------------+----------------------------------+-------------------------------------+---------------------------------+----------+\n| Interaction         | $df_1*df_2$ | ::: {.fragment .highlight-green} | $MS_{b3} = \\frac{SS_{b3}}{df_{b3}}$ | $F_2 = \\frac{MS_{b3}}{MS_{w3}}$ | $p_3$    |\n|                     |             | $SS_{int}$                       |                                     |                                 |          |\n|                     |             | :::                              |                                     |                                 |          |\n+---------------------+-------------+----------------------------------+-------------------------------------+---------------------------------+----------+\n| Residual (within)   | $N-G$       | $SS_w$                           | $MS_w = \\frac{SS_w}{df_w}$          |                                 |          |\n+---------------------+-------------+----------------------------------+-------------------------------------+---------------------------------+----------+\n| Total               | $N-1$       | $SS_{total}$                     |                                     |                                 |          |\n+---------------------+-------------+----------------------------------+-------------------------------------+---------------------------------+----------+\n\n------------------------------------------------------------------------\n\n## Calculate $SS_{interaction}$\n\nNow we need to be able to take into account the interaction term\n\nThis is done by calculating all other $SS$ and then performing:\n\n$$\nSS_{interaction} = SS_{total} - SS_{b1} - SS_{b2} - SS_{w}\n$$\n\n# Example: Calculate the $SS$ for the new data\n\n::: incremental\n-   no...we aren't doing that\n:::\n\n------------------------------------------------------------------------\n\n# Running in R\n\nTake a peak at the dataset again so we can look at the variables and their names\n\n```{r}\nhead(sleep_data)\n```\n\n------------------------------------------------------------------------\n\n## Running in R\n\nWe will use the `aov()` function to set up our model\n\n```{r}\nfit2 <- aov(sleep_quality ~ phone_type + phone_usage + phone_type*phone_usage, \n            data = sleep_data)\n\n#OR \n\nfit2 <- aov(sleep_quality ~ phone_type * phone_usage, \n            data = sleep_data)\n\nsummary(fit2)\n\n```\n\n------------------------------------------------------------------------\n\n```{r}\n#| code-fold: true\nggplot(sleep_data, aes(x = phone_usage, \n                       y = sleep_quality, \n                       color = phone_type, \n                       group = phone_type)) +\n  geom_point() +\n  geom_smooth(method = \"lm\",\n              se=FALSE) +\n  labs(x = \"Phone Usage\", \n       y = \"Sleep Quality\", \n       color = \"Phone Type\") +\n  theme_minimal()\n```\n\n## Different Outcome\n\nCreated the outcome of `sleep_quality2` to be completely random instead of following a formula so that we could visualize the effect\n\n```{r}\nfit3 <- aov(sleep_quality2 ~ phone_type * phone_usage, \n            data = sleep_data)\nsummary(fit3)\n```\n\n------------------------------------------------------------------------\n\n```{r}\n# Create an interaction plot using ggplot2\nggplot(sleep_data, aes(x = phone_usage, y = sleep_quality2, color = phone_type, group = phone_type)) +\n  geom_point() +\n  geom_smooth(method = \"lm\",\n              se=FALSE) +\n  labs(x = \"Phone Usage\", y = \"Sleep Quality\", color = \"Phone Type\") +\n  theme_minimal()\n```\n\n# Next time...\n\n::: incremental\n-   ¯\\\\\\_(ツ)\\_/¯\n\n-   Correlation & Regression\n:::\n","srcMarkdownNoYaml":"\n\n```{r, include = F}\nknitr::opts_chunk$set(message = FALSE, warning = FALSE)\n```\n\n## Last Class\n\n::: nonincremental\n-   One Way ANOVA\n    -   Comparing means across multiple groups/levels\n:::\n\n------------------------------------------------------------------------\n\n## Looking Ahead\n\n-   R-Workshop! ([Link to Sign up](https://docs.google.com/forms/d/e/1FAIpQLSd2Gjcb88kWHBS7LXNDaLCZ9Sb7aroRceMjrLl5K36epewdtQ/viewform?usp=sf_link))\n    -   11/3 & 12/1 from 2-3pm\n-   Professor will put together guidelines for the final project and have dates to complete various components\n-   Lab 3 - Starting Today will be due before class on 11/6\n    -   Will be able to resubmit for any missed items up to 1 week from when I post the initial feedback\n-   Correlation & Regression\n\n------------------------------------------------------------------------\n\n## Today...\n\nOne-Way ANOVA\n\n-   Contrasts/post-hoc tests\n\nTwo-Way ANOVA\n\n```{r, results = 'hide', message = F, warning = F}\n# File management\nlibrary(here)\n# for dplyr, ggplot2\nlibrary(tidyverse)\n#Loading data\nlibrary(rio)\n# Estimating Marginal Means\nlibrary(emmeans)\n# Pretty Tables\nlibrary(kableExtra)\n\n#Remove Scientific Notation \noptions(scipen=999)\n```\n\n# Review of One-Way ANOVA\n\n## One-Way ANOVA\n\nGoal: Inform of differences among the levels of our variable of interest (Omnibus Test)\n\nUsing the between and within group variance to create the $F$-statistic/ratio\n\nHypotheses:\n\n$$\nH_0: it\\: is\\: true\\: that\\: \\mu_1 = \\mu_2 = \\mu_3 =\\: ...\\mu_k \n\\\\ \nH_1: it\\: is\\: \\boldsymbol{not}\\: true\\: that\\: \\mu_1 = \\mu_2 = \\mu_3 =\\: ...\\mu_k\n$$\n\n------------------------------------------------------------------------\n\n![](/images/ANOVA.png){width=\"729\"}\n\n::: columns\n::: {.column width=\"50%\"}\n$F = \\frac{MS_{between}}{MS_{within}} = \\frac{small}{large} < 1$\n:::\n\n::: {.column width=\"50%\"}\n$F = \\frac{MS_{between}}{MS_{within}} = \\frac{large}{small} > 1$\n:::\n:::\n\n## Review of the NHST process\n\n1.  Collect Sample and define hypotheses\n\n2.  Set alpha level\n\n3.  Determine the sampling distribution (will be using $F$-distribution now)\n\n4.  Identify the critical value\n\n5.  Calculate test statistic for sample collected\n\n6.  Inspect & compare statistic to critical value; Calculate probability\n\n## Steps to calculating F-ratio\n\n1.  Variance to Sum of Squares (Between & Within)\n2.  Degrees of Freedom\n3.  Mean squares values\n4.  F-Statistic\n\n## Calculating the F-Statistic\n\n$$F = \\frac{MS_b}{MS_w}$$\n\nIf the null hypothesis is true, $F$ has an expected value close to 1 (numerator and denominator are estimates of the same variability)\n\nIf it is false, the numerator will likely be larger, because systematic, between-group differences contribute to the variance of the means, but not to variance within group.\n\n------------------------------------------------------------------------\n\n```{r fig.width = 10, fig.height = 6}\n#| code-fold: true\n#| \ndata.frame(F = c(0,8)) %>%\n  ggplot(aes(x = F)) +\n  stat_function(fun = function(x) df(x, df1 = 3, df2 = 196), \n                geom = \"line\") +\n  stat_function(fun = function(x) df(x, df1 = 3, df2 = 196), \n                geom = \"area\", xlim = c(2.65, 8), fill = \"purple\") +\n  geom_vline(aes(xintercept = 2.65), color = \"purple\") +\n  scale_y_continuous(\"Density\") + scale_x_continuous(\"F statistic\", breaks = NULL) +\n  theme_bw(base_size = 20)\n```\n\nIf data are normally distributed, then the variance is $\\chi^2$ distributed\n\n$F$-distributions are one-tailed tests. Recall that we're interested in how far away our test statistic from the null $(F = 1).$\n\n------------------------------------------------------------------------\n\n## ANOVA table\n\n+---------------------+----------+----------------+----------------------------+-------------------------+----------+\n| Source of Variation | df       | Sum of Squares | Mean Squares               | F-statistic             | p-value  |\n+:===================:+:========:+:==============:+:==========================:+:=======================:+:========:+\n| Group               | $G-1$    | $SS_b$         | $MS_b = \\frac{SS_b}{df_b}$ | $F = \\frac{MS_b}{MS_w}$ | $p$      |\n+---------------------+----------+----------------+----------------------------+-------------------------+----------+\n| Residual            | $N-G$    | $SS_w$         | $MS_w = \\frac{SS_w}{df_w}$ |                         |          |\n+---------------------+----------+----------------+----------------------------+-------------------------+----------+\n| Total               | $N-1$    | $SS_{total}$   |                            |                         |          |\n+---------------------+----------+----------------+----------------------------+-------------------------+----------+\n\n# Contrasts/Post-Hoc\n\n------------------------------------------------------------------------\n\n## Contrasts/Post-Hoc Tests\n\nPerformed when there is a significant difference among the groups to examine which groups are different\n\n1.  **Contrasts**: When we have *a priori* hypotheses\n2.  **Post-hoc Tests**: When we want to test everything\n\n------------------------------------------------------------------------\n\nThese comparisons take the general form of t-tests, but note some extensions:\n\n-   the pooled variance estimate comes from $SS_{\\text{residual}}$, meaning it pulls information from all groups\n\n-   the degrees of freedom for the $t$-test is $N-k$, so using all data\n\n------------------------------------------------------------------------\n\n## Previous Spooky Data\n\nEMF rating across multiple locations\n\n```{r}\nspooky <- import(\"https://raw.githubusercontent.com/dharaden/dharaden.github.io/main/courses/PSYC640_fall23/data/SS%20Calculations.csv\") %>% \n  select(Location, EMF) #There were some extra empty variables in there that we don't care about\n\nfit_1 <- aov(EMF ~ Location, data = spooky)\nsummary(fit_1)\n\n```\n\n------------------------------------------------------------------------\n\n## Post-hoc tests\n\n```{r}\n#library(emmeans)\nemmeans(fit_1, pairwise ~ Location, adjust = \"none\")\n```\n\n------------------------------------------------------------------------\n\n## Family-wise error\n\nThese pairwise comparisons can quickly grow in number as the number of Groups increases. With 8 (k) Groups, we have k(k-1)/2 = 28 possible pairwise comparisons.\n\nAs the number of groups in the ANOVA grows, the number of possible pairwise comparisons increases dramatically.\n\n```{r,echo = F, fig.width = 10, fig.height = 5.5}\ndata.frame(g = 2:15) %>%\n  mutate(num = (g*(g-1))/2) %>%\n  ggplot(aes(x = g, y = num)) +\n  geom_line(size = 1.5) +\n  geom_point(size = 3)+\n  scale_x_continuous(\"Number of Groups in the ANOVA\") +\n  scale_y_continuous(\"Number of Pairwise Comparisons\") +\n  theme_bw()\n```\n\n------------------------------------------------------------------------\n\nAs the number of tests grows, and assuming the null hypothesis is true, the probability that we will make one or more Type I errors increases. To approximate the magnitude of the problem, we can assume that the multiple pairwise comparisons are independent. The probability that we **don't** make a Type I error for [**one**]{.underline} test is:\n\n$$P(\\text{No Type I}, 1 \\text{ test}) = 1-\\alpha$$\n\n------------------------------------------------------------------------\n\nThe probability that we don't make a Type I error for [**two**]{.underline} tests is:\n\n$$P(\\text{No Type I}, 2 \\text{ test}) = (1-\\alpha)(1-\\alpha)$$\n\nFor C tests, the probability that we make **no** Type I errors is\n\n$$P(\\text{No Type I}, C \\text{ tests}) = (1-\\alpha)^C$$\n\nWe can then use the following to calculate the probability that we make one or more Type I errors in a collection of C independent tests.\n\n$$P(\\text{At least one Type I}, C \\text{ tests}) = 1-(1-\\alpha)^C$$\n\n------------------------------------------------------------------------\n\nThe Type I error inflation that accompanies multiple comparisons motivates the large number of \"correction\" procedures that have been developed.\n\n```{r,echo = F, fig.width = 10, fight.height = 6}\ndata.frame(g = 2:15) %>%\n  mutate(num = (g*(g-1))/2,\n         p_notype1 = (1-.05)^num,\n         p_type1 = 1-p_notype1) %>%\n  ggplot(aes(x = g, y = p_type1)) +\n  geom_line(size = 1.5) +\n  geom_point(size = 3)+\n  scale_x_continuous(\"Number of Groups in the ANOVA\") +\n  scale_y_continuous(\"Probability of a Type I Error\") +\n  theme_bw()\n```\n\n------------------------------------------------------------------------\n\nMultiple comparisons, each tested with $\\alpha_{per-test}$, increases the family-wise $\\alpha$ level.\n\n$$\\large \\alpha_{family-wise} = 1 - (1-\\alpha_{per-test})^C$$ Šidák showed that the family-wise a could be controlled to a desired level (e.g., .05) by changing the $\\alpha_{per-test}$ to:\n\n$$\\large \\alpha_{per-wise} = 1 - (1-\\alpha_{family-wise})^{\\frac{1}{C}}$$\n\n------------------------------------------------------------------------\n\n### Bonferroni\n\nBonferroni (and Dunn, and others) suggested this simple approximation:\n\n$$\\large \\alpha_{per-test} = \\frac{\\alpha_{family-wise}}{C}$$\n\nThis is typically called the Bonferroni correction and is very often used even though better alternatives are available.\n\n------------------------------------------------------------------------\n\n```{r}\nemmeans(fit_1, pairwise ~ Location, adjust = \"bonferroni\")\n```\n\n------------------------------------------------------------------------\n\nThe Bonferroni procedure is conservative. Other correction procedures have been developed that control family-wise Type I error at .05 but that are more powerful than the Bonferroni procedure. The most common one is the Holm procedure.\n\nThe Holm procedure does not make a constant adjustment to each per-test $\\alpha$. Instead it makes adjustments in stages depending on the relative size of each pairwise p-value.\n\n------------------------------------------------------------------------\n\n### Holm correction\n\n1.  Rank order the p-values from largest to smallest.\n2.  Start with the smallest p-value. Multiply it by its rank.\n3.  Go to the next smallest p-value. Multiply it by its rank. If the result is larger than the adjusted p-value of next smallest rank, keep it. Otherwise replace with the previous step adjusted p-value.\n4.  Repeat Step 3 for the remaining p-values.\n5.  Judge significance of each new p-value against $\\alpha = .05$.\n\n------------------------------------------------------------------------\n\n```{r,echo = F, results = 'asis'}\n\ndata.frame(or = c(.0012, .0023, .0450, .0470, .0530, .2100),\n           rank = c(6:1)) %>%\n  mutate(rankp = or*rank,\n         holm = p.adjust(or, method = \"holm\"),\n         bon = p.adjust(or, method = \"bonferroni\")) %>%\n  kable(., \n        col.names = c(\"Original p value\", \"Rank\", \n                      \"Rank x p\", \"Holm\", \"Bonferroni\")) \n```\n\n------------------------------------------------------------------------\n\n```{r}\nemmeans(fit_1, pairwise ~ Location, adjust = \"holm\")\n```\n\n# ANOVA is regression\n\nIn regression, we can accommodate categorical predictors. How does this compare to ANOVA?\n\n-   Same omnibus test of the model!\n\n\\*(Really the same model, but packaged differently.)\n\n-   When would you use one versus the other?\n\n------------------------------------------------------------------------\n\n::: columns\n::: {.column width=\"50%\"}\n**ANOVA**\n\n-   More traditional for 3+ groups\n\n-   Comparing/controlling multiple categorical variables\n:::\n\n::: {.column width=\"50%\"}\n**Regression**\n\n-   Best for two groups\n\n-   Incorporating continuous predictors too\n\n-   Good for 3+ groups when you have more specific hypotheses (contrasts)\n:::\n:::\n\n# Two-Way ANOVA\n\n------------------------------------------------------------------------\n\n## What is a Two-Way ANOVA?\n\nExamines the impact of ***2*** nominal/categorical variables on a continuous outcome\n\nWe can now examine:\n\n-   The impact of variable 1 on the outcome (Main Effect)\n\n-   The impact of variable 2 on the outcome (Main Effect)\n\n-   The *interaction* of variable 1 & 2 on the outcome (Interaction Effect)\n\n    ::: incremental\n    -   The effect of variable 1 ***depends*** on the level of variable 2\n    :::\n\n------------------------------------------------------------------------\n\n## Two-Way ANOVA: Assumptions\n\nSame as what we've examined previously, plus a couple more:\n\n::: columns\n::: {.column width=\"50%\"}\n-   Independence\n\n-   Normality of Residuals\n\n-   Homoscedasticity (Homogeneity of Variance)\n:::\n\n::: {.column width=\"50%\" style=\"font-size: 30px\"}\n::: incremental\n-   Additivity\n\n    -   The effects of each factor are consistent across all levels of the other factor\n\n-   Multicollinearity\n\n    -   Correlations between factors. This can make it difficult to separate unique contributions to the outcome\n\n-   Equal Cell Sizes (N)\n:::\n:::\n:::\n\n------------------------------------------------------------------------\n\n## Main Effect & Interactions\n\nMain Effect: Basically a one-way ANOVA\n\n-   The effect of variable 1 is the same across all levels of variable 2\n\nInteraction:\n\n-   Able to examine the effect of variable 1 across different levels of variable 2\n\n-   Basically speaking, **the effect of variable 1 on our outcome *DEPENDS on the levels of variable 2***\n\n# Example Data\n\n------------------------------------------------------------------------\n\n## Data\n\n::: {style=\"font-size: 30px\"}\nWe are interested on the impact of phone usage on overall sleep quality\n\nWe include 2 variables of interest: 1) Phone Type (iOS vs. Android) and 2) Amount of usage (High, Medium & Low) to examine if there are differences in terms of sleep quality\n\n*Note: It is important to consider [HOW]{.underline} we operationalize constructs as some things we have as factors could easily be continuous variables*\n:::\n\n```{r}\n#| code-fold: true\n#Generate some data\n\n# Set random seed for reproducibility\nset.seed(42)\n\nn <- 500  # Set number of observations\n\n# Generate Type of Phone data\nphone_type <- sample(c(\"Android\", \"iOS\"), \n                     n, \n                     replace = TRUE)\n\n# Generate Phone Usage data\nphone_usage <- factor(sample(c(\"Low\", \"Medium\", \"High\"), \n                             n, \n                             replace = TRUE), \n                      levels= c(\"Low\", \"Medium\", \"High\"))\n\n# Generate Sleep Quality data (with some variation)\n# Intentionally inflating to highlight main effects\nsleep_quality <- round(\n  rnorm(n, mean = ifelse(phone_type == \"Android\", 5, 7) + ifelse(phone_usage == \"High\", 1, -1), sd = 1),\n  1\n)\n\n# Generate Sleep Quality data (with some variation)\nsleep_quality2 <- round(rnorm(n, mean = 6, sd = 3),1)\n\n# Create a data frame\nsleep_data <- data.frame(phone_type, \n                         phone_usage, \n                         sleep_quality,\n                         sleep_quality2)\n\nhead(sleep_data)\n```\n\n------------------------------------------------------------------------\n\n## Test Statistics\n\nWe've gone too far today without me showing some equations\n\nWith one way anova, we calculated the $SS_{between}$ and the $SS_{within}$ and were able to use those to capture the F-statistic\n\nNow we have another variable to take into account. Therefore, we need to calculate:\n\n$$\nSS_{between Group1}, \\: SS_{between Group2}\n$$\n\n$$\nSS_{within}, \\: SS_{total}\n$$\n\n------------------------------------------------------------------------\n\n## $F$-Statistic/Ratio\n\nHelpful site for hand calculations: ([link](https://www.statology.org/two-way-anova-by-hand/))\n\n-   Since we have these various sum of squares, we can then fill out the ANOVA table\n\n-   This also means that we will have *multiple* F-Statistics\n\n------------------------------------------------------------------------\n\n+---------------------+-------------+----------------------------------+-------------------------------------+---------------------------------+----------+\n| Source of Variation | df          | Sum of Squares                   | Mean Squares                        | F-statistic                     | p-value  |\n+:===================:+:===========:+:================================:+:===================================:+:===============================:+:========:+\n| Group1              | $j-1$       | $SS_{b1}$                        | $MS_{b1} = \\frac{SS_{b1}}{df_{b1}}$ | $F_1 = \\frac{MS_{b1}}{MS_{w1}}$ | $p_1$    |\n+---------------------+-------------+----------------------------------+-------------------------------------+---------------------------------+----------+\n| Group2              | $k-1$       | $SS_{b2}$                        | $MS_{b2} = \\frac{SS_{b2}}{df_{b2}}$ | $F_2 = \\frac{MS_{b2}}{MS_{w2}}$ | $p_2$    |\n+---------------------+-------------+----------------------------------+-------------------------------------+---------------------------------+----------+\n| Interaction         | $df_1*df_2$ | ::: {.fragment .highlight-green} | $MS_{b3} = \\frac{SS_{b3}}{df_{b3}}$ | $F_2 = \\frac{MS_{b3}}{MS_{w3}}$ | $p_3$    |\n|                     |             | $SS_{int}$                       |                                     |                                 |          |\n|                     |             | :::                              |                                     |                                 |          |\n+---------------------+-------------+----------------------------------+-------------------------------------+---------------------------------+----------+\n| Residual (within)   | $N-G$       | $SS_w$                           | $MS_w = \\frac{SS_w}{df_w}$          |                                 |          |\n+---------------------+-------------+----------------------------------+-------------------------------------+---------------------------------+----------+\n| Total               | $N-1$       | $SS_{total}$                     |                                     |                                 |          |\n+---------------------+-------------+----------------------------------+-------------------------------------+---------------------------------+----------+\n\n------------------------------------------------------------------------\n\n## Calculate $SS_{interaction}$\n\nNow we need to be able to take into account the interaction term\n\nThis is done by calculating all other $SS$ and then performing:\n\n$$\nSS_{interaction} = SS_{total} - SS_{b1} - SS_{b2} - SS_{w}\n$$\n\n# Example: Calculate the $SS$ for the new data\n\n::: incremental\n-   no...we aren't doing that\n:::\n\n------------------------------------------------------------------------\n\n# Running in R\n\nTake a peak at the dataset again so we can look at the variables and their names\n\n```{r}\nhead(sleep_data)\n```\n\n------------------------------------------------------------------------\n\n## Running in R\n\nWe will use the `aov()` function to set up our model\n\n```{r}\nfit2 <- aov(sleep_quality ~ phone_type + phone_usage + phone_type*phone_usage, \n            data = sleep_data)\n\n#OR \n\nfit2 <- aov(sleep_quality ~ phone_type * phone_usage, \n            data = sleep_data)\n\nsummary(fit2)\n\n```\n\n------------------------------------------------------------------------\n\n```{r}\n#| code-fold: true\nggplot(sleep_data, aes(x = phone_usage, \n                       y = sleep_quality, \n                       color = phone_type, \n                       group = phone_type)) +\n  geom_point() +\n  geom_smooth(method = \"lm\",\n              se=FALSE) +\n  labs(x = \"Phone Usage\", \n       y = \"Sleep Quality\", \n       color = \"Phone Type\") +\n  theme_minimal()\n```\n\n## Different Outcome\n\nCreated the outcome of `sleep_quality2` to be completely random instead of following a formula so that we could visualize the effect\n\n```{r}\nfit3 <- aov(sleep_quality2 ~ phone_type * phone_usage, \n            data = sleep_data)\nsummary(fit3)\n```\n\n------------------------------------------------------------------------\n\n```{r}\n# Create an interaction plot using ggplot2\nggplot(sleep_data, aes(x = phone_usage, y = sleep_quality2, color = phone_type, group = phone_type)) +\n  geom_point() +\n  geom_smooth(method = \"lm\",\n              se=FALSE) +\n  labs(x = \"Phone Usage\", y = \"Sleep Quality\", color = \"Phone Type\") +\n  theme_minimal()\n```\n\n# Next time...\n\n::: incremental\n-   ¯\\\\\\_(ツ)\\_/¯\n\n-   Correlation & Regression\n:::\n"},"formats":{"revealjs":{"identifier":{"display-name":"RevealJS","target-format":"revealjs","base-format":"revealjs"},"execute":{"fig-width":10,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":false,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"wrap","code-link":false,"code-line-numbers":true,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","html-math-method":{"method":"mathjax","url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML-full"},"slide-level":2,"to":"revealjs","incremental":false,"output-file":"10_ANOVA2.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":false,"quarto-version":"1.3.433","auto-stretch":true,"editor":"visual","title":"Two-Way ANOVA","subtitle":"PSYC 640 - Fall 2023","author":"Dustin Haraden, PhD","editor_options":{"chunk_output_type":"console"},"multiplex":true,"scrollable":true,"slideNumber":true,"touch":true,"theme":"dark"}}},"projectFormats":["html"]}