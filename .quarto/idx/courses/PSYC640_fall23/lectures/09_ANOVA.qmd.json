{"title":"Introduction to ANOVA","markdown":{"yaml":{"title":"Introduction to ANOVA","subtitle":"PSYC 640 - Fall 2023","author":"Dustin Haraden, PhD","format":{"revealjs":{"multiplex":true,"scrollable":true,"slide-number":true,"incremental":false,"touch":true,"code-overflow":"wrap","theme":"dark"}},"execute":{"echo":true},"editor":"visual","editor_options":{"chunk_output_type":"console"}},"headingText":"Last Week","containsRefs":false,"markdown":"\n\n```{r, include = F}\nknitr::opts_chunk$set(message = FALSE, warning = FALSE)\n```\n\n\n::: nonincremental\n-   Comparing Means with 2 groups: $t$-test\n    -   Independent Samples $t$-test Review\n    -   Paired Samples $t$-test\n:::\n\n------------------------------------------------------------------------\n\n## Looking Ahead\n\n-   Plan to have 2 more labs that will be similar to the last lab\n\n    -   Likely take place on 10/25 and sometime the week of 11/13\n\n-   Outside of these labs, I am going to plan on having additional mini-labs\n\n    -   Likely to take place on 11/1, 11/22 and 11/29 (will update based on how things are going in class)\n\n------------------------------------------------------------------------\n\n## Today...\n\nIntroduction to ANOVA (Analysis of Variance)\n\n```{r, results = 'hide', message = F, warning = F}\n# File management\nlibrary(here)\n# for dplyr, ggplot2\nlibrary(tidyverse)\n#Loading data\nlibrary(rio)\n\n#Remove Scientific Notation \noptions(scipen=999)\n```\n\n# Overview of ANOVA\n\n## What is ANOVA? ([LSR Ch. 14](https://learningstatisticswithr.com/book/anova.html))\n\n::: incremental\n-   ANOVA stands for [***An***]{.underline}alysis [***o***]{.underline}f [***Va***]{.underline}riance\n-   Comparing means between two or more groups (usually 3 or more)\n    -   Continuous outcome and grouping variable with 2 or more levels\n-   Under the larger umbrella of general linear models\n    -   ANOVA is basically a regression with only categorical predictors\n-   Likely the most widely used tool in Psychology\n:::\n\n## Different Types of ANOVA\n\n-   ::: {.fragment .highlight-green}\n    One-Way ANOVA\n    :::\n\n-   Two-Way ANOVA\n\n-   Repeated Measures ANOVA\n\n-   ANCOVA\n\n-   MANOVA\n\n## One-Way ANOVA\n\nGoal: Inform of differences among the levels of our variable of interest (Omnibus Test)\n\n-   But cannot tell us more than that...\n\nHypotheses:\n\n$$\nH_0: it\\: is\\: true\\: that\\: \\mu_1 = \\mu_2 = \\mu_3 =\\: ...\\mu_k \n\\\\ \nH_1: it\\: is\\: \\boldsymbol{not}\\: true\\: that\\: \\mu_1 = \\mu_2 = \\mu_3 =\\: ...\\mu_k\n$$\n\n## Wait...Means or Variance?\n\nWe are using the variance to create a ratio (within group versus between group variance) to determine differences in means\n\n-   We are not directly investigating variance, but operationalize variance to create the ratio:\n\n$$\nF_{df_b, \\: df_w} = \\frac{MS_{between}}{MS_{within}}\n$$\n\n------------------------------------------------------------------------\n\n![](/images/ANOVA.png){width=\"729\"}\n\n------------------------------------------------------------------------\n\n![](/images/ANOVA.png){width=\"729\"}\n\n::: columns\n::: {.column width=\"50%\"}\n$F = \\frac{MS_{between}}{MS_{within}} = \\frac{small}{large} < 1$\n:::\n\n::: {.column width=\"50%\"}\n$F = \\frac{MS_{between}}{MS_{within}} = \\frac{large}{small} > 1$\n:::\n:::\n\n## ANOVA: Assumptions\n\n::: {style=\"font-size: 30px\"}\n**Independence**\n\n-   Observations between and within groups should be independent. No autocorrelation\n\n**Homogeneity of Variance**\n\n-   The variances within each group should be roughly equal\n    -   Levene's test --\\> Welch's ANOVA for unequal variances\n\n**Normality**\n\n-   The data within each group should follow a normal distribution\n    -   Shapiro-Wilk test --\\> can transform the data or use non-parametric tests\n:::\n\n# NHST with ANOVA\n\n## Review of the NHST process\n\n::: incremental\n1.  Collect Sample and define hypotheses\n\n2.  Set alpha level\n\n3.  Determine the sampling distribution (will be using $F$-distribution now)\n\n4.  Identify the critical value\n\n5.  Calculate test statistic for sample collected\n\n6.  Inspect & compare statistic to critical value; Calculate probability\n:::\n\n## Steps to calculating F-ratio\n\n1.  Capture variance both between and within groups\n2.  Variance to Sum of Squares\n3.  Degrees of Freedom\n4.  Mean squares values\n5.  F-Statistic\n\n## Capturing Variance\n\nWe have calculated variance before!\n\n$$\nVar = \\frac{1}{N}\\sum(x_i - \\bar{x})^2\n$$\n\nNow we have to take into account the variance between and within the groups:\n\n$$\nVar(Y) = \\frac{1}{N} \\sum^G_{k=1}\\sum^{N_k}_{i=i}(Y_{ik} - \\bar{Y})^2\n$$\n\n::: {style=\"font-size: 20px; text-align: center\"}\nNotice that we have the summation across each group ( $G$ ) and the person in the group ( $N_k$ )\n:::\n\n------------------------------------------------------------------------\n\n## Variance to Sum of Squares\n\n**Total Sum of Squares -** Adding up the sum of squares instead of getting the average (notice the removal of $\\frac{1}{N}$)\n\n$$\nSS_{total} = \\sum^G_{k=1}\\sum^{N_k}_{i=i}(Y_{ik} - \\bar{Y})^2\n$$\n\nCan be broken up to see what is the variation ***between*** the groups AND the variation ***within*** the groups\n\n$$\nSS_{total}=SS_{between}+SS_{within}\n$$\n\n::: {style=\"font-size: 20px; text-align: center\"}\nThis gets us closer to understanding the difference between means\n:::\n\n# Sum of Squares\n\n------------------------------------------------------------------------\n\n### Sum of Squares\n\n$$\nSS_{total}=SS_{between}+SS_{within}\n$$\n\n![](/images/var_ss.PNG){fig-align=\"center\"}\n\n------------------------------------------------------------------------\n\n### Example Data\n\n![](/images/table1.PNG){fig-align=\"center\"}\n\n------------------------------------------------------------------------\n\n### Example Data\n\n![](/images/table2.PNG){fig-align=\"center\"}\n\n------------------------------------------------------------------------\n\n### Example Data\n\n![](/images/table3.PNG){fig-align=\"center\"}\n\n------------------------------------------------------------------------\n\n### Example Data\n\n![](/images/table4.PNG){fig-align=\"center\"}\n\n------------------------------------------------------------------------\n\n## Sum of Squares - Between\n\nThe difference between the *group mean* and *grand mean*\n\n$$\nSS_{between} = \\sum^G_{k=1}N_k(\\bar{Y_k} - \\bar{Y})^2\n$$\n\n| Group  | Group Mean $\\bar{Y_k}$ | Grand Mean $\\bar{Y}$ |\n|:------:|:----------------------:|:--------------------:|\n|  Cool  |           32           |         41.8         |\n| Uncool |          56.5          |         41.8         |\n\n------------------------------------------------------------------------\n\n## Sum of Squares - Between\n\nThe difference between the *group mean* and *grand mean*\n\n$$ SS_{between} = \\sum^G_{k=1}N_k(\\bar{Y_k} - \\bar{Y})^2 $$\n\n| Group  | Group Mean $\\bar{Y_k}$ | Grand Mean $\\bar{Y}$ | Sq. Dev. |  N  | Weighted Sq. Dev. |\n|:------:|:----------------------:|:--------------------:|:--------:|:---:|:-----------------:|\n|  Cool  |           32           |         41.8         |  96.04   |  3  |      288.12       |\n| Uncool |          56.5          |         41.8         |  216.09  |  2  |      432.18       |\n\n------------------------------------------------------------------------\n\n## Sum of Squares - Between\n\nThe difference between the *group mean* and *grand mean*\n\n$$ SS_{between} = \\sum^G_{k=1}N_k(\\bar{Y_k} - \\bar{Y})^2 $$\n\nNow we can sum the Weighted Squared Deviations together to get our Sum of Squares Between:\n\n```{r}\nssb <- 288.12 + 432.18\nssb\n```\n\n------------------------------------------------------------------------\n\n## Sum of Squares - Within\n\nThe difference between the [*individual*]{.underline} and their [*group mean*]{.underline}\n\n$$\nSS_{within} = \\sum^G_{k=1}\\sum^{N_k}_{i=i}(Y_{ik} - \\bar{Y_k})^2\n$$\n\n|    Name    | Grumpiness $Y_{ik}$ | Group Mean $\\bar{Y_K}$ |\n|:----------:|:-------------------:|:----------------------:|\n|   Frodo    |         20          |           32           |\n|    Sam     |         55          |           32           |\n|   Bandit   |         21          |           32           |\n| Dolores U. |         91          |          56.5          |\n|   Dustin   |         22          |          56.5          |\n\n------------------------------------------------------------------------\n\n## Sum of Squares - Within\n\nThe difference between the [*individual*]{.underline} and their [*group mean*]{.underline}\n\n$$ SS_{within} = \\sum^G_{k=1}\\sum^{N_k}_{i=i}(Y_{ik} - \\bar{Y_k})^2 $$\n\n|    Name    | Grumpiness $Y_{ik}$ | Group Mean $\\bar{Y_K}$ | Sq. Dev |\n|:----------:|:-------------------:|:----------------------:|---------|\n|   Frodo    |         20          |           32           | 144     |\n|    Sam     |         55          |           32           | 529     |\n|   Bandit   |         21          |           32           | 121     |\n| Dolores U. |         91          |          56.5          | 1190.25 |\n|   Dustin   |         22          |          56.5          | 1190.25 |\n\n```{r}\n#| code-fold: true\nscore <- c(20, 55, 21, 91, 22)\ngroup_m <- c(32, 32, 32, 56.5, 56.5)\nsq_dev <- (score - group_m)^2\n```\n\n------------------------------------------------------------------------\n\n## Sum of Squares - Within\n\nThe difference between the [*individual*]{.underline} and their [*group mean*]{.underline}\n\n$$ SS_{within} = \\sum^G_{k=1}\\sum^{N_k}_{i=i}(Y_{ik} - \\bar{Y_k})^2 $$ Now we can sum the Squared Deviations together to get our Sum of Squares Within:\n\n```{r}\nsum(sq_dev)\n```\n\n## Sum of Squares\n\nCan start to have an idea of what this looks like\n\n$$\nSS_{between} = \\sum^G_{k=1}N_k(\\bar{Y_k} - \\bar{Y})^2 = 720.3\n$$\n\n$$\nSS_{within} = \\sum^G_{k=1}\\sum^{N_k}_{i=i}(Y_{ik} - \\bar{Y_k})^2 = 3174.5\n$$\n\nNext we have to take into account the degrees of freedom\n\n# Degrees of Freedom - ANOVA\n\n------------------------------------------------------------------------\n\n## Degrees of Freedom\n\nSince we have 2 types of variations that we are examining, this needs to be reflected in the degrees of freedom\n\n1.  Take the number of groups and subtract 1\\\n    $df_{between} = G - 1$\n\n2.  Take the total number of observations and subtract the number of groups\\\n\n    $df_{within} = N - G$\n\n# Mean Squares\n\n------------------------------------------------------------------------\n\n## Calculating Mean Squares\n\nNext we convert our summed squares value into a \"mean squares\"\n\nThis is done by dividing by the respective degrees of freedom\n\n$$\nMS_b = \\frac{SS_b}{df_b}\n$$\n\n$$\nMS_W = \\frac{SS_w}{df_w}\n$$\n\n------------------------------------------------------------------------\n\n## Calculating Mean Squares - Example\n\nLet's take a look at how this applies to our example: $$ MS_b = \\frac{SS_b}{G-1} = \\frac{720.3}{2-1} = 720.3 $$\n\n$$ MS_W = \\frac{SS_w}{N-G} = \\frac{3174.5}{5-2} = 1058.167  $$\n\n# $F$-Statistic\n\n------------------------------------------------------------------------\n\n## Calculating the F-Statistic\n\n$$F = \\frac{MS_b}{MS_w}$$\n\nIf the null hypothesis is true, $F$ has an expected value close to 1 (numerator and denominator are estimates of the same variability)\n\nIf it is false, the numerator will likely be larger, because systematic, between-group differences contribute to the variance of the means, but not to variance within group.\n\n------------------------------------------------------------------------\n\n```{r fig.width = 10, fig.height = 6}\n#| code-fold: true\n#| \ndata.frame(F = c(0,8)) %>%\n  ggplot(aes(x = F)) +\n  stat_function(fun = function(x) df(x, df1 = 3, df2 = 196), \n                geom = \"line\") +\n  stat_function(fun = function(x) df(x, df1 = 3, df2 = 196), \n                geom = \"area\", xlim = c(2.65, 8), fill = \"purple\") +\n  geom_vline(aes(xintercept = 2.65), color = \"purple\") +\n  scale_y_continuous(\"Density\") + scale_x_continuous(\"F statistic\", breaks = NULL) +\n  theme_bw(base_size = 20)\n```\n\nIf data are normally distributed, then the variance is $\\chi^2$ distributed\n\n$F$-distributions are one-tailed tests. Recall that we're interested in how far away our test statistic from the null $(F = 1).$\n\n------------------------------------------------------------------------\n\n## Calculating F-statistic: Example\n\n$$F = \\frac{MS_b}{MS_w} = \\frac{720.3}{1058.167} = 0.68$$\n\n[Link](http://courses.atlas.illinois.edu/spring2016/STAT/STAT200/pf.html) to probability calculator\n\n------------------------------------------------------------------------\n\n```{r fig.width = 10, fig.height = 6}\n#| code-fold: true\n#| \ndata.frame(F = c(0,8)) %>%\n  ggplot(aes(x = F)) +\n  stat_function(fun = function(x) df(x, df1 = 3, df2 = 196), \n                geom = \"line\") +\n  stat_function(fun = function(x) df(x, df1 = 3, df2 = 196), \n                geom = \"area\", xlim = c(2.65, 8), fill = \"purple\") +\n  geom_vline(aes(xintercept = 2.65), color = \"purple\") + \n  geom_vline(aes(xintercept = 0.68), color = \"red\") + \n  annotate(\"text\",\n           label = \"F=0.68\", \n           x = 1.1, y = 0.65, size = 8, color = \"red\") + \n  scale_y_continuous(\"Density\") + scale_x_continuous(\"F statistic\", breaks = NULL) +\n  theme_bw(base_size = 20)\n```\n\nWhat can we conclude?\n\n------------------------------------------------------------------------\n\n## Contrasts/Post-Hoc Tests\n\nPerformed when there is a significant difference among the groups to examine which groups are different\n\n1.  **Contrasts**: When we have *a priori* hypotheses\n2.  **Post-hoc Tests**: When we want to test everything\n\n# Reporting Results\n\n------------------------------------------------------------------------\n\n## Tables\n\nOften times the output will be in the form of a table and then it is often reported this way in the manuscript\n\n| Source of Variation |  df   | Sum of Squares |        Mean Squares        |       F-statistic       | p-value |\n|:-------------------:|:-----:|:--------------:|:--------------------------:|:-----------------------:|:-------:|\n|        Group        | $G-1$ |     $SS_b$     | $MS_b = \\frac{SS_b}{df_b}$ | $F = \\frac{MS_b}{MS_w}$ |   $p$   |\n|      Residual       | $N-G$ |     $SS_w$     | $MS_w = \\frac{SS_w}{df_w}$ |                         |         |\n|        Total        | $N-1$ |  $SS_{total}$  |                            |                         |         |\n\n------------------------------------------------------------------------\n\n## In-Text\n\n> A one-way analysis of variance was used to test for differences in the \\[variable of interest/outcome variable\\] as a function of \\[whatever the factor is\\]. Specifically, differences in \\[variable of interest\\] were assessed for the \\[list different levels and be sure to include (M= , SD= )\\] . The one-way ANOVA revealed a significant/nonsignificant effect of \\[factor\\] on scores on the \\[variable of interest\\] (F(dfb, dfw) = f-ratio, p = p-value, η2 = effect size).\n>\n> Planned comparisons were conducted to compare expected differences among the \\[however many groups\\] means. Planned contrasts revealed that participants in the \\[one of the conditions\\] had a greater/fewer \\[variable of interest\\] and then include the p-value. This same type of sentence is repeated for whichever contrasts you completed. Descriptive statistics were reported in Table 1.\n\n# Spooky Data Example\n\n------------------------------------------------------------------------\n\n## New Data Collection: Example\n\nWe want to be able to connect with the paranormal. Collected data at different locations to examine whether there are certain areas that have more ghost activity. We have multiple ratings (EMF) at the various locations to determine the potential presence of ghosts. The locations were determined by a select group of undergraduate researchers. They include:\n\n::: columns\n::: {.column width=\"50%\"}\n-   Walmart\n-   Abandoned Walmart\n-   Ikea\n-   Getysburg\n:::\n\n::: {.column width=\"50%\"}\n-   The Woods\n-   Unused Stairwell\n-   RIT Tunnels\n-   This classroom\n:::\n:::\n\n------------------------------------------------------------------------\n\n## Review of the NHST process\n\n::: incremental\n1.  Collect Sample and define hypotheses\n\n2.  Set alpha level\n\n3.  Determine the sampling distribution (will be using $F$-distribution now)\n\n4.  Identify the critical value\n\n5.  Calculate test statistic for sample collected\n\n6.  Inspect & compare statistic to critical value; Calculate probability\n:::\n\n------------------------------------------------------------------------\n\n## Example:\n\nTake a look at the data and compute the following:\n\n| Source of Variation |  df   | Sum of Squares |        Mean Squares        |       F-statistic       | p-value |\n|:-------------------:|:-----:|:--------------:|:--------------------------:|:-----------------------:|:-------:|\n|        Group        | $G-1$ |     $SS_b$     | $MS_b = \\frac{SS_b}{df_b}$ | $F = \\frac{MS_b}{MS_w}$ |   $p$   |\n|      Residual       | $N-G$ |     $SS_w$     | $MS_w = \\frac{SS_w}{df_w}$ |                         |         |\n|        Total        | $N-1$ |  $SS_{total}$  |                            |                         |         |\n\nCan use R or Excel\n\n# Next time...\n\n-   Two-Way ANOVA!\n","srcMarkdownNoYaml":"\n\n```{r, include = F}\nknitr::opts_chunk$set(message = FALSE, warning = FALSE)\n```\n\n## Last Week\n\n::: nonincremental\n-   Comparing Means with 2 groups: $t$-test\n    -   Independent Samples $t$-test Review\n    -   Paired Samples $t$-test\n:::\n\n------------------------------------------------------------------------\n\n## Looking Ahead\n\n-   Plan to have 2 more labs that will be similar to the last lab\n\n    -   Likely take place on 10/25 and sometime the week of 11/13\n\n-   Outside of these labs, I am going to plan on having additional mini-labs\n\n    -   Likely to take place on 11/1, 11/22 and 11/29 (will update based on how things are going in class)\n\n------------------------------------------------------------------------\n\n## Today...\n\nIntroduction to ANOVA (Analysis of Variance)\n\n```{r, results = 'hide', message = F, warning = F}\n# File management\nlibrary(here)\n# for dplyr, ggplot2\nlibrary(tidyverse)\n#Loading data\nlibrary(rio)\n\n#Remove Scientific Notation \noptions(scipen=999)\n```\n\n# Overview of ANOVA\n\n## What is ANOVA? ([LSR Ch. 14](https://learningstatisticswithr.com/book/anova.html))\n\n::: incremental\n-   ANOVA stands for [***An***]{.underline}alysis [***o***]{.underline}f [***Va***]{.underline}riance\n-   Comparing means between two or more groups (usually 3 or more)\n    -   Continuous outcome and grouping variable with 2 or more levels\n-   Under the larger umbrella of general linear models\n    -   ANOVA is basically a regression with only categorical predictors\n-   Likely the most widely used tool in Psychology\n:::\n\n## Different Types of ANOVA\n\n-   ::: {.fragment .highlight-green}\n    One-Way ANOVA\n    :::\n\n-   Two-Way ANOVA\n\n-   Repeated Measures ANOVA\n\n-   ANCOVA\n\n-   MANOVA\n\n## One-Way ANOVA\n\nGoal: Inform of differences among the levels of our variable of interest (Omnibus Test)\n\n-   But cannot tell us more than that...\n\nHypotheses:\n\n$$\nH_0: it\\: is\\: true\\: that\\: \\mu_1 = \\mu_2 = \\mu_3 =\\: ...\\mu_k \n\\\\ \nH_1: it\\: is\\: \\boldsymbol{not}\\: true\\: that\\: \\mu_1 = \\mu_2 = \\mu_3 =\\: ...\\mu_k\n$$\n\n## Wait...Means or Variance?\n\nWe are using the variance to create a ratio (within group versus between group variance) to determine differences in means\n\n-   We are not directly investigating variance, but operationalize variance to create the ratio:\n\n$$\nF_{df_b, \\: df_w} = \\frac{MS_{between}}{MS_{within}}\n$$\n\n------------------------------------------------------------------------\n\n![](/images/ANOVA.png){width=\"729\"}\n\n------------------------------------------------------------------------\n\n![](/images/ANOVA.png){width=\"729\"}\n\n::: columns\n::: {.column width=\"50%\"}\n$F = \\frac{MS_{between}}{MS_{within}} = \\frac{small}{large} < 1$\n:::\n\n::: {.column width=\"50%\"}\n$F = \\frac{MS_{between}}{MS_{within}} = \\frac{large}{small} > 1$\n:::\n:::\n\n## ANOVA: Assumptions\n\n::: {style=\"font-size: 30px\"}\n**Independence**\n\n-   Observations between and within groups should be independent. No autocorrelation\n\n**Homogeneity of Variance**\n\n-   The variances within each group should be roughly equal\n    -   Levene's test --\\> Welch's ANOVA for unequal variances\n\n**Normality**\n\n-   The data within each group should follow a normal distribution\n    -   Shapiro-Wilk test --\\> can transform the data or use non-parametric tests\n:::\n\n# NHST with ANOVA\n\n## Review of the NHST process\n\n::: incremental\n1.  Collect Sample and define hypotheses\n\n2.  Set alpha level\n\n3.  Determine the sampling distribution (will be using $F$-distribution now)\n\n4.  Identify the critical value\n\n5.  Calculate test statistic for sample collected\n\n6.  Inspect & compare statistic to critical value; Calculate probability\n:::\n\n## Steps to calculating F-ratio\n\n1.  Capture variance both between and within groups\n2.  Variance to Sum of Squares\n3.  Degrees of Freedom\n4.  Mean squares values\n5.  F-Statistic\n\n## Capturing Variance\n\nWe have calculated variance before!\n\n$$\nVar = \\frac{1}{N}\\sum(x_i - \\bar{x})^2\n$$\n\nNow we have to take into account the variance between and within the groups:\n\n$$\nVar(Y) = \\frac{1}{N} \\sum^G_{k=1}\\sum^{N_k}_{i=i}(Y_{ik} - \\bar{Y})^2\n$$\n\n::: {style=\"font-size: 20px; text-align: center\"}\nNotice that we have the summation across each group ( $G$ ) and the person in the group ( $N_k$ )\n:::\n\n------------------------------------------------------------------------\n\n## Variance to Sum of Squares\n\n**Total Sum of Squares -** Adding up the sum of squares instead of getting the average (notice the removal of $\\frac{1}{N}$)\n\n$$\nSS_{total} = \\sum^G_{k=1}\\sum^{N_k}_{i=i}(Y_{ik} - \\bar{Y})^2\n$$\n\nCan be broken up to see what is the variation ***between*** the groups AND the variation ***within*** the groups\n\n$$\nSS_{total}=SS_{between}+SS_{within}\n$$\n\n::: {style=\"font-size: 20px; text-align: center\"}\nThis gets us closer to understanding the difference between means\n:::\n\n# Sum of Squares\n\n------------------------------------------------------------------------\n\n### Sum of Squares\n\n$$\nSS_{total}=SS_{between}+SS_{within}\n$$\n\n![](/images/var_ss.PNG){fig-align=\"center\"}\n\n------------------------------------------------------------------------\n\n### Example Data\n\n![](/images/table1.PNG){fig-align=\"center\"}\n\n------------------------------------------------------------------------\n\n### Example Data\n\n![](/images/table2.PNG){fig-align=\"center\"}\n\n------------------------------------------------------------------------\n\n### Example Data\n\n![](/images/table3.PNG){fig-align=\"center\"}\n\n------------------------------------------------------------------------\n\n### Example Data\n\n![](/images/table4.PNG){fig-align=\"center\"}\n\n------------------------------------------------------------------------\n\n## Sum of Squares - Between\n\nThe difference between the *group mean* and *grand mean*\n\n$$\nSS_{between} = \\sum^G_{k=1}N_k(\\bar{Y_k} - \\bar{Y})^2\n$$\n\n| Group  | Group Mean $\\bar{Y_k}$ | Grand Mean $\\bar{Y}$ |\n|:------:|:----------------------:|:--------------------:|\n|  Cool  |           32           |         41.8         |\n| Uncool |          56.5          |         41.8         |\n\n------------------------------------------------------------------------\n\n## Sum of Squares - Between\n\nThe difference between the *group mean* and *grand mean*\n\n$$ SS_{between} = \\sum^G_{k=1}N_k(\\bar{Y_k} - \\bar{Y})^2 $$\n\n| Group  | Group Mean $\\bar{Y_k}$ | Grand Mean $\\bar{Y}$ | Sq. Dev. |  N  | Weighted Sq. Dev. |\n|:------:|:----------------------:|:--------------------:|:--------:|:---:|:-----------------:|\n|  Cool  |           32           |         41.8         |  96.04   |  3  |      288.12       |\n| Uncool |          56.5          |         41.8         |  216.09  |  2  |      432.18       |\n\n------------------------------------------------------------------------\n\n## Sum of Squares - Between\n\nThe difference between the *group mean* and *grand mean*\n\n$$ SS_{between} = \\sum^G_{k=1}N_k(\\bar{Y_k} - \\bar{Y})^2 $$\n\nNow we can sum the Weighted Squared Deviations together to get our Sum of Squares Between:\n\n```{r}\nssb <- 288.12 + 432.18\nssb\n```\n\n------------------------------------------------------------------------\n\n## Sum of Squares - Within\n\nThe difference between the [*individual*]{.underline} and their [*group mean*]{.underline}\n\n$$\nSS_{within} = \\sum^G_{k=1}\\sum^{N_k}_{i=i}(Y_{ik} - \\bar{Y_k})^2\n$$\n\n|    Name    | Grumpiness $Y_{ik}$ | Group Mean $\\bar{Y_K}$ |\n|:----------:|:-------------------:|:----------------------:|\n|   Frodo    |         20          |           32           |\n|    Sam     |         55          |           32           |\n|   Bandit   |         21          |           32           |\n| Dolores U. |         91          |          56.5          |\n|   Dustin   |         22          |          56.5          |\n\n------------------------------------------------------------------------\n\n## Sum of Squares - Within\n\nThe difference between the [*individual*]{.underline} and their [*group mean*]{.underline}\n\n$$ SS_{within} = \\sum^G_{k=1}\\sum^{N_k}_{i=i}(Y_{ik} - \\bar{Y_k})^2 $$\n\n|    Name    | Grumpiness $Y_{ik}$ | Group Mean $\\bar{Y_K}$ | Sq. Dev |\n|:----------:|:-------------------:|:----------------------:|---------|\n|   Frodo    |         20          |           32           | 144     |\n|    Sam     |         55          |           32           | 529     |\n|   Bandit   |         21          |           32           | 121     |\n| Dolores U. |         91          |          56.5          | 1190.25 |\n|   Dustin   |         22          |          56.5          | 1190.25 |\n\n```{r}\n#| code-fold: true\nscore <- c(20, 55, 21, 91, 22)\ngroup_m <- c(32, 32, 32, 56.5, 56.5)\nsq_dev <- (score - group_m)^2\n```\n\n------------------------------------------------------------------------\n\n## Sum of Squares - Within\n\nThe difference between the [*individual*]{.underline} and their [*group mean*]{.underline}\n\n$$ SS_{within} = \\sum^G_{k=1}\\sum^{N_k}_{i=i}(Y_{ik} - \\bar{Y_k})^2 $$ Now we can sum the Squared Deviations together to get our Sum of Squares Within:\n\n```{r}\nsum(sq_dev)\n```\n\n## Sum of Squares\n\nCan start to have an idea of what this looks like\n\n$$\nSS_{between} = \\sum^G_{k=1}N_k(\\bar{Y_k} - \\bar{Y})^2 = 720.3\n$$\n\n$$\nSS_{within} = \\sum^G_{k=1}\\sum^{N_k}_{i=i}(Y_{ik} - \\bar{Y_k})^2 = 3174.5\n$$\n\nNext we have to take into account the degrees of freedom\n\n# Degrees of Freedom - ANOVA\n\n------------------------------------------------------------------------\n\n## Degrees of Freedom\n\nSince we have 2 types of variations that we are examining, this needs to be reflected in the degrees of freedom\n\n1.  Take the number of groups and subtract 1\\\n    $df_{between} = G - 1$\n\n2.  Take the total number of observations and subtract the number of groups\\\n\n    $df_{within} = N - G$\n\n# Mean Squares\n\n------------------------------------------------------------------------\n\n## Calculating Mean Squares\n\nNext we convert our summed squares value into a \"mean squares\"\n\nThis is done by dividing by the respective degrees of freedom\n\n$$\nMS_b = \\frac{SS_b}{df_b}\n$$\n\n$$\nMS_W = \\frac{SS_w}{df_w}\n$$\n\n------------------------------------------------------------------------\n\n## Calculating Mean Squares - Example\n\nLet's take a look at how this applies to our example: $$ MS_b = \\frac{SS_b}{G-1} = \\frac{720.3}{2-1} = 720.3 $$\n\n$$ MS_W = \\frac{SS_w}{N-G} = \\frac{3174.5}{5-2} = 1058.167  $$\n\n# $F$-Statistic\n\n------------------------------------------------------------------------\n\n## Calculating the F-Statistic\n\n$$F = \\frac{MS_b}{MS_w}$$\n\nIf the null hypothesis is true, $F$ has an expected value close to 1 (numerator and denominator are estimates of the same variability)\n\nIf it is false, the numerator will likely be larger, because systematic, between-group differences contribute to the variance of the means, but not to variance within group.\n\n------------------------------------------------------------------------\n\n```{r fig.width = 10, fig.height = 6}\n#| code-fold: true\n#| \ndata.frame(F = c(0,8)) %>%\n  ggplot(aes(x = F)) +\n  stat_function(fun = function(x) df(x, df1 = 3, df2 = 196), \n                geom = \"line\") +\n  stat_function(fun = function(x) df(x, df1 = 3, df2 = 196), \n                geom = \"area\", xlim = c(2.65, 8), fill = \"purple\") +\n  geom_vline(aes(xintercept = 2.65), color = \"purple\") +\n  scale_y_continuous(\"Density\") + scale_x_continuous(\"F statistic\", breaks = NULL) +\n  theme_bw(base_size = 20)\n```\n\nIf data are normally distributed, then the variance is $\\chi^2$ distributed\n\n$F$-distributions are one-tailed tests. Recall that we're interested in how far away our test statistic from the null $(F = 1).$\n\n------------------------------------------------------------------------\n\n## Calculating F-statistic: Example\n\n$$F = \\frac{MS_b}{MS_w} = \\frac{720.3}{1058.167} = 0.68$$\n\n[Link](http://courses.atlas.illinois.edu/spring2016/STAT/STAT200/pf.html) to probability calculator\n\n------------------------------------------------------------------------\n\n```{r fig.width = 10, fig.height = 6}\n#| code-fold: true\n#| \ndata.frame(F = c(0,8)) %>%\n  ggplot(aes(x = F)) +\n  stat_function(fun = function(x) df(x, df1 = 3, df2 = 196), \n                geom = \"line\") +\n  stat_function(fun = function(x) df(x, df1 = 3, df2 = 196), \n                geom = \"area\", xlim = c(2.65, 8), fill = \"purple\") +\n  geom_vline(aes(xintercept = 2.65), color = \"purple\") + \n  geom_vline(aes(xintercept = 0.68), color = \"red\") + \n  annotate(\"text\",\n           label = \"F=0.68\", \n           x = 1.1, y = 0.65, size = 8, color = \"red\") + \n  scale_y_continuous(\"Density\") + scale_x_continuous(\"F statistic\", breaks = NULL) +\n  theme_bw(base_size = 20)\n```\n\nWhat can we conclude?\n\n------------------------------------------------------------------------\n\n## Contrasts/Post-Hoc Tests\n\nPerformed when there is a significant difference among the groups to examine which groups are different\n\n1.  **Contrasts**: When we have *a priori* hypotheses\n2.  **Post-hoc Tests**: When we want to test everything\n\n# Reporting Results\n\n------------------------------------------------------------------------\n\n## Tables\n\nOften times the output will be in the form of a table and then it is often reported this way in the manuscript\n\n| Source of Variation |  df   | Sum of Squares |        Mean Squares        |       F-statistic       | p-value |\n|:-------------------:|:-----:|:--------------:|:--------------------------:|:-----------------------:|:-------:|\n|        Group        | $G-1$ |     $SS_b$     | $MS_b = \\frac{SS_b}{df_b}$ | $F = \\frac{MS_b}{MS_w}$ |   $p$   |\n|      Residual       | $N-G$ |     $SS_w$     | $MS_w = \\frac{SS_w}{df_w}$ |                         |         |\n|        Total        | $N-1$ |  $SS_{total}$  |                            |                         |         |\n\n------------------------------------------------------------------------\n\n## In-Text\n\n> A one-way analysis of variance was used to test for differences in the \\[variable of interest/outcome variable\\] as a function of \\[whatever the factor is\\]. Specifically, differences in \\[variable of interest\\] were assessed for the \\[list different levels and be sure to include (M= , SD= )\\] . The one-way ANOVA revealed a significant/nonsignificant effect of \\[factor\\] on scores on the \\[variable of interest\\] (F(dfb, dfw) = f-ratio, p = p-value, η2 = effect size).\n>\n> Planned comparisons were conducted to compare expected differences among the \\[however many groups\\] means. Planned contrasts revealed that participants in the \\[one of the conditions\\] had a greater/fewer \\[variable of interest\\] and then include the p-value. This same type of sentence is repeated for whichever contrasts you completed. Descriptive statistics were reported in Table 1.\n\n# Spooky Data Example\n\n------------------------------------------------------------------------\n\n## New Data Collection: Example\n\nWe want to be able to connect with the paranormal. Collected data at different locations to examine whether there are certain areas that have more ghost activity. We have multiple ratings (EMF) at the various locations to determine the potential presence of ghosts. The locations were determined by a select group of undergraduate researchers. They include:\n\n::: columns\n::: {.column width=\"50%\"}\n-   Walmart\n-   Abandoned Walmart\n-   Ikea\n-   Getysburg\n:::\n\n::: {.column width=\"50%\"}\n-   The Woods\n-   Unused Stairwell\n-   RIT Tunnels\n-   This classroom\n:::\n:::\n\n------------------------------------------------------------------------\n\n## Review of the NHST process\n\n::: incremental\n1.  Collect Sample and define hypotheses\n\n2.  Set alpha level\n\n3.  Determine the sampling distribution (will be using $F$-distribution now)\n\n4.  Identify the critical value\n\n5.  Calculate test statistic for sample collected\n\n6.  Inspect & compare statistic to critical value; Calculate probability\n:::\n\n------------------------------------------------------------------------\n\n## Example:\n\nTake a look at the data and compute the following:\n\n| Source of Variation |  df   | Sum of Squares |        Mean Squares        |       F-statistic       | p-value |\n|:-------------------:|:-----:|:--------------:|:--------------------------:|:-----------------------:|:-------:|\n|        Group        | $G-1$ |     $SS_b$     | $MS_b = \\frac{SS_b}{df_b}$ | $F = \\frac{MS_b}{MS_w}$ |   $p$   |\n|      Residual       | $N-G$ |     $SS_w$     | $MS_w = \\frac{SS_w}{df_w}$ |                         |         |\n|        Total        | $N-1$ |  $SS_{total}$  |                            |                         |         |\n\nCan use R or Excel\n\n# Next time...\n\n-   Two-Way ANOVA!\n"},"formats":{"revealjs":{"identifier":{"display-name":"RevealJS","target-format":"revealjs","base-format":"revealjs"},"execute":{"fig-width":10,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":false,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"wrap","code-link":false,"code-line-numbers":true,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","html-math-method":{"method":"mathjax","url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML-full"},"slide-level":2,"to":"revealjs","incremental":false,"output-file":"09_ANOVA.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":false,"quarto-version":"1.3.433","auto-stretch":true,"editor":"visual","title":"Introduction to ANOVA","subtitle":"PSYC 640 - Fall 2023","author":"Dustin Haraden, PhD","editor_options":{"chunk_output_type":"console"},"multiplex":true,"scrollable":true,"slideNumber":true,"touch":true,"theme":"dark"}}},"projectFormats":["html"]}