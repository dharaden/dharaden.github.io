{"title":"Wk 5 - Hypothesis & Power","markdown":{"yaml":{"title":"Wk 5 - Hypothesis & Power","subtitle":"PSYC 640 - Fall 2023","author":"Dustin Haraden","format":{"revealjs":{"multiplex":true,"slide-number":true,"incremental":true,"touch":true,"code-overflow":"wrap","theme":"night"}},"execute":{"echo":true},"editor":"visual","editor_options":{"chunk_output_type":"console"}},"headingText":"Lessons from Lab 1","containsRefs":false,"markdown":"\n\n\n-   Getting data into R is surprisingly hard\n\n-   The console doesn't come with you\n\n-   Work together\n\n-   Professor gets too excited about R\n\n```{r}\nlibrary(tidyverse) #plotting\nlibrary(ggpubr) #prettier figures\n```\n\n------------------------------------------------------------------------\n\n## Sampling Revisited\n\nWe use features of the sample (*statistics*) to inform us about features of the population (*parameters*). The quality of this information goes up as sample size goes up -- **the Law of Large Numbers**. The quality of this information is easier to defend with random samples.\n\nAll sample statistics are wrong (they do not match the population parameters exactly) but they become more useful (better matches) as sample size increases.\n\n------------------------------------------------------------------------\n\n## Some Terminology\n\n| Population                                       | Sample                                           |\n|--------------------------------------------------|--------------------------------------------------|\n| $\\mu$ (mu) = Population Mean                     | $\\bar{X}$ (x bar) = Sample Mean                  |\n| $\\sigma$ (sigma) = Population Standard Deviation | $s$ = $\\hat{\\sigma}$ = Sample Standard Deviation |\n| $\\sigma^2$ (sigma squared) = Population Variance | $s^2$ = $\\hat{\\sigma^2}$ = Sample Variance       |\n\n------------------------------------------------------------------------\n\n::: columns\n::: {.column width=\"30%\"}\n*Population distribution*\n\nThe parameters of this distribution are unknown. We use the sample to inform us about the likely characteristics of the population.\n:::\n\n::: {.column width=\"70%\"}\n```{r,  warning = F, message = F}\n#| code-fold: true\nggplot(data.frame(x = seq(-4, 4)), aes(x)) +\n  stat_function(fun = function(x) dnorm(x), geom = \"area\") +\n  scale_x_continuous(\"Variable X\") +\n  scale_y_continuous(\"Density\") +\n  labs(title = expression(Population~mu*\"=0\"~sigma~\"=1\"))+\n  theme(text = element_text(size = 20))\n\n```\n:::\n:::\n\n------------------------------------------------------------------------\n\n::: columns\n::: {.column width=\"30%\"}\n*Samples from the population.*\n\n[Each sample distribution will resemble the population. That resemblance will be better as sample size increases: The Law of Large Numbers.]{style=\"font-size:30px;\"}\n\n[Statistics (e.g., mean) can be calculated for any sample.]{style=\"font-size:30px;\"}\n:::\n\n::: {.column width=\"70%\"}\n```{r, warning=F, message=F, fig.height = 6.5}\n#| code-fold: true\nlibrary(ggpubr) #for multiple plots\nsample_size = 30\nset.seed(101919)\nfor(i in 1:4){\n  sample = rnorm(n = sample_size)\n  m = round(mean(sample),3)\n  s = round(sd(sample),2)\n  p = data.frame(x = sample) %>%\n    ggplot(aes(x = x)) +\n    geom_histogram(color = \"white\") +\n    geom_vline(aes(xintercept = mean(x)), \n               color = \"purple\", size = 2, alpha = .5)+\n    scale_x_continuous(limits = c(-4,4)) +\n    scale_y_continuous(\"\", breaks = NULL) +\n    labs(title = as.expression(bquote(\"Sample\"~.(i)~\", m =\"~.(m)~\", sd =\"~.(s))))\n  assign(paste0(\"p\",i), p) +\n    theme(text = element_text(size = 20))\n}\n\nggarrange(p1,p2,p3,p4, ncol =2, nrow = 2)\n```\n:::\n:::\n\n------------------------------------------------------------------------\n\n::: columns\n::: {.column width=\"30%\"}\n[The statistics from a large number of samples also have a distribution: the **sampling distribution**.]{style=\"font-size:30px;\"}\n\n[By the **Central Limit Theorem**, this distribution will be normal as sample size increases.]{style=\"font-size:30px;\"}\n:::\n\n::: {.column width=\"70%\"}\n```{r sampling, warning = F, message = F}\n#| code-fold: true\nreps = 5000\nmeans = rep(0, reps)\nse = 1/sqrt(sample_size)\nset.seed(101919)\nfor(i in 1:reps){\n  means[i] = mean(rnorm(n = sample_size))\n}\ndata.frame(mean = means) %>%\n  ggplot(aes(x = mean)) + \n  geom_histogram(aes(y = ..density..), \n                 fill = \"purple\", \n                 color = \"white\") +  \n  stat_function(fun = function(x) dnorm(x, mean = 0, sd = se), inherit.aes = F, size = 1.5) \n```\n:::\n:::\n\n[This distribution has a standard deviation, called the **standard error of the mean**. *Its mean converges on* $\\mu$.]{style=\"font-size:30px;\"}\n\n::: notes\nSampling distributions can be constructed around any statistic -- ranges, standard deviations, difference scores. The standard errors of those distributions are also standard errors. (E.g., the standard error of the difference.)\n:::\n\n------------------------------------------------------------------------\n\n::: columns\n::: {.column width=\"30%\"}\n[We don't actually have to take a large number of random samples to construct the sampling distribution. It is a theoretical result of the Central Limit Theorem. We just need an estimate of the population parameter, $s$, which we can get from the sample.]{style=\"font-size:30px;\"}\n:::\n\n::: {.column width=\"70%\"}\n```{r ref.label=\"sampling\", message = F, warning = F}\n#| code-fold: true\n\n```\n:::\n:::\n\n------------------------------------------------------------------------\n\n::: columns\n::: {.column width=\"30%\"}\n[We don't actually have to take a large number of random samples to construct the sampling distribution. It is a theoretical result of the Central Limit Theorem. We just need an estimate of the population parameter, $s$, which we can get from the sample.]{style=\"font-size:30px;\"}\n:::\n\n::: {.column width=\"70%\"}\n```{r, message = F, warning = F}\n#| code-fold: true\nggplot(data.frame(x = seq(min(means), max(means), by = .05)), aes(x)) +\n  stat_function(fun = function(x) dnorm(x, mean = 0, sd = se), size = 1.5) \n```\n:::\n:::\n\n------------------------------------------------------------------------\n\nThe sampling distribution of means can be used to make probabilistic statements about means in the same way that the standard normal distribution is used to make probabilistic statements about scores.\n\nFor example, we can determine the range within which the population mean is likely to be with a particular level of confidence.\n\nOr, we can propose different values for the population mean and ask how typical or rare the sample mean would be if that population value were true. We can then compare the plausibility of different such \"models\" of the population.\n\n------------------------------------------------------------------------\n\nThe key is that we have a sampling distribution of the mean with a standard deviation **(the Standard Error of the Mean)** that is linked to the population:\n\n$$SEM = \\sigma_M = \\frac{\\sigma}{\\sqrt{N}}$$\n\nWe do not know $\\sigma$ but we can estimate it based on the sample statistic:\n\n$$\\hat{\\sigma} = s = \\sqrt{\\frac{1}{N-1}\\sum_{i=1}^N(X-\\bar{X})^2}$$\n\n------------------------------------------------------------------------\n\n$$\\hat{\\sigma} = s = \\sqrt{\\frac{1}{N-1}\\sum_{i=1}^N(X-\\bar{X})^2}$$\n\nThis is the sample estimate of the population standard deviation. This is an unbiased estimate of $\\sigma$ and relies on the sample mean, which is an unbiased estimate of $\\mu$.\n\n$$SEM = \\sigma_M = \\frac{\\hat{\\sigma}}{\\sqrt{N}} = \\frac{\\text{Estimate of pop SD}}{\\sqrt{N}}$$\n\n::: notes\n(Most methods of calculating standard deviation assume you're estimating the population $\\sigma$ from a sample and correct for bias.)\n:::\n\n------------------------------------------------------------------------\n\nThe sampling distribution of the mean has variability, represented by the SEM, reflecting uncertainty in the sample mean as an estimate of the population mean.\n\nThe assumption of normality allows us to construct an interval within which we have good reason to believe a sample mean will fall if it comes from a particular population:\n\n$$\\bar{X} - (1.96\\times SEM) \\leq \\mu \\leq \\bar{X} + (1.96\\times SEM) $$\n\n------------------------------------------------------------------------\n\n$$\\bar{X} - (1.96\\times SEM) \\leq \\mu \\leq \\bar{X} + (1.96\\times SEM) $$\n\nThis is referred to as a **95% confidence interval (CI)**. Note the assumption of normality, which should hold by the Central Limit Theorem, if N is sufficiently large.\n\nThe 95% CI is sometimes represented as:\n\n$$CI_{95} = \\bar{X} \\pm [1.96\\frac{\\hat{\\sigma}}{\\sqrt{N}}]$$\n\nWhere does 1.96 come from?\n\n------------------------------------------------------------------------\n\nRemember the Empirical Rule (95%)\n\n```{r}\n#| code-fold: true\n#| \nggplot(data.frame(x = seq(-4, 4)), aes(x)) +\n  stat_function(fun = function(x) dnorm(x)) +\n  stat_function(fun = function(x) dnorm(x),\n                xlim = c(-1.96, 1.96), geom = \"area\", fill = \"purple\", alpha = .3) +\n  geom_vline(aes(xintercept = 1.96), color = \"purple\")+\n  geom_vline(aes(xintercept = -1.96), color = \"purple\") +\n  geom_label(aes(x = 1.96, y = .3, label = \"1.96\"))+\n  geom_label(aes(x = -1.96, y = .3, label = \"-1.96\"))\n```\n\n------------------------------------------------------------------------\n\nWhat if you didn't know the value?\n\n```{r}\n#| code-fold: true\nggplot(data.frame(x = seq(-4, 4)), aes(x)) +\n  stat_function(fun = function(x) dnorm(x)) +\n  stat_function(fun = function(x) dnorm(x),\n                xlim = c(-1.96, 1.96), geom = \"area\", fill = \"purple\", alpha = .3) +\n  geom_vline(aes(xintercept = 1.96), color = \"purple\")+\n  geom_vline(aes(xintercept = -1.96), color = \"purple\") +\n  geom_label(aes(x = 1.96, y = .3, label = \"?\"))+\n  geom_label(aes(x = -1.96, y = .3, label = \"?\"))\n```\n\n------------------------------------------------------------------------\n\nWhat if you didn't know the value?\n\n```{r}\n#| code-fold: true\nggplot(data.frame(x = seq(-4, 4)), aes(x)) +\n  stat_function(fun = function(x) dnorm(x)) +\n  stat_function(fun = function(x) dnorm(x),\n                xlim = c(-1.96, 1.96), geom = \"area\", fill = \"purple\", alpha = .3) +\n  stat_function(fun = function(x) dnorm(x),\n                xlim = c(-4, -1.96), geom = \"area\", fill = \"green\", alpha = .3) +\n  stat_function(fun = function(x) dnorm(x),\n                xlim = c(4, 1.96), geom = \"area\", fill = \"green\", alpha = .3) +\n  geom_vline(aes(xintercept = 1.96), color = \"purple\")+\n  geom_vline(aes(xintercept = -1.96), color = \"purple\") +\n  geom_label(aes(x = 1.96, y = .3, label = \"?\"))+\n  geom_label(aes(x = -1.96, y = .3, label = \"?\"))\n```\n\n------------------------------------------------------------------------\n\nWhat if you didn't know the value?\n\n```{r, fig.height=6}\n#| code-fold: true\nggplot(data.frame(x = seq(-4, 4)), aes(x)) +\n  stat_function(fun = function(x) dnorm(x)) +\n  stat_function(fun = function(x) dnorm(x),\n                xlim = c(-4, 1.96), geom = \"area\", fill = \"purple\", alpha = .3) +\n  geom_vline(aes(xintercept = 1.96), color = \"purple\")+\n  geom_label(aes(x = 1.96, y = .3, label = \"?\"))\n```\n\n```{r}\nqnorm(.975)\n```\n\n------------------------------------------------------------------------\n\n::: columns\n::: {.column width=\"30%\"}\n[The normal distribution assumes we know the population mean and standard deviation. But we don't. We only know the sample mean and standard deviation, and those have some uncertainty about them.]{style=\"font-size:30px;line-height: .5em\"}\n:::\n\n::: {.column width=\"70%\"}\n```{r, fig.width = 7}\n#| code-fold: true\nggplot(data.frame(x = seq(-4, 4)), aes(x)) +\n  stat_function(fun = function(x) dnorm(x), \n                aes(color = \"Normal\", linetype = \"Normal\")) +\n  stat_function(fun = function(x) dt(x, df = 1), \n                aes(color = \"t(1)\", linetype = \"t(1)\")) +\n  stat_function(fun = function(x) dt(x, df = 5), \n                aes(color = \"t(5)\", linetype = \"t(5)\")) +\n  stat_function(fun = function(x) dt(x, df = 25), \n                aes(color = \"t(25)\", linetype = \"t(25)\")) +\n  stat_function(fun = function(x) dt(x, df = 100), \n                aes(color = \"t(100)\", linetype = \"t(100)\")) + \n  scale_x_continuous(\"Variable X\") +\n  scale_y_continuous(\"Density\") +\n  scale_color_manual(\"\", \n                     values = c(\"red\", \"black\", \"black\", \"blue\", \"blue\")) +\n  scale_linetype_manual(\"\", \n                        values = c(\"solid\", \"solid\", \"dashed\", \"solid\", \"dashed\")) +\n  ggtitle(\"The Normal and t Distributions\") +\n  theme(text = element_text(size=20),legend.position = \"bottom\")\n```\n:::\n:::\n\n------------------------------------------------------------------------\n\n::: columns\n::: {.column width=\"30%\"}\n[That uncertainty is reduced with large samples, so that the normal is \"close enough.\" In small samples, the $t$ distribution provides a better approximation.]{style=\"font-size:30px;\"}\n:::\n\n::: {.column width=\"70%\"}\n```{r, fig.width = 7}\n#| code-fold: true\nggplot(data.frame(x = seq(-4, 4)), aes(x)) +\n  stat_function(fun = function(x) dnorm(x), \n                aes(color = \"Normal\", linetype = \"Normal\")) +\n  stat_function(fun = function(x) dt(x, df = 1), \n                aes(color = \"t(1)\", linetype = \"t(1)\")) +\n  stat_function(fun = function(x) dt(x, df = 5), \n                aes(color = \"t(5)\", linetype = \"t(5)\")) +\n  stat_function(fun = function(x) dt(x, df = 25), \n                aes(color = \"t(25)\", linetype = \"t(25)\")) +\n  stat_function(fun = function(x) dt(x, df = 100), \n                aes(color = \"t(100)\", linetype = \"t(100)\")) + \n  scale_x_continuous(\"Variable X\") +\n  scale_y_continuous(\"Density\") +\n  scale_color_manual(\"\", \n                     values = c(\"red\", \"black\", \"black\", \"blue\", \"blue\")) +\n  scale_linetype_manual(\"\", \n                        values = c(\"solid\", \"solid\", \"dashed\", \"solid\", \"dashed\")) +\n  ggtitle(\"The Normal and t Distributions\") +\n  theme(text = element_text(size=20),legend.position = \"bottom\")\n```\n:::\n:::\n\n------------------------------------------------------------------------\n\n[For small samples, we need to use the t distribution with its fatter tails. This produces wider confidence intervals---the penalty we have to pay for our ignorance about the population.<br>The form of the confidence interval remains the same. We simply substitute a corresponding value from the t distribution (using df =$N -1$).]{style=\"font-size:28px;\"}\n\n$$CI_{95} = \\bar{X} \\pm [1.96\\frac{\\hat{\\sigma}}{\\sqrt{N}}]$$\n\n$$CI_{95} = \\bar{X} \\pm [Z_{.975}\\frac{\\hat{\\sigma}}{\\sqrt{N}}]$$\n\n$$CI_{95} = \\bar{X} \\pm [t_{.975, df = N-1}\\frac{\\hat{\\sigma}}{\\sqrt{N}}]$$\n\n------------------------------------------------------------------------\n\n::: nonincremental\n-   The meaning of the confidence interval can be a bit confusing and arises from the peculiar language forced on us by the frequentist viewpoint.\n-   The CI DOES NOT mean \"there is a 95% probability that the true mean lies inside the confidence interval.\"\n-   It means that if we carried out random sampling from the population a large number of times, and calculated the 95% confidence interval each time, then 95% of those intervals can be expected to contain the population mean.\n:::\n\n------------------------------------------------------------------------\n\nIn previous years, incoming first year graduate students had an average coffee consumption of 7.6 cups per week and a standard deviation of 2.4.\n\nThe next incoming class will have 43 students. What range of exam means would be plausible if this class is similar to past classes (comes from the same population)?\n\n$$\\bar{X} - (1.96\\times SEM) \\leq \\mu \\leq \\bar{X} + (1.96\\times SEM) $$\n\n------------------------------------------------------------------------\n\nCalculated with the ***normal distribution***\n\n```{r}\nM <- 7.6 \nSD <- 2.4\nN <- 43\nsem <- SD/sqrt(N)\n\nci_lb_z <- M - (sem * qnorm(p = .975))\nci_ub_z <-  M + (sem * qnorm(p = .975))\nc(ci_lb_z, ci_ub_z)\n```\n\nCalculated with the ***t-distribution***\n\n```{r}\nM <- 7.6 \nSD <- 2.4\nN <- 43\nsem <- SD/sqrt(N)\n\n\nci_lb_t <- M - (sem * qt(p = .975, df = N-1))\nci_ub_t <- M + (sem * qt(p = .975, df = N-1))\nc(ci_lb_t, ci_ub_t)\n```\n\nWhat do you notice about the two?\n\n# Lab 2 Time\n\n::: nonincremental\n-   Using Rainbow Parentheses\n\n-   Visual Editor & Knitting document\n:::\n","srcMarkdownNoYaml":"\n\n## Lessons from Lab 1\n\n-   Getting data into R is surprisingly hard\n\n-   The console doesn't come with you\n\n-   Work together\n\n-   Professor gets too excited about R\n\n```{r}\nlibrary(tidyverse) #plotting\nlibrary(ggpubr) #prettier figures\n```\n\n------------------------------------------------------------------------\n\n## Sampling Revisited\n\nWe use features of the sample (*statistics*) to inform us about features of the population (*parameters*). The quality of this information goes up as sample size goes up -- **the Law of Large Numbers**. The quality of this information is easier to defend with random samples.\n\nAll sample statistics are wrong (they do not match the population parameters exactly) but they become more useful (better matches) as sample size increases.\n\n------------------------------------------------------------------------\n\n## Some Terminology\n\n| Population                                       | Sample                                           |\n|--------------------------------------------------|--------------------------------------------------|\n| $\\mu$ (mu) = Population Mean                     | $\\bar{X}$ (x bar) = Sample Mean                  |\n| $\\sigma$ (sigma) = Population Standard Deviation | $s$ = $\\hat{\\sigma}$ = Sample Standard Deviation |\n| $\\sigma^2$ (sigma squared) = Population Variance | $s^2$ = $\\hat{\\sigma^2}$ = Sample Variance       |\n\n------------------------------------------------------------------------\n\n::: columns\n::: {.column width=\"30%\"}\n*Population distribution*\n\nThe parameters of this distribution are unknown. We use the sample to inform us about the likely characteristics of the population.\n:::\n\n::: {.column width=\"70%\"}\n```{r,  warning = F, message = F}\n#| code-fold: true\nggplot(data.frame(x = seq(-4, 4)), aes(x)) +\n  stat_function(fun = function(x) dnorm(x), geom = \"area\") +\n  scale_x_continuous(\"Variable X\") +\n  scale_y_continuous(\"Density\") +\n  labs(title = expression(Population~mu*\"=0\"~sigma~\"=1\"))+\n  theme(text = element_text(size = 20))\n\n```\n:::\n:::\n\n------------------------------------------------------------------------\n\n::: columns\n::: {.column width=\"30%\"}\n*Samples from the population.*\n\n[Each sample distribution will resemble the population. That resemblance will be better as sample size increases: The Law of Large Numbers.]{style=\"font-size:30px;\"}\n\n[Statistics (e.g., mean) can be calculated for any sample.]{style=\"font-size:30px;\"}\n:::\n\n::: {.column width=\"70%\"}\n```{r, warning=F, message=F, fig.height = 6.5}\n#| code-fold: true\nlibrary(ggpubr) #for multiple plots\nsample_size = 30\nset.seed(101919)\nfor(i in 1:4){\n  sample = rnorm(n = sample_size)\n  m = round(mean(sample),3)\n  s = round(sd(sample),2)\n  p = data.frame(x = sample) %>%\n    ggplot(aes(x = x)) +\n    geom_histogram(color = \"white\") +\n    geom_vline(aes(xintercept = mean(x)), \n               color = \"purple\", size = 2, alpha = .5)+\n    scale_x_continuous(limits = c(-4,4)) +\n    scale_y_continuous(\"\", breaks = NULL) +\n    labs(title = as.expression(bquote(\"Sample\"~.(i)~\", m =\"~.(m)~\", sd =\"~.(s))))\n  assign(paste0(\"p\",i), p) +\n    theme(text = element_text(size = 20))\n}\n\nggarrange(p1,p2,p3,p4, ncol =2, nrow = 2)\n```\n:::\n:::\n\n------------------------------------------------------------------------\n\n::: columns\n::: {.column width=\"30%\"}\n[The statistics from a large number of samples also have a distribution: the **sampling distribution**.]{style=\"font-size:30px;\"}\n\n[By the **Central Limit Theorem**, this distribution will be normal as sample size increases.]{style=\"font-size:30px;\"}\n:::\n\n::: {.column width=\"70%\"}\n```{r sampling, warning = F, message = F}\n#| code-fold: true\nreps = 5000\nmeans = rep(0, reps)\nse = 1/sqrt(sample_size)\nset.seed(101919)\nfor(i in 1:reps){\n  means[i] = mean(rnorm(n = sample_size))\n}\ndata.frame(mean = means) %>%\n  ggplot(aes(x = mean)) + \n  geom_histogram(aes(y = ..density..), \n                 fill = \"purple\", \n                 color = \"white\") +  \n  stat_function(fun = function(x) dnorm(x, mean = 0, sd = se), inherit.aes = F, size = 1.5) \n```\n:::\n:::\n\n[This distribution has a standard deviation, called the **standard error of the mean**. *Its mean converges on* $\\mu$.]{style=\"font-size:30px;\"}\n\n::: notes\nSampling distributions can be constructed around any statistic -- ranges, standard deviations, difference scores. The standard errors of those distributions are also standard errors. (E.g., the standard error of the difference.)\n:::\n\n------------------------------------------------------------------------\n\n::: columns\n::: {.column width=\"30%\"}\n[We don't actually have to take a large number of random samples to construct the sampling distribution. It is a theoretical result of the Central Limit Theorem. We just need an estimate of the population parameter, $s$, which we can get from the sample.]{style=\"font-size:30px;\"}\n:::\n\n::: {.column width=\"70%\"}\n```{r ref.label=\"sampling\", message = F, warning = F}\n#| code-fold: true\n\n```\n:::\n:::\n\n------------------------------------------------------------------------\n\n::: columns\n::: {.column width=\"30%\"}\n[We don't actually have to take a large number of random samples to construct the sampling distribution. It is a theoretical result of the Central Limit Theorem. We just need an estimate of the population parameter, $s$, which we can get from the sample.]{style=\"font-size:30px;\"}\n:::\n\n::: {.column width=\"70%\"}\n```{r, message = F, warning = F}\n#| code-fold: true\nggplot(data.frame(x = seq(min(means), max(means), by = .05)), aes(x)) +\n  stat_function(fun = function(x) dnorm(x, mean = 0, sd = se), size = 1.5) \n```\n:::\n:::\n\n------------------------------------------------------------------------\n\nThe sampling distribution of means can be used to make probabilistic statements about means in the same way that the standard normal distribution is used to make probabilistic statements about scores.\n\nFor example, we can determine the range within which the population mean is likely to be with a particular level of confidence.\n\nOr, we can propose different values for the population mean and ask how typical or rare the sample mean would be if that population value were true. We can then compare the plausibility of different such \"models\" of the population.\n\n------------------------------------------------------------------------\n\nThe key is that we have a sampling distribution of the mean with a standard deviation **(the Standard Error of the Mean)** that is linked to the population:\n\n$$SEM = \\sigma_M = \\frac{\\sigma}{\\sqrt{N}}$$\n\nWe do not know $\\sigma$ but we can estimate it based on the sample statistic:\n\n$$\\hat{\\sigma} = s = \\sqrt{\\frac{1}{N-1}\\sum_{i=1}^N(X-\\bar{X})^2}$$\n\n------------------------------------------------------------------------\n\n$$\\hat{\\sigma} = s = \\sqrt{\\frac{1}{N-1}\\sum_{i=1}^N(X-\\bar{X})^2}$$\n\nThis is the sample estimate of the population standard deviation. This is an unbiased estimate of $\\sigma$ and relies on the sample mean, which is an unbiased estimate of $\\mu$.\n\n$$SEM = \\sigma_M = \\frac{\\hat{\\sigma}}{\\sqrt{N}} = \\frac{\\text{Estimate of pop SD}}{\\sqrt{N}}$$\n\n::: notes\n(Most methods of calculating standard deviation assume you're estimating the population $\\sigma$ from a sample and correct for bias.)\n:::\n\n------------------------------------------------------------------------\n\nThe sampling distribution of the mean has variability, represented by the SEM, reflecting uncertainty in the sample mean as an estimate of the population mean.\n\nThe assumption of normality allows us to construct an interval within which we have good reason to believe a sample mean will fall if it comes from a particular population:\n\n$$\\bar{X} - (1.96\\times SEM) \\leq \\mu \\leq \\bar{X} + (1.96\\times SEM) $$\n\n------------------------------------------------------------------------\n\n$$\\bar{X} - (1.96\\times SEM) \\leq \\mu \\leq \\bar{X} + (1.96\\times SEM) $$\n\nThis is referred to as a **95% confidence interval (CI)**. Note the assumption of normality, which should hold by the Central Limit Theorem, if N is sufficiently large.\n\nThe 95% CI is sometimes represented as:\n\n$$CI_{95} = \\bar{X} \\pm [1.96\\frac{\\hat{\\sigma}}{\\sqrt{N}}]$$\n\nWhere does 1.96 come from?\n\n------------------------------------------------------------------------\n\nRemember the Empirical Rule (95%)\n\n```{r}\n#| code-fold: true\n#| \nggplot(data.frame(x = seq(-4, 4)), aes(x)) +\n  stat_function(fun = function(x) dnorm(x)) +\n  stat_function(fun = function(x) dnorm(x),\n                xlim = c(-1.96, 1.96), geom = \"area\", fill = \"purple\", alpha = .3) +\n  geom_vline(aes(xintercept = 1.96), color = \"purple\")+\n  geom_vline(aes(xintercept = -1.96), color = \"purple\") +\n  geom_label(aes(x = 1.96, y = .3, label = \"1.96\"))+\n  geom_label(aes(x = -1.96, y = .3, label = \"-1.96\"))\n```\n\n------------------------------------------------------------------------\n\nWhat if you didn't know the value?\n\n```{r}\n#| code-fold: true\nggplot(data.frame(x = seq(-4, 4)), aes(x)) +\n  stat_function(fun = function(x) dnorm(x)) +\n  stat_function(fun = function(x) dnorm(x),\n                xlim = c(-1.96, 1.96), geom = \"area\", fill = \"purple\", alpha = .3) +\n  geom_vline(aes(xintercept = 1.96), color = \"purple\")+\n  geom_vline(aes(xintercept = -1.96), color = \"purple\") +\n  geom_label(aes(x = 1.96, y = .3, label = \"?\"))+\n  geom_label(aes(x = -1.96, y = .3, label = \"?\"))\n```\n\n------------------------------------------------------------------------\n\nWhat if you didn't know the value?\n\n```{r}\n#| code-fold: true\nggplot(data.frame(x = seq(-4, 4)), aes(x)) +\n  stat_function(fun = function(x) dnorm(x)) +\n  stat_function(fun = function(x) dnorm(x),\n                xlim = c(-1.96, 1.96), geom = \"area\", fill = \"purple\", alpha = .3) +\n  stat_function(fun = function(x) dnorm(x),\n                xlim = c(-4, -1.96), geom = \"area\", fill = \"green\", alpha = .3) +\n  stat_function(fun = function(x) dnorm(x),\n                xlim = c(4, 1.96), geom = \"area\", fill = \"green\", alpha = .3) +\n  geom_vline(aes(xintercept = 1.96), color = \"purple\")+\n  geom_vline(aes(xintercept = -1.96), color = \"purple\") +\n  geom_label(aes(x = 1.96, y = .3, label = \"?\"))+\n  geom_label(aes(x = -1.96, y = .3, label = \"?\"))\n```\n\n------------------------------------------------------------------------\n\nWhat if you didn't know the value?\n\n```{r, fig.height=6}\n#| code-fold: true\nggplot(data.frame(x = seq(-4, 4)), aes(x)) +\n  stat_function(fun = function(x) dnorm(x)) +\n  stat_function(fun = function(x) dnorm(x),\n                xlim = c(-4, 1.96), geom = \"area\", fill = \"purple\", alpha = .3) +\n  geom_vline(aes(xintercept = 1.96), color = \"purple\")+\n  geom_label(aes(x = 1.96, y = .3, label = \"?\"))\n```\n\n```{r}\nqnorm(.975)\n```\n\n------------------------------------------------------------------------\n\n::: columns\n::: {.column width=\"30%\"}\n[The normal distribution assumes we know the population mean and standard deviation. But we don't. We only know the sample mean and standard deviation, and those have some uncertainty about them.]{style=\"font-size:30px;line-height: .5em\"}\n:::\n\n::: {.column width=\"70%\"}\n```{r, fig.width = 7}\n#| code-fold: true\nggplot(data.frame(x = seq(-4, 4)), aes(x)) +\n  stat_function(fun = function(x) dnorm(x), \n                aes(color = \"Normal\", linetype = \"Normal\")) +\n  stat_function(fun = function(x) dt(x, df = 1), \n                aes(color = \"t(1)\", linetype = \"t(1)\")) +\n  stat_function(fun = function(x) dt(x, df = 5), \n                aes(color = \"t(5)\", linetype = \"t(5)\")) +\n  stat_function(fun = function(x) dt(x, df = 25), \n                aes(color = \"t(25)\", linetype = \"t(25)\")) +\n  stat_function(fun = function(x) dt(x, df = 100), \n                aes(color = \"t(100)\", linetype = \"t(100)\")) + \n  scale_x_continuous(\"Variable X\") +\n  scale_y_continuous(\"Density\") +\n  scale_color_manual(\"\", \n                     values = c(\"red\", \"black\", \"black\", \"blue\", \"blue\")) +\n  scale_linetype_manual(\"\", \n                        values = c(\"solid\", \"solid\", \"dashed\", \"solid\", \"dashed\")) +\n  ggtitle(\"The Normal and t Distributions\") +\n  theme(text = element_text(size=20),legend.position = \"bottom\")\n```\n:::\n:::\n\n------------------------------------------------------------------------\n\n::: columns\n::: {.column width=\"30%\"}\n[That uncertainty is reduced with large samples, so that the normal is \"close enough.\" In small samples, the $t$ distribution provides a better approximation.]{style=\"font-size:30px;\"}\n:::\n\n::: {.column width=\"70%\"}\n```{r, fig.width = 7}\n#| code-fold: true\nggplot(data.frame(x = seq(-4, 4)), aes(x)) +\n  stat_function(fun = function(x) dnorm(x), \n                aes(color = \"Normal\", linetype = \"Normal\")) +\n  stat_function(fun = function(x) dt(x, df = 1), \n                aes(color = \"t(1)\", linetype = \"t(1)\")) +\n  stat_function(fun = function(x) dt(x, df = 5), \n                aes(color = \"t(5)\", linetype = \"t(5)\")) +\n  stat_function(fun = function(x) dt(x, df = 25), \n                aes(color = \"t(25)\", linetype = \"t(25)\")) +\n  stat_function(fun = function(x) dt(x, df = 100), \n                aes(color = \"t(100)\", linetype = \"t(100)\")) + \n  scale_x_continuous(\"Variable X\") +\n  scale_y_continuous(\"Density\") +\n  scale_color_manual(\"\", \n                     values = c(\"red\", \"black\", \"black\", \"blue\", \"blue\")) +\n  scale_linetype_manual(\"\", \n                        values = c(\"solid\", \"solid\", \"dashed\", \"solid\", \"dashed\")) +\n  ggtitle(\"The Normal and t Distributions\") +\n  theme(text = element_text(size=20),legend.position = \"bottom\")\n```\n:::\n:::\n\n------------------------------------------------------------------------\n\n[For small samples, we need to use the t distribution with its fatter tails. This produces wider confidence intervals---the penalty we have to pay for our ignorance about the population.<br>The form of the confidence interval remains the same. We simply substitute a corresponding value from the t distribution (using df =$N -1$).]{style=\"font-size:28px;\"}\n\n$$CI_{95} = \\bar{X} \\pm [1.96\\frac{\\hat{\\sigma}}{\\sqrt{N}}]$$\n\n$$CI_{95} = \\bar{X} \\pm [Z_{.975}\\frac{\\hat{\\sigma}}{\\sqrt{N}}]$$\n\n$$CI_{95} = \\bar{X} \\pm [t_{.975, df = N-1}\\frac{\\hat{\\sigma}}{\\sqrt{N}}]$$\n\n------------------------------------------------------------------------\n\n::: nonincremental\n-   The meaning of the confidence interval can be a bit confusing and arises from the peculiar language forced on us by the frequentist viewpoint.\n-   The CI DOES NOT mean \"there is a 95% probability that the true mean lies inside the confidence interval.\"\n-   It means that if we carried out random sampling from the population a large number of times, and calculated the 95% confidence interval each time, then 95% of those intervals can be expected to contain the population mean.\n:::\n\n------------------------------------------------------------------------\n\nIn previous years, incoming first year graduate students had an average coffee consumption of 7.6 cups per week and a standard deviation of 2.4.\n\nThe next incoming class will have 43 students. What range of exam means would be plausible if this class is similar to past classes (comes from the same population)?\n\n$$\\bar{X} - (1.96\\times SEM) \\leq \\mu \\leq \\bar{X} + (1.96\\times SEM) $$\n\n------------------------------------------------------------------------\n\nCalculated with the ***normal distribution***\n\n```{r}\nM <- 7.6 \nSD <- 2.4\nN <- 43\nsem <- SD/sqrt(N)\n\nci_lb_z <- M - (sem * qnorm(p = .975))\nci_ub_z <-  M + (sem * qnorm(p = .975))\nc(ci_lb_z, ci_ub_z)\n```\n\nCalculated with the ***t-distribution***\n\n```{r}\nM <- 7.6 \nSD <- 2.4\nN <- 43\nsem <- SD/sqrt(N)\n\n\nci_lb_t <- M - (sem * qt(p = .975, df = N-1))\nci_ub_t <- M + (sem * qt(p = .975, df = N-1))\nc(ci_lb_t, ci_ub_t)\n```\n\nWhat do you notice about the two?\n\n# Lab 2 Time\n\n::: nonincremental\n-   Using Rainbow Parentheses\n\n-   Visual Editor & Knitting document\n:::\n"},"formats":{"revealjs":{"identifier":{"display-name":"RevealJS","target-format":"revealjs","base-format":"revealjs"},"execute":{"fig-width":10,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":false,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"wrap","code-link":false,"code-line-numbers":true,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","html-math-method":{"method":"mathjax","url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML-full"},"slide-level":2,"to":"revealjs","incremental":true,"output-file":"05_Pre-Lab.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":false,"quarto-version":"1.3.433","auto-stretch":true,"editor":"visual","title":"Wk 5 - Hypothesis & Power","subtitle":"PSYC 640 - Fall 2023","author":"Dustin Haraden","editor_options":{"chunk_output_type":"console"},"multiplex":true,"slideNumber":true,"touch":true,"theme":"night"}}},"projectFormats":["html"]}