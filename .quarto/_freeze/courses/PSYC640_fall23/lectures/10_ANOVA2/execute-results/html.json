{
  "hash": "2b1618866704929fbdcc33b207fcfc7d",
  "result": {
    "markdown": "---\ntitle: \"Two-Way ANOVA\"\nsubtitle: \"PSYC 640 - Fall 2023\"\nauthor: \"Dustin Haraden, PhD\"\nformat: \n  revealjs:\n    multiplex: true\n    scrollable: true\n    slide-number: true\n    incremental: false\n    touch: true\n    code-overflow: wrap\n    theme: dark\nexecute: \n  echo: true\neditor: visual\neditor_options: \n  chunk_output_type: console\n---\n\n\n\n\n## Last Class\n\n::: nonincremental\n-   One Way ANOVA\n    -   Comparing means across multiple groups/levels\n:::\n\n------------------------------------------------------------------------\n\n## Looking Ahead\n\n-   R-Workshop! ([Link to Sign up](https://docs.google.com/forms/d/e/1FAIpQLSd2Gjcb88kWHBS7LXNDaLCZ9Sb7aroRceMjrLl5K36epewdtQ/viewform?usp=sf_link))\n    -   11/3 & 12/1 from 2-3pm\n-   Professor will put together guidelines for the final project and have dates to complete various components\n-   Lab 3 - Starting Today will be due before class on 11/6\n    -   Will be able to resubmit for any missed items up to 1 week from when I post the initial feedback\n-   Correlation & Regression\n\n------------------------------------------------------------------------\n\n## Today...\n\nOne-Way ANOVA\n\n-   Contrasts/post-hoc tests\n\nTwo-Way ANOVA\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# File management\nlibrary(here)\n# for dplyr, ggplot2\nlibrary(tidyverse)\n#Loading data\nlibrary(rio)\n# Estimating Marginal Means\nlibrary(emmeans)\n# Pretty Tables\nlibrary(kableExtra)\n\n#Remove Scientific Notation \noptions(scipen=999)\n```\n:::\n\n\n# Review of One-Way ANOVA\n\n## One-Way ANOVA\n\nGoal: Inform of differences among the levels of our variable of interest (Omnibus Test)\n\nUsing the between and within group variance to create the $F$-statistic/ratio\n\nHypotheses:\n\n$$\nH_0: it\\: is\\: true\\: that\\: \\mu_1 = \\mu_2 = \\mu_3 =\\: ...\\mu_k \n\\\\ \nH_1: it\\: is\\: \\boldsymbol{not}\\: true\\: that\\: \\mu_1 = \\mu_2 = \\mu_3 =\\: ...\\mu_k\n$$\n\n------------------------------------------------------------------------\n\n![](/images/ANOVA.png){width=\"729\"}\n\n::: columns\n::: {.column width=\"50%\"}\n$F = \\frac{MS_{between}}{MS_{within}} = \\frac{small}{large} < 1$\n:::\n\n::: {.column width=\"50%\"}\n$F = \\frac{MS_{between}}{MS_{within}} = \\frac{large}{small} > 1$\n:::\n:::\n\n## Review of the NHST process\n\n1.  Collect Sample and define hypotheses\n\n2.  Set alpha level\n\n3.  Determine the sampling distribution (will be using $F$-distribution now)\n\n4.  Identify the critical value\n\n5.  Calculate test statistic for sample collected\n\n6.  Inspect & compare statistic to critical value; Calculate probability\n\n## Steps to calculating F-ratio\n\n1.  Variance to Sum of Squares (Between & Within)\n2.  Degrees of Freedom\n3.  Mean squares values\n4.  F-Statistic\n\n## Calculating the F-Statistic\n\n$$F = \\frac{MS_b}{MS_w}$$\n\nIf the null hypothesis is true, $F$ has an expected value close to 1 (numerator and denominator are estimates of the same variability)\n\nIf it is false, the numerator will likely be larger, because systematic, between-group differences contribute to the variance of the means, but not to variance within group.\n\n------------------------------------------------------------------------\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\ndata.frame(F = c(0,8)) %>%\n  ggplot(aes(x = F)) +\n  stat_function(fun = function(x) df(x, df1 = 3, df2 = 196), \n                geom = \"line\") +\n  stat_function(fun = function(x) df(x, df1 = 3, df2 = 196), \n                geom = \"area\", xlim = c(2.65, 8), fill = \"purple\") +\n  geom_vline(aes(xintercept = 2.65), color = \"purple\") +\n  scale_y_continuous(\"Density\") + scale_x_continuous(\"F statistic\", breaks = NULL) +\n  theme_bw(base_size = 20)\n```\n\n::: {.cell-output-display}\n![](10_ANOVA2_files/figure-revealjs/unnamed-chunk-3-1.png){width=960}\n:::\n:::\n\n\nIf data are normally distributed, then the variance is $\\chi^2$ distributed\n\n$F$-distributions are one-tailed tests. Recall that we're interested in how far away our test statistic from the null $(F = 1).$\n\n------------------------------------------------------------------------\n\n## ANOVA table\n\n+---------------------+----------+----------------+----------------------------+-------------------------+----------+\n| Source of Variation | df       | Sum of Squares | Mean Squares               | F-statistic             | p-value  |\n+:===================:+:========:+:==============:+:==========================:+:=======================:+:========:+\n| Group               | $G-1$    | $SS_b$         | $MS_b = \\frac{SS_b}{df_b}$ | $F = \\frac{MS_b}{MS_w}$ | $p$      |\n+---------------------+----------+----------------+----------------------------+-------------------------+----------+\n| Residual            | $N-G$    | $SS_w$         | $MS_w = \\frac{SS_w}{df_w}$ |                         |          |\n+---------------------+----------+----------------+----------------------------+-------------------------+----------+\n| Total               | $N-1$    | $SS_{total}$   |                            |                         |          |\n+---------------------+----------+----------------+----------------------------+-------------------------+----------+\n\n# Contrasts/Post-Hoc\n\n------------------------------------------------------------------------\n\n## Contrasts/Post-Hoc Tests\n\nPerformed when there is a significant difference among the groups to examine which groups are different\n\n1.  **Contrasts**: When we have *a priori* hypotheses\n2.  **Post-hoc Tests**: When we want to test everything\n\n------------------------------------------------------------------------\n\nThese comparisons take the general form of t-tests, but note some extensions:\n\n-   the pooled variance estimate comes from $SS_{\\text{residual}}$, meaning it pulls information from all groups\n\n-   the degrees of freedom for the $t$-test is $N-k$, so using all data\n\n------------------------------------------------------------------------\n\n## Previous Spooky Data\n\nEMF rating across multiple locations\n\n\n::: {.cell}\n\n```{.r .cell-code}\nspooky <- import(\"https://raw.githubusercontent.com/dharaden/dharaden.github.io/main/courses/PSYC640_fall23/data/SS%20Calculations.csv\") %>% \n  select(Location, EMF) #There were some extra empty variables in there that we don't care about\n\nfit_1 <- aov(EMF ~ Location, data = spooky)\nsummary(fit_1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n             Df Sum Sq Mean Sq F value              Pr(>F)    \nLocation      7   5115   730.7   523.5 <0.0000000000000002 ***\nResiduals   138    193     1.4                                \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n\n------------------------------------------------------------------------\n\n## Post-hoc tests\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#library(emmeans)\nemmeans(fit_1, pairwise ~ Location, adjust = \"none\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n$emmeans\n Location          emmean    SE  df lower.CL upper.CL\n Abandoned Walmart  11.42 0.278 138    10.87    11.97\n Gettysburg         21.23 0.278 138    20.68    21.78\n Ikea               16.27 0.278 138    15.72    16.82\n RIT Tunnels         7.51 0.278 138     6.96     8.06\n Stats Classroom    12.35 0.271 138    11.82    12.89\n The Woods          25.01 0.278 138    24.46    25.56\n Unused Stairwell   14.23 0.278 138    13.68    14.78\n Walmart             6.81 0.271 138     6.28     7.35\n\nConfidence level used: 0.95 \n\n$contrasts\n contrast                             estimate    SE  df t.ratio p.value\n Abandoned Walmart - Gettysburg         -9.809 0.394 138 -24.910  <.0001\n Abandoned Walmart - Ikea               -4.847 0.394 138 -12.308  <.0001\n Abandoned Walmart - RIT Tunnels         3.913 0.394 138   9.937  <.0001\n Abandoned Walmart - Stats Classroom    -0.932 0.389 138  -2.399  0.0178\n Abandoned Walmart - The Woods         -13.587 0.394 138 -34.501  <.0001\n Abandoned Walmart - Unused Stairwell   -2.809 0.394 138  -7.134  <.0001\n Abandoned Walmart - Walmart             4.607 0.389 138  11.857  <.0001\n Gettysburg - Ikea                       4.962 0.394 138  12.601  <.0001\n Gettysburg - RIT Tunnels               13.723 0.394 138  34.847  <.0001\n Gettysburg - Stats Classroom            8.877 0.389 138  22.845  <.0001\n Gettysburg - The Woods                 -3.777 0.394 138  -9.592  <.0001\n Gettysburg - Unused Stairwell           7.000 0.394 138  17.775  <.0001\n Gettysburg - Walmart                   14.417 0.389 138  37.101  <.0001\n Ikea - RIT Tunnels                      8.760 0.394 138  22.246  <.0001\n Ikea - Stats Classroom                  3.915 0.389 138  10.074  <.0001\n Ikea - The Woods                       -8.740 0.394 138 -22.193  <.0001\n Ikea - Unused Stairwell                 2.038 0.394 138   5.174  <.0001\n Ikea - Walmart                          9.454 0.389 138  24.330  <.0001\n RIT Tunnels - Stats Classroom          -4.846 0.389 138 -12.470  <.0001\n RIT Tunnels - The Woods               -17.500 0.394 138 -44.439  <.0001\n RIT Tunnels - Unused Stairwell         -6.723 0.394 138 -17.072  <.0001\n RIT Tunnels - Walmart                   0.694 0.389 138   1.786  0.0763\n Stats Classroom - The Woods           -12.654 0.389 138 -32.565  <.0001\n Stats Classroom - Unused Stairwell     -1.877 0.389 138  -4.831  <.0001\n Stats Classroom - Walmart               5.540 0.383 138  14.453  <.0001\n The Woods - Unused Stairwell           10.777 0.394 138  27.367  <.0001\n The Woods - Walmart                    18.194 0.389 138  46.821  <.0001\n Unused Stairwell - Walmart              7.417 0.389 138  19.087  <.0001\n```\n:::\n:::\n\n\n------------------------------------------------------------------------\n\n## Family-wise error\n\nThese pairwise comparisons can quickly grow in number as the number of Groups increases. With 8 (k) Groups, we have k(k-1)/2 = 28 possible pairwise comparisons.\n\nAs the number of groups in the ANOVA grows, the number of possible pairwise comparisons increases dramatically.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](10_ANOVA2_files/figure-revealjs/unnamed-chunk-6-1.png){width=960}\n:::\n:::\n\n\n------------------------------------------------------------------------\n\nAs the number of tests grows, and assuming the null hypothesis is true, the probability that we will make one or more Type I errors increases. To approximate the magnitude of the problem, we can assume that the multiple pairwise comparisons are independent. The probability that we **don't** make a Type I error for [**one**]{.underline} test is:\n\n$$P(\\text{No Type I}, 1 \\text{ test}) = 1-\\alpha$$\n\n------------------------------------------------------------------------\n\nThe probability that we don't make a Type I error for [**two**]{.underline} tests is:\n\n$$P(\\text{No Type I}, 2 \\text{ test}) = (1-\\alpha)(1-\\alpha)$$\n\nFor C tests, the probability that we make **no** Type I errors is\n\n$$P(\\text{No Type I}, C \\text{ tests}) = (1-\\alpha)^C$$\n\nWe can then use the following to calculate the probability that we make one or more Type I errors in a collection of C independent tests.\n\n$$P(\\text{At least one Type I}, C \\text{ tests}) = 1-(1-\\alpha)^C$$\n\n------------------------------------------------------------------------\n\nThe Type I error inflation that accompanies multiple comparisons motivates the large number of \"correction\" procedures that have been developed.\n\n\n::: {.cell fight.height='6'}\n::: {.cell-output-display}\n![](10_ANOVA2_files/figure-revealjs/unnamed-chunk-7-1.png){width=960}\n:::\n:::\n\n\n------------------------------------------------------------------------\n\nMultiple comparisons, each tested with $\\alpha_{per-test}$, increases the family-wise $\\alpha$ level.\n\n$$\\large \\alpha_{family-wise} = 1 - (1-\\alpha_{per-test})^C$$ Šidák showed that the family-wise a could be controlled to a desired level (e.g., .05) by changing the $\\alpha_{per-test}$ to:\n\n$$\\large \\alpha_{per-wise} = 1 - (1-\\alpha_{family-wise})^{\\frac{1}{C}}$$\n\n------------------------------------------------------------------------\n\n### Bonferroni\n\nBonferroni (and Dunn, and others) suggested this simple approximation:\n\n$$\\large \\alpha_{per-test} = \\frac{\\alpha_{family-wise}}{C}$$\n\nThis is typically called the Bonferroni correction and is very often used even though better alternatives are available.\n\n------------------------------------------------------------------------\n\n\n::: {.cell}\n\n```{.r .cell-code}\nemmeans(fit_1, pairwise ~ Location, adjust = \"bonferroni\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n$emmeans\n Location          emmean    SE  df lower.CL upper.CL\n Abandoned Walmart  11.42 0.278 138    10.87    11.97\n Gettysburg         21.23 0.278 138    20.68    21.78\n Ikea               16.27 0.278 138    15.72    16.82\n RIT Tunnels         7.51 0.278 138     6.96     8.06\n Stats Classroom    12.35 0.271 138    11.82    12.89\n The Woods          25.01 0.278 138    24.46    25.56\n Unused Stairwell   14.23 0.278 138    13.68    14.78\n Walmart             6.81 0.271 138     6.28     7.35\n\nConfidence level used: 0.95 \n\n$contrasts\n contrast                             estimate    SE  df t.ratio p.value\n Abandoned Walmart - Gettysburg         -9.809 0.394 138 -24.910  <.0001\n Abandoned Walmart - Ikea               -4.847 0.394 138 -12.308  <.0001\n Abandoned Walmart - RIT Tunnels         3.913 0.394 138   9.937  <.0001\n Abandoned Walmart - Stats Classroom    -0.932 0.389 138  -2.399  0.4972\n Abandoned Walmart - The Woods         -13.587 0.394 138 -34.501  <.0001\n Abandoned Walmart - Unused Stairwell   -2.809 0.394 138  -7.134  <.0001\n Abandoned Walmart - Walmart             4.607 0.389 138  11.857  <.0001\n Gettysburg - Ikea                       4.962 0.394 138  12.601  <.0001\n Gettysburg - RIT Tunnels               13.723 0.394 138  34.847  <.0001\n Gettysburg - Stats Classroom            8.877 0.389 138  22.845  <.0001\n Gettysburg - The Woods                 -3.777 0.394 138  -9.592  <.0001\n Gettysburg - Unused Stairwell           7.000 0.394 138  17.775  <.0001\n Gettysburg - Walmart                   14.417 0.389 138  37.101  <.0001\n Ikea - RIT Tunnels                      8.760 0.394 138  22.246  <.0001\n Ikea - Stats Classroom                  3.915 0.389 138  10.074  <.0001\n Ikea - The Woods                       -8.740 0.394 138 -22.193  <.0001\n Ikea - Unused Stairwell                 2.038 0.394 138   5.174  <.0001\n Ikea - Walmart                          9.454 0.389 138  24.330  <.0001\n RIT Tunnels - Stats Classroom          -4.846 0.389 138 -12.470  <.0001\n RIT Tunnels - The Woods               -17.500 0.394 138 -44.439  <.0001\n RIT Tunnels - Unused Stairwell         -6.723 0.394 138 -17.072  <.0001\n RIT Tunnels - Walmart                   0.694 0.389 138   1.786  1.0000\n Stats Classroom - The Woods           -12.654 0.389 138 -32.565  <.0001\n Stats Classroom - Unused Stairwell     -1.877 0.389 138  -4.831  0.0001\n Stats Classroom - Walmart               5.540 0.383 138  14.453  <.0001\n The Woods - Unused Stairwell           10.777 0.394 138  27.367  <.0001\n The Woods - Walmart                    18.194 0.389 138  46.821  <.0001\n Unused Stairwell - Walmart              7.417 0.389 138  19.087  <.0001\n\nP value adjustment: bonferroni method for 28 tests \n```\n:::\n:::\n\n\n------------------------------------------------------------------------\n\nThe Bonferroni procedure is conservative. Other correction procedures have been developed that control family-wise Type I error at .05 but that are more powerful than the Bonferroni procedure. The most common one is the Holm procedure.\n\nThe Holm procedure does not make a constant adjustment to each per-test $\\alpha$. Instead it makes adjustments in stages depending on the relative size of each pairwise p-value.\n\n------------------------------------------------------------------------\n\n### Holm correction\n\n1.  Rank order the p-values from largest to smallest.\n2.  Start with the smallest p-value. Multiply it by its rank.\n3.  Go to the next smallest p-value. Multiply it by its rank. If the result is larger than the adjusted p-value of next smallest rank, keep it. Otherwise replace with the previous step adjusted p-value.\n4.  Repeat Step 3 for the remaining p-values.\n5.  Judge significance of each new p-value against $\\alpha = .05$.\n\n------------------------------------------------------------------------\n\n\n::: {.cell-output-display}\n`````{=html}\n<table>\n <thead>\n  <tr>\n   <th style=\"text-align:right;\"> Original p value </th>\n   <th style=\"text-align:right;\"> Rank </th>\n   <th style=\"text-align:right;\"> Rank x p </th>\n   <th style=\"text-align:right;\"> Holm </th>\n   <th style=\"text-align:right;\"> Bonferroni </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:right;\"> 0.0012 </td>\n   <td style=\"text-align:right;\"> 6 </td>\n   <td style=\"text-align:right;\"> 0.0072 </td>\n   <td style=\"text-align:right;\"> 0.0072 </td>\n   <td style=\"text-align:right;\"> 0.0072 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 0.0023 </td>\n   <td style=\"text-align:right;\"> 5 </td>\n   <td style=\"text-align:right;\"> 0.0115 </td>\n   <td style=\"text-align:right;\"> 0.0115 </td>\n   <td style=\"text-align:right;\"> 0.0138 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 0.0450 </td>\n   <td style=\"text-align:right;\"> 4 </td>\n   <td style=\"text-align:right;\"> 0.1800 </td>\n   <td style=\"text-align:right;\"> 0.1800 </td>\n   <td style=\"text-align:right;\"> 0.2700 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 0.0470 </td>\n   <td style=\"text-align:right;\"> 3 </td>\n   <td style=\"text-align:right;\"> 0.1410 </td>\n   <td style=\"text-align:right;\"> 0.1800 </td>\n   <td style=\"text-align:right;\"> 0.2820 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 0.0530 </td>\n   <td style=\"text-align:right;\"> 2 </td>\n   <td style=\"text-align:right;\"> 0.1060 </td>\n   <td style=\"text-align:right;\"> 0.1800 </td>\n   <td style=\"text-align:right;\"> 0.3180 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 0.2100 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 0.2100 </td>\n   <td style=\"text-align:right;\"> 0.2100 </td>\n   <td style=\"text-align:right;\"> 1.0000 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n\n\n------------------------------------------------------------------------\n\n\n::: {.cell}\n\n```{.r .cell-code}\nemmeans(fit_1, pairwise ~ Location, adjust = \"holm\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n$emmeans\n Location          emmean    SE  df lower.CL upper.CL\n Abandoned Walmart  11.42 0.278 138    10.87    11.97\n Gettysburg         21.23 0.278 138    20.68    21.78\n Ikea               16.27 0.278 138    15.72    16.82\n RIT Tunnels         7.51 0.278 138     6.96     8.06\n Stats Classroom    12.35 0.271 138    11.82    12.89\n The Woods          25.01 0.278 138    24.46    25.56\n Unused Stairwell   14.23 0.278 138    13.68    14.78\n Walmart             6.81 0.271 138     6.28     7.35\n\nConfidence level used: 0.95 \n\n$contrasts\n contrast                             estimate    SE  df t.ratio p.value\n Abandoned Walmart - Gettysburg         -9.809 0.394 138 -24.910  <.0001\n Abandoned Walmart - Ikea               -4.847 0.394 138 -12.308  <.0001\n Abandoned Walmart - RIT Tunnels         3.913 0.394 138   9.937  <.0001\n Abandoned Walmart - Stats Classroom    -0.932 0.389 138  -2.399  0.0355\n Abandoned Walmart - The Woods         -13.587 0.394 138 -34.501  <.0001\n Abandoned Walmart - Unused Stairwell   -2.809 0.394 138  -7.134  <.0001\n Abandoned Walmart - Walmart             4.607 0.389 138  11.857  <.0001\n Gettysburg - Ikea                       4.962 0.394 138  12.601  <.0001\n Gettysburg - RIT Tunnels               13.723 0.394 138  34.847  <.0001\n Gettysburg - Stats Classroom            8.877 0.389 138  22.845  <.0001\n Gettysburg - The Woods                 -3.777 0.394 138  -9.592  <.0001\n Gettysburg - Unused Stairwell           7.000 0.394 138  17.775  <.0001\n Gettysburg - Walmart                   14.417 0.389 138  37.101  <.0001\n Ikea - RIT Tunnels                      8.760 0.394 138  22.246  <.0001\n Ikea - Stats Classroom                  3.915 0.389 138  10.074  <.0001\n Ikea - The Woods                       -8.740 0.394 138 -22.193  <.0001\n Ikea - Unused Stairwell                 2.038 0.394 138   5.174  <.0001\n Ikea - Walmart                          9.454 0.389 138  24.330  <.0001\n RIT Tunnels - Stats Classroom          -4.846 0.389 138 -12.470  <.0001\n RIT Tunnels - The Woods               -17.500 0.394 138 -44.439  <.0001\n RIT Tunnels - Unused Stairwell         -6.723 0.394 138 -17.072  <.0001\n RIT Tunnels - Walmart                   0.694 0.389 138   1.786  0.0763\n Stats Classroom - The Woods           -12.654 0.389 138 -32.565  <.0001\n Stats Classroom - Unused Stairwell     -1.877 0.389 138  -4.831  <.0001\n Stats Classroom - Walmart               5.540 0.383 138  14.453  <.0001\n The Woods - Unused Stairwell           10.777 0.394 138  27.367  <.0001\n The Woods - Walmart                    18.194 0.389 138  46.821  <.0001\n Unused Stairwell - Walmart              7.417 0.389 138  19.087  <.0001\n\nP value adjustment: holm method for 28 tests \n```\n:::\n:::\n\n\n# ANOVA is regression\n\nIn regression, we can accommodate categorical predictors. How does this compare to ANOVA?\n\n-   Same omnibus test of the model!\n\n\\*(Really the same model, but packaged differently.)\n\n-   When would you use one versus the other?\n\n------------------------------------------------------------------------\n\n::: columns\n::: {.column width=\"50%\"}\n**ANOVA**\n\n-   More traditional for 3+ groups\n\n-   Comparing/controlling multiple categorical variables\n:::\n\n::: {.column width=\"50%\"}\n**Regression**\n\n-   Best for two groups\n\n-   Incorporating continuous predictors too\n\n-   Good for 3+ groups when you have more specific hypotheses (contrasts)\n:::\n:::\n\n# Two-Way ANOVA\n\n------------------------------------------------------------------------\n\n## What is a Two-Way ANOVA?\n\nExamines the impact of ***2*** nominal/categorical variables on a continuous outcome\n\nWe can now examine:\n\n-   The impact of variable 1 on the outcome (Main Effect)\n\n-   The impact of variable 2 on the outcome (Main Effect)\n\n-   The *interaction* of variable 1 & 2 on the outcome (Interaction Effect)\n\n    ::: incremental\n    -   The effect of variable 1 ***depends*** on the level of variable 2\n    :::\n\n------------------------------------------------------------------------\n\n## Two-Way ANOVA: Assumptions\n\nSame as what we've examined previously, plus a couple more:\n\n::: columns\n::: {.column width=\"50%\"}\n-   Independence\n\n-   Normality of Residuals\n\n-   Homoscedasticity (Homogeneity of Variance)\n:::\n\n::: {.column width=\"50%\" style=\"font-size: 30px\"}\n::: incremental\n-   Additivity\n\n    -   The effects of each factor are consistent across all levels of the other factor\n\n-   Multicollinearity\n\n    -   Correlations between factors. This can make it difficult to separate unique contributions to the outcome\n\n-   Equal Cell Sizes (N)\n:::\n:::\n:::\n\n------------------------------------------------------------------------\n\n## Main Effect & Interactions\n\nMain Effect: Basically a one-way ANOVA\n\n-   The effect of variable 1 is the same across all levels of variable 2\n\nInteraction:\n\n-   Able to examine the effect of variable 1 across different levels of variable 2\n\n-   Basically speaking, **the effect of variable 1 on our outcome *DEPENDS on the levels of variable 2***\n\n# Example Data\n\n------------------------------------------------------------------------\n\n## Data\n\n::: {style=\"font-size: 30px\"}\nWe are interested on the impact of phone usage on overall sleep quality\n\nWe include 2 variables of interest: 1) Phone Type (iOS vs. Android) and 2) Amount of usage (High, Medium & Low) to examine if there are differences in terms of sleep quality\n\n*Note: It is important to consider [HOW]{.underline} we operationalize constructs as some things we have as factors could easily be continuous variables*\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\n#Generate some data\n\n# Set random seed for reproducibility\nset.seed(42)\n\nn <- 500  # Set number of observations\n\n# Generate Type of Phone data\nphone_type <- sample(c(\"Android\", \"iOS\"), \n                     n, \n                     replace = TRUE)\n\n# Generate Phone Usage data\nphone_usage <- factor(sample(c(\"Low\", \"Medium\", \"High\"), \n                             n, \n                             replace = TRUE), \n                      levels= c(\"Low\", \"Medium\", \"High\"))\n\n# Generate Sleep Quality data (with some variation)\n# Intentionally inflating to highlight main effects\nsleep_quality <- round(\n  rnorm(n, mean = ifelse(phone_type == \"Android\", 5, 7) + ifelse(phone_usage == \"High\", 1, -1), sd = 1),\n  1\n)\n\n# Generate Sleep Quality data (with some variation)\nsleep_quality2 <- round(rnorm(n, mean = 6, sd = 3),1)\n\n# Create a data frame\nsleep_data <- data.frame(phone_type, \n                         phone_usage, \n                         sleep_quality,\n                         sleep_quality2)\n\nhead(sleep_data)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  phone_type phone_usage sleep_quality sleep_quality2\n1    Android        High           5.0           12.5\n2    Android         Low           3.8           12.8\n3    Android      Medium           3.9            8.7\n4    Android      Medium           3.3            5.8\n5        iOS         Low           5.6            8.0\n6        iOS        High           7.8            3.6\n```\n:::\n:::\n\n\n------------------------------------------------------------------------\n\n## Test Statistics\n\nWe've gone too far today without me showing some equations\n\nWith one way anova, we calculated the $SS_{between}$ and the $SS_{within}$ and were able to use those to capture the F-statistic\n\nNow we have another variable to take into account. Therefore, we need to calculate:\n\n$$\nSS_{between Group1}, \\: SS_{between Group2}\n$$\n\n$$\nSS_{within}, \\: SS_{total}\n$$\n\n------------------------------------------------------------------------\n\n## $F$-Statistic/Ratio\n\nHelpful site for hand calculations: ([link](https://www.statology.org/two-way-anova-by-hand/))\n\n-   Since we have these various sum of squares, we can then fill out the ANOVA table\n\n-   This also means that we will have *multiple* F-Statistics\n\n------------------------------------------------------------------------\n\n+---------------------+-------------+----------------------------------+-------------------------------------+---------------------------------+----------+\n| Source of Variation | df          | Sum of Squares                   | Mean Squares                        | F-statistic                     | p-value  |\n+:===================:+:===========:+:================================:+:===================================:+:===============================:+:========:+\n| Group1              | $j-1$       | $SS_{b1}$                        | $MS_{b1} = \\frac{SS_{b1}}{df_{b1}}$ | $F_1 = \\frac{MS_{b1}}{MS_{w1}}$ | $p_1$    |\n+---------------------+-------------+----------------------------------+-------------------------------------+---------------------------------+----------+\n| Group2              | $k-1$       | $SS_{b2}$                        | $MS_{b2} = \\frac{SS_{b2}}{df_{b2}}$ | $F_2 = \\frac{MS_{b2}}{MS_{w2}}$ | $p_2$    |\n+---------------------+-------------+----------------------------------+-------------------------------------+---------------------------------+----------+\n| Interaction         | $df_1*df_2$ | ::: {.fragment .highlight-green} | $MS_{b3} = \\frac{SS_{b3}}{df_{b3}}$ | $F_2 = \\frac{MS_{b3}}{MS_{w3}}$ | $p_3$    |\n|                     |             | $SS_{int}$                       |                                     |                                 |          |\n|                     |             | :::                              |                                     |                                 |          |\n+---------------------+-------------+----------------------------------+-------------------------------------+---------------------------------+----------+\n| Residual (within)   | $N-G$       | $SS_w$                           | $MS_w = \\frac{SS_w}{df_w}$          |                                 |          |\n+---------------------+-------------+----------------------------------+-------------------------------------+---------------------------------+----------+\n| Total               | $N-1$       | $SS_{total}$                     |                                     |                                 |          |\n+---------------------+-------------+----------------------------------+-------------------------------------+---------------------------------+----------+\n\n------------------------------------------------------------------------\n\n## Calculate $SS_{interaction}$\n\nNow we need to be able to take into account the interaction term\n\nThis is done by calculating all other $SS$ and then performing:\n\n$$\nSS_{interaction} = SS_{total} - SS_{b1} - SS_{b2} - SS_{w}\n$$\n\n# Example: Calculate the $SS$ for the new data\n\n::: incremental\n-   no...we aren't doing that\n:::\n\n------------------------------------------------------------------------\n\n# Running in R\n\nTake a peak at the dataset again so we can look at the variables and their names\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhead(sleep_data)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  phone_type phone_usage sleep_quality sleep_quality2\n1    Android        High           5.0           12.5\n2    Android         Low           3.8           12.8\n3    Android      Medium           3.9            8.7\n4    Android      Medium           3.3            5.8\n5        iOS         Low           5.6            8.0\n6        iOS        High           7.8            3.6\n```\n:::\n:::\n\n\n------------------------------------------------------------------------\n\n## Running in R\n\nWe will use the `aov()` function to set up our model\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit2 <- aov(sleep_quality ~ phone_type + phone_usage + phone_type*phone_usage, \n            data = sleep_data)\n\n#OR \n\nfit2 <- aov(sleep_quality ~ phone_type * phone_usage, \n            data = sleep_data)\n\nsummary(fit2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                        Df Sum Sq Mean Sq F value              Pr(>F)    \nphone_type               1  544.6   544.6 515.727 <0.0000000000000002 ***\nphone_usage              2  498.7   249.4 236.148 <0.0000000000000002 ***\nphone_type:phone_usage   2    0.7     0.3   0.317               0.728    \nResiduals              494  521.6     1.1                                \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n\n------------------------------------------------------------------------\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nggplot(sleep_data, aes(x = phone_usage, \n                       y = sleep_quality, \n                       color = phone_type, \n                       group = phone_type)) +\n  geom_point() +\n  geom_smooth(method = \"lm\",\n              se=FALSE) +\n  labs(x = \"Phone Usage\", \n       y = \"Sleep Quality\", \n       color = \"Phone Type\") +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](10_ANOVA2_files/figure-revealjs/unnamed-chunk-14-1.png){width=960}\n:::\n:::\n\n\n## Different Outcome\n\nCreated the outcome of `sleep_quality2` to be completely random instead of following a formula so that we could visualize the effect\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit3 <- aov(sleep_quality2 ~ phone_type * phone_usage, \n            data = sleep_data)\nsummary(fit3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                        Df Sum Sq Mean Sq F value Pr(>F)   \nphone_type               1     86   85.65   9.162 0.0026 **\nphone_usage              2     56   27.83   2.977 0.0519 . \nphone_type:phone_usage   2      8    4.18   0.447 0.6399   \nResiduals              494   4618    9.35                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n\n------------------------------------------------------------------------\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create an interaction plot using ggplot2\nggplot(sleep_data, aes(x = phone_usage, y = sleep_quality2, color = phone_type, group = phone_type)) +\n  geom_point() +\n  geom_smooth(method = \"lm\",\n              se=FALSE) +\n  labs(x = \"Phone Usage\", y = \"Sleep Quality\", color = \"Phone Type\") +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](10_ANOVA2_files/figure-revealjs/unnamed-chunk-16-1.png){width=960}\n:::\n:::\n\n\n# Next time...\n\n::: incremental\n-   ¯\\\\\\_(ツ)\\_/¯\n\n-   Correlation & Regression\n:::\n",
    "supporting": [
      "10_ANOVA2_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\r\n<script>\r\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\r\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\r\n  // slide changes (different for each slide format).\r\n  (function () {\r\n    // dispatch for htmlwidgets\r\n    function fireSlideEnter() {\r\n      const event = window.document.createEvent(\"Event\");\r\n      event.initEvent(\"slideenter\", true, true);\r\n      window.document.dispatchEvent(event);\r\n    }\r\n\r\n    function fireSlideChanged(previousSlide, currentSlide) {\r\n      fireSlideEnter();\r\n\r\n      // dispatch for shiny\r\n      if (window.jQuery) {\r\n        if (previousSlide) {\r\n          window.jQuery(previousSlide).trigger(\"hidden\");\r\n        }\r\n        if (currentSlide) {\r\n          window.jQuery(currentSlide).trigger(\"shown\");\r\n        }\r\n      }\r\n    }\r\n\r\n    // hookup for slidy\r\n    if (window.w3c_slidy) {\r\n      window.w3c_slidy.add_observer(function (slide_num) {\r\n        // slide_num starts at position 1\r\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\r\n      });\r\n    }\r\n\r\n  })();\r\n</script>\r\n\r\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}