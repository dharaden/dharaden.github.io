---
title: "Multiple Regression: A story of betas"
subtitle: "PSYC 640 - Fall 2023"
author: "Dustin Haraden, PhD"
format: 
  revealjs:
    multiplex: true
    scrollable: true
    slide-number: true
    incremental: false
    touch: true
    code-overflow: wrap
    highlightLines: true
    theme: dark
execute: 
  echo: true
editor: visual
editor_options: 
  chunk_output_type: console
---

```{r, include = F}
knitr::opts_chunk$set(message = FALSE, warning = FALSE)
```

```{r, echo = F, results='hide',warning=FALSE,message=FALSE}
options(scipen = 999)

library(knitr)
# function to display only part of the output
hook_output <- knit_hooks$get("output")
knit_hooks$set(output = function(x, options) {
  lines <- options$output.lines
  if (is.null(lines)) {
    return(hook_output(x, options))  # pass to default hook
  }
  x <- unlist(strsplit(x, "\n"))
  more <- "..."
  if (length(lines)==1) {        # first n lines
    if (length(x) > lines) {
      # truncate the output, but add ....
      x <- c(head(x, lines), more)
    }
  } else {
    x <- c(more, x[lines], more)
  }
  # paste these lines together
  x <- paste(c(x, ""), collapse = "\n")
  hook_output(x, options)
})

knitr::opts_chunk$set(message = FALSE, 
                      warning = FALSE) # suppress messages
```

## Last Time

::: nonincremental
-   Coefficient of Determination
-   Coefficient of Alienation
:::

------------------------------------------------------------------------

## Today...

-   More Regression (but more details)

    -   Omnibus test

-   Multiple Regression? Comparing Models?

-   Probably some Group work

```{r, results = 'hide', message = F, warning = F}
options(scipen=999)
library(tidyverse)
library(rio)
library(broom)
library(psych)
library(easystats)
library(sjPlot)
library(car)
```

------------------------------------------------------------------------

### Data for Today

```{r}
school <- read_csv("https://raw.githubusercontent.com/dharaden/dharaden.github.io/main/data/example2-chisq.csv") %>% 
  mutate(Sleep_Hours_Non_Schoolnight = as.numeric(Sleep_Hours_Non_Schoolnight), 
         text_messages_yesterday = as.numeric(Text_Messages_Sent_Yesterday), 
         video_game_hrs = as.numeric(Video_Games_Hours),
         social_med_hrs = as.numeric(Social_Websites_Hours)) %>% 
  filter(Sleep_Hours_Non_Schoolnight < 24) #removing impossible values
```

------------------------------------------------------------------------

### Example

```{r}
#| highlight-output: "10"

fit.1 <- lm(Sleep_Hours_Non_Schoolnight ~ Ageyears, 
           data = school)
summary(fit.1) 
summary(fit.1)$r.squared
r2(fit.1)
```

------------------------------------------------------------------------

### Example

```{r}
school %>% 
  ggplot(aes(x = Ageyears, y = Sleep_Hours_Non_Schoolnight)) + 
  geom_point() + geom_smooth(method = "lm", se = F)
```

------------------------------------------------------------------------

### Example - `easystats`

```{r}
predicted_f1 <- estimate_expectation(fit.1, data = "grid")
plot(predicted_f1)

```

------------------------------------------------------------------------

### Example - `easystats` Check_model

```{r}
check_model(fit.1)
model_performance(fit.1)
```

------------------------------------------------------------------------

## Inferential tests

NHST is about making decisions:

-   these two means are/are not different
-   this correlation is/is not significant
-   the distribution of this categorical variable is/is not different between these groups

. . .

In regression, there are several inferential tests being conducted at once. The first is called the [omnibus test]{style="color:purple"} -- this is a test of whether the model fits the data.

------------------------------------------------------------------------

### Omnibus test

Historically we use [the *F* distribution]{style="color:purple"} to estimate the significance of our model, because it works with our ability to partition variance.

What is our null hypothesis?

. . .

**The model does not account for variance in** $Y$ (spoiler...ANOVA)

------------------------------------------------------------------------

## *F* Distributions and regression

*F* statistics are not testing the likelihood of differences; they test the likelihood of ratios. In this case, we want to determine whether the variance explained by our model is larger in magnitude than another variance.

Which variance?

------------------------------------------------------------------------

::: columns
::: {.column width="50%"}
$$\Large F_{\nu_1\nu_2} = \frac{\frac{\chi^2_{\nu_1}}{\nu_1}}{\frac{\chi^2_{\nu_2}}{\nu_2}}$$
:::

::: {.column width="50%"}
$$\Large F_{\nu_1\nu_2} = \frac{\frac{\text{Variance}_{\text{Model}}}{\nu_1}}{\frac{\text{Variance}_{\text{Residual}}}{\nu_2}}$$
:::
:::

$$\Large F = \frac{MS_{Model}}{MS_{residual}}$$

------------------------------------------------------------------------

::: columns
::: {.column width="50%"}
The degrees of freedom for our model are

$$DF_1 = k$$ $$DF_2 = N-k-1$$

Where k is the number of IV's in your model, and N is the sample size.
:::

::: {.column width="50%"}
Mean squares are calculated by taking the relevant Sums of Squares and dividing by their respective degrees of freedom.

-   $SS_{\text{Model}}$ is divided by $DF_1$

-   $SS_{\text{Residual}}$ is divided by $DF_2$
:::
:::

------------------------------------------------------------------------

```{r}
anova(fit.1)
```

------------------------------------------------------------------------

```{r,highlight.output=18}
summary(fit.1)
```

------------------------------------------------------------------------

## Mean square error (MSE)

-   [AKA]{style="font-size:80%"}[mean square residual]{style="color:purple"} and [mean square within]{style="color:purple"}

-   [unbiased estimate of error variance]{style="font-size:80%"}

    -   [measure of discrepancy between the data and the model]{style="font-size:80%"}

-   [the MSE is the variance around the fitted regression line]{style="font-size:80%"}

-   [Note: it is a transformation of the standard error of the estimate (and residual standard error)!]{style="font-size:80%"}

::: notes
What does it mean to be "unbiased"?

-   Variance estimates are biased because they are more likely to underestimate the true pop variance than overestimate.
:::

# Multiple Regression

## Regression Equation

Going from this:

$$
\hat{Y} = b_0 + b_1X1
$$

. . .

To this

$$
\hat{Y} = b_0 + b_1X_1 + b_2X_2 + ... + b_kX_k
$$

. . .

Regression coefficient values are now "partial" - Represents the contribution to all of outcome ($\hat{Y}$) from *unique* aspects of each $X$

------------------------------------------------------------------------

## Interpreting Coefficients

$$
\hat{Y} = b_0 + b_1X_1 + b_2X_2 + ... + b_kX_k
$$

-   Focus on a specific predictor (e.g., $X_1$)

-   For every 1 unit change in $X_1$, there is a $b_1$ unit change in $Y$, [**holding all other predictors constant**]{style="color:red"}

**Note**: Properties of OLS still hold true

-   The sum of residuals will be 0

-   Each predictor ($X$) will be uncorrelated with the residuals

------------------------------------------------------------------------

### Example - Multiple Predictors

Let's include another variable into our model to predict sleep

```{r}
fit.2 <- lm(Sleep_Hours_Non_Schoolnight ~ Ageyears + video_game_hrs + social_med_hrs, 
           data = school)
summary(fit.2)
```

------------------------------------------------------------------------

### Example - Coefficient interpretation

[sjPlot](https://strengejacke.github.io/sjPlot/articles/tab_model_estimates.html)

```{r}
sjPlot::tab_model(fit.2)
```

------------------------------------------------------------------------

### Example - Visualizing

How do we visualize this?

------------------------------------------------------------------------

### Example - Visualizing

How do we visualize this?

```{r}
car::avPlots(fit.2)
```

------------------------------------------------------------------------

## Holding Constant??? Wut

```{r, output.lines = 10:15}
summary(fit.2)
```

-   The average amount of sleep decreases by 0.08 hours for every 1 year older the youth his **holding the number of hours playing video games and the number of hours on social media constant.**

-   The average amount of sleep decreases by 0.004 hours for every 1 hour of social media use **holding age and hours of video game usage constant.**

What does this mean?? Also can be called "controlling for" or "taking into account" the other variables

Language comes from experimental research in which they can keep one condition unchanged while manipulating the other

------------------------------------------------------------------------

## Holding Constant - "Controlling for"

![](/images/control.gif){fig-align="center"}

Taken from [\@nickchk](https://twitter.com/nickchk)

------------------------------------------------------------------------

## Creating the Model

There can be many different ways in which we create and compare models with multiple predictors

::: incremental
-   **Simultaneous**: Enter all variables in the model at once
    -   Usually the most conservative and defensible approach (unless there is theory to support a hierarchical approach)
-   **Hierarchically**: Building a sequence of models where a single variable is included/excluded at each step
    -   This is **hierarchical/stepwise regression.** Different from HLM (Hierarchical Linear Modeling)
:::

------------------------------------------------------------------------

## Model Selection ([LSR 15.10](https://learningstatisticswithr.com/book/regression.html#modelselreg))

How can we tell if one model is "better" than the other (it explains more variance in outcome)?

-   Each predictor (or set of predictors) is investigated as to what it adds to the model when it is entered
-   The order of the variables depends on an *a priori* hypothesis

The concept is to ask how much variance is unexplained by our model. The leftovers can be compared to an alternate model to see if the new variable adds anything or if we should focus on **parsimony**

------------------------------------------------------------------------

### Model Comparison Example

```{r, , output.lines = 1:7}
m.2 <- lm(Sleep_Hours_Non_Schoolnight ~ Ageyears + video_game_hrs, 
           data = school)
m.3 <- lm(Sleep_Hours_Non_Schoolnight ~ Ageyears + video_game_hrs + social_med_hrs, 
           data = school)
anova(m.2, m.3)
```

```{r}
r2(m.2)
r2(m.3)
```

------------------------------------------------------------------------

### Model Comparison - sjPlot

```{r}
tab_model(m.2, m.3)
```

# Model Diagnostics

------------------------------------------------------------------------

## Checking the model ([LSR 15.9](https://learningstatisticswithr.com/book/regression.html#regressiondiagnostics))

Whenever we are looking at the regression model diagnostics, we are often considering the residual values. The types of residuals we can look at are:

::: incremental
-   Ordinary Residuals - Raw

-   Standardized Residuals

-   Studentized Residuals - Takes into account what the SD *would* have been with the removal of the $i$th observation
:::

------------------------------------------------------------------------

### Model Checks - Outlier

We tend to look for various things that may impact our results through the lens of residuals

**1) Outliers** - variables with high Studentized Residuals

![](/images/outlier.PNG){fig-align="center"}

------------------------------------------------------------------------

### Model Checks - Leverage

We tend to look for various things that may impact our results through the lens of residuals

**2) Leverage** - variable is different in all ways from others (not just residuals)

![](/images/leverage.PNG){fig-align="center"}

------------------------------------------------------------------------

### Model Checks - Influence

We tend to look for various things that may impact our results through the lens of residuals

**3) Influence** - outlier with high leverage (*Cook's Distance*)

![](/images/influence.PNG){fig-align="center"}

------------------------------------------------------------------------

### Model Checks - Plots

```{r}
#Cook's Distance
plot(m.3, which = 4)
```

------------------------------------------------------------------------

### Model Checks - Plots

```{r}
#Leverage
plot(m.3, which = 5)
```

------------------------------------------------------------------------

## Checking Collinearity

We need to check to see if our predictor variables are too highly correlated with each other

To do so we use ***variance inflation factors (VIF)***

-   There will be a VIF that is associated with each predictor in the model
-   Interpreting VIF ([link](https://www.statisticshowto.com/variance-inflation-factor/)) - Starts at 1 and goes up
    -   1 = Not Correlated
    -   1-5 = Moderately Correlated
    -   5+ = Highly Correlated

```{r}
car::vif(m.3)
```

------------------------------------------------------------------------

## `easystats` making stats easy

Can check the model with a simple function

```{r}
check_model(m.3)
```

# Rounding up Multiple Regression

It is a powerful and complex tool

# Next time...

-   More R fun
-   Group work

# Stop...Group Time

![](/images/mchammer.gif){fig-align="center"}
