---
title: "Comparing Means: t-tests"
subtitle: "PSYC 640 - Fall 2023"
author: "Dustin Haraden, PhD"
format: 
  revealjs:
    multiplex: true
    slide-number: true
    incremental: true
    touch: true
    code-overflow: wrap
    theme: dark
execute: 
  echo: true
editor: visual
editor_options: 
  chunk_output_type: console
---

```{r, include = F}
knitr::opts_chunk$set(message = FALSE, warning = FALSE)
```

## Last week

::: nonincremental
-   Categorical Data analysis with the $\chi^2$ distribution
    -   Test of Independence
    -   Goodness of Fit test
-   Single Sample $t$-test
:::

```{r, results = 'hide', message = F, warning = F}
# File management
library(here)
# for dplyr, ggplot
library(tidyverse)
# Making things look nice
library(ggpubr)
#Loading data
library(rio)
# Assumption Checks
library(car)

#Remove Scientific Notation 
options(scipen=999)

```

------------------------------------------------------------------------

## Today...

-   Comparing Means with the $t$-test
    -   Independent samples
    -   Paired Samples

------------------------------------------------------------------------

## Comparing Means

Calculated using a t-test. To calculate the t-statistic, you will use this formula:

$$t_{df=N-1} = \frac{\bar{X}-\mu}{\frac{\hat{\sigma}}{\sqrt{N}}}$$

The heavier tails of the t-distribution, especially for small N, are the penalty we pay for having to estimate the population standard deviation from the sample.

------------------------------------------------------------------------

### Load in the dataset from last class

```{r}
school <- import("https://raw.githubusercontent.com/dharaden/dharaden.github.io/main/data/example2-chisq.csv") %>%  
  mutate(Score_in_memory_game = as.numeric(Score_in_memory_game))
school <- school %>% 
  filter(!is.na(Score_in_memory_game))
```

------------------------------------------------------------------------

### One-sample *t*-tests vs Z-test

|                                             | Z-test                    | *t*-test                        |
|------------------|---------------------------|---------------------------|
| $\large{\mu}$                               | known                     | known                           |
| $\sigma$                                    | known                     | unknown                         |
| sem or $\sigma_M$                           | $\frac{\sigma}{\sqrt{N}}$ | $\frac{\hat{\sigma}}{\sqrt{N}}$ |
| Probability distribution                    | standard normal           | $t$                             |
| DF                                          | none                      | $N-1$                           |
| Tails                                       | One or two                | One or two                      |
| Critical value $(\alpha = .05, two-tailed)$ | 1.96                      | Depends on DF                   |

------------------------------------------------------------------------

### **Assumptions of the one-sample *t*-test**

**Normality.** We assume the sampling distribution of the mean is normally distributed. Under what two conditions can we be assured that this is true?

**Independence.** Observations in the dataset are not associated with one another. Put another way, collecting a score from Participant A doesn't tell me anything about what Participant B will say. How can we be safe in this assumption?

------------------------------------------------------------------------

### A brief example

Using the same Census at School data, we find that New York students who participated in a memory game ( $N = 224$ ) completed the game in an average time of 44.2 seconds ( $s = 15.3$ ). We know that the average US student completed the game in 45.04 seconds. How do our students compare? <br>

<br>

**Hypotheses**

$H_0: \mu = 45.05$

$H_1: \mu \neq 45.05$

------------------------------------------------------------------------

::: columns
::: {.column width="50%"}
$$\mu = 45.05$$

$$N = 227$$

$$ \bar{X} = 44.2 $$

$$ s = 15.3 $$

$$
\sigma = Unknown
$$
:::

::: {.column width="50%"}
```{r}
t.test(x = school$Score_in_memory_game, 
       mu = 45.05,
       alternative = "two.sided")
```
:::
:::

------------------------------------------------------------------------

```{r}
lsr::oneSampleTTest(x = school$Score_in_memory_game,
                    mu = 45.05, one.sided = FALSE)
```

------------------------------------------------------------------------

## Writing Up a t-test

> "A one-sample t-test was conducted to determine if the mean \[variable name\] differed from a hypothesized population mean of \[population mean\]. The sample mean was M = \[sample mean\], which was significantly \[greater than/less than/different from\] the hypothesized population mean, t(df) = \[t-value\], p = \[p-value\]."

A one-sample t-test was conducted to determine if the mean score in a memory game for NY students differed from the US population mean. The sample mean was $M = 44.164$ (SD = 15.32, CI = \[42.15, 46.18\]), which was not significantly different from the population mean, $t(223) = -0.87$, $p = 0.388$.

------------------------------------------------------------------------

Single sample t-tests are not used super often in practice

You will mainly see them when interpreting effect sizes of coefficients in your model

```{r}
#Load sleep data: https://vincentarelbundock.github.io/Rdatasets/datasets.html
sleep <- read_csv("https://raw.githubusercontent.com/dharaden/dharaden.github.io/main/data/sleepstudy.csv")
model = lm(Reaction ~ Days, data = sleep)
summary(model)
```

------------------------------------------------------------------------

## Types of t-Tests

Single Samples t-test

Independent Samples t-test

Paired Samples t-test

------------------------------------------------------------------------

## Types of t-Tests

~~Single Samples t-test~~

Independent Samples t-test

::: columns
::: {.column width="50%"}
-   Random Sampling

-   Independent observations
:::

::: {.column width="50%"}
-   Approximately normal distributions

-   Homogeneity of variances
:::
:::

Paired Samples t-test

-   Approximately normal distributions

-   Homogeneity of variances

------------------------------------------------------------------------

## Assumptions of t-Tests

Moving forward for today, we will use this dataset

::: nonincremental
-   100 students from New York

-   100 students from New Mexico
:::

```{r}

state_school <- import("https://raw.githubusercontent.com/dharaden/dharaden.github.io/main/data/NM-NY_CAS.csv") 

```

## Normality Assumption

1.  **Check for Normality**: Visualizing data (histograms), Q-Q plots, and statistical tests (Shapiro-Wilk, Anderson-Darling) to assess normality.

2.  **Remedies for Violations**: data transformation or non-parametric alternatives when data is not normally distributed.

------------------------------------------------------------------------

### Normality Assumption - Visualizing

Visualizing Data

```{r}
hist(as.numeric(state_school$Sleep_Hours_Schoolnight))

```

------------------------------------------------------------------------

Let's make it pretty

```{r}
state_school %>% 
  ggplot(aes(Sleep_Hours_Schoolnight,
             fill = Region)) +
  geom_bar(position = "dodge")

```

------------------------------------------------------------------------

### Normality Assumption --- Q-Q Plot

A Q-Q plot is a graphical method for assessing whether a dataset follows
a normal distribution. It compares the quantiles of your data to the
quantiles of a theoretical normal distribution. If your data follows a
normal distribution, the points in the Q-Q plot should form a straight
line.

```{r}
qqPlot(state_school$Sleep_Hours_Schoolnight)
```

------------------------------------------------------------------------

### Normality Assumption --- Shapiro-Wilk Test

Examines the Null Hypothesis that the data are normally distributed

```{r}
shapiro.test(state_school$Sleep_Hours_Schoolnight)
```

------------------------------------------------------------------------

### Normality Assumption 

-   Strict adherence to normality assumptions is not always necessary,

    -   Larger samples bring in Central Limit Theorem

-   However, assessing normality is still a valuable step in understanding distributions and potential impacts on your analyses

------------------------------------------------------------------------

### Failure of Normality Assumptions

What can we do if our data violate these normality assumptions?

-   Logarithmic Transformations

-   Square Root Transformations

-   Non-parametric tests

    -   **Mann-Whitney U Test (Wilcoxon Rank-Sum Test):** Used for comparing two independent groups

    ```{=html}
    <!-- -->
    ```
    -   **Wilcoxon Signed-Rank Test:** Used for comparing two paired or matched groups

------------------------------------------------------------------------

## Homogeneity of Variance

1.  **Check for Equality of Variances**: Levene's test to assess if variances are equal between groups

2.  **Remedies for Violations**: Welch's t-test for unequal variances.

------------------------------------------------------------------------

### Levene's Test

This test is used to examine if the variance is equal across groups. The Null Hypothesis is that the variances are equal

```{r}
# Perform Levene's test for equality of variances
leveneTest(Sleep_Hours_Schoolnight ~ Region, 
           data = state_school)
```

<br>

### Remedy for Violation

-   Welch's t-test (does not assume equal variances)

# Independent Samples t-test

[Chapter 13.3 in Learning Stats with R](https://learningstatisticswithr.com/book/ttest.html#studentttest)

Two different types: Student's & Welch's

-   Start with Student's t-test which assumes equal variances between the groups

$$
t = \frac{\bar{X_1} - \bar{X_2}}{SE}
$$

------------------------------------------------------------------------

## Student's t-test 

$$
H_0 : \mu_1 = \mu_2  \ \  H_1 : \mu_1 \neq \mu_2
$$

![](/images/student_H.png){fig-align="center"}

Stopping right here

<https://learningstatisticswithr.com/book/ttest.html#the-same-pooled-estimate-described-differently>

## Paired Samples t-Test in R

1.  **Using `t.test()` Function with Paired Data**: Show how to use the `t.test()` function with paired data and explain the `paired` argument.

2.  **Interpreting Output**: Discuss how to interpret the results of the paired samples t-test.

3.  **Practical Example in R**: Provide a hands-on example with paired data, demonstrating data preparation, t-test, and result interpretation.

------------------------------------------------------------------------

# Reporting and Interpreting Results

1.  **How to Present t-Test Results**: Explain how to report findings, including mean differences, confidence intervals, p-values, and effect size measures like Cohen's d.

2.  **Practical Interpretation**: Emphasize the importance of translating statistical results into practical implications. How do the findings impact decision-making?

------------------------------------------------------------------------

# Common Mistakes and Pitfalls

-   **Misinterpreting p-Values**: Discuss the common misconception of p-values as binary indicators of significance.

-   **Violations of Assumptions**: Highlight the potential consequences of violating assumptions and how to address them.

-   **Sample Size Considerations**: Explain the impact of sample size on t-test results and the importance of power analysis.

-   **Multiple Testing Issues**: Discuss the problem of inflated Type I error rates when conducting multiple t-tests and potential solutions (e.g., Bonferroni correction).

------------------------------------------------------------------------

# Additional Topics and Advanced t-Tests (if time allows)

-   **One-Sample t-Test**: Briefly introduce the one-sample t-test for comparing a sample mean to a known or hypothesized population mean.

-   **Welch's t-Test**: Explain Welch's t-test as an alternative for unequal variances.

-   **ANOVA and Post hoc tests**: Mention ANOVA for comparing means among multiple groups and briefly introduce post hoc tests for pairwise comparisons.

-   **Non-parametric alternatives (e.g., Mann-Whitney U test)**: Discuss non-parametric alternatives for situations where t-tests' assumptions are not met.

------------------------------------------------------------------------

# Practical Exercises (Hands-on)

-   Participants will perform t-tests on provided datasets using R.
-   Instruct participants to work in pairs or small groups, analyze data, and interpret results.

------------------------------------------------------------------------

# Q&A Session

-   Encourage participants to ask questions and seek clarification on t-test concepts, calculations, and practical applications.

------------------------------------------------------------------------

# Conclusion

1.  **Recap of Key Takeaways**: Summarize the key concepts covered in the lecture, including t-test types, assumptions, and interpretation.

2.  **Emphasize the Importance**: Reiterate the significance of using t-tests correctly and making informed decisions based on statistical analysis.

------------------------------------------------------------------------

# Additional Resources

-   Provide references, books, websites, and further reading material for participants interested in delving deeper into t-tests and statistics.

------------------------------------------------------------------------

# Closing Remarks

1.  **Thank the Participants**: Express gratitude for their active participation and engagement during the lecture.

2.  **Encourage Further Exploration**: Encourage participants to practice t-tests, explore real-world data, and continue their statistical learning journey.

## Next time...
