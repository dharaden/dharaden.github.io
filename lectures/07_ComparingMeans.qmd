---
title: "Comparing Means: t-tests"
subtitle: "PSYC 640 - Fall 2023"
author: "Dustin Haraden, PhD"
format: 
  revealjs:
    multiplex: true
    slide-number: true
    incremental: true
    touch: true
    code-overflow: wrap
    theme: dark
execute: 
  echo: true
editor: visual
editor_options: 
  chunk_output_type: console
---

```{r, include = F}
knitr::opts_chunk$set(message = FALSE, warning = FALSE)
```

## Last week

::: nonincremental
-   Categorical Data analysis with the $\chi^2$ distribution
    -   Test of Independence
    -   Goodness of Fit test
-   Single Sample $t$-test
:::

```{r, results = 'hide', message = F, warning = F}
# File management
library(here)
# for dplyr, ggplot
library(tidyverse)
# Making things look nice
library(ggpubr)
#Loading data
library(rio)
# Assumption Checks
library(car)

library(lsr)

#Remove Scientific Notation 
options(scipen=999)

```

------------------------------------------------------------------------

## Today...

-   Comparing Means with the $t$-test
    -   Independent samples
    -   Paired Samples

------------------------------------------------------------------------

## Comparing Means

Calculated using a t-test. To calculate the t-statistic, you will use this formula:

$$t_{df=N-1} = \frac{\bar{X}-\mu}{\frac{\hat{\sigma}}{\sqrt{N}}}$$

The heavier tails of the t-distribution, especially for small N, are the penalty we pay for having to estimate the population standard deviation from the sample.

------------------------------------------------------------------------

### Load in the dataset from last class

```{r}
school <- import("https://raw.githubusercontent.com/dharaden/dharaden.github.io/main/data/example2-chisq.csv") %>%  
  mutate(Score_in_memory_game = as.numeric(Score_in_memory_game))
school <- school %>% 
  filter(!is.na(Score_in_memory_game))
```

------------------------------------------------------------------------

### One-sample *t*-tests vs Z-test

|                                             | Z-test                    | *t*-test                        |
|---------------------------------------------|---------------------------|---------------------------------|
| $\large{\mu}$                               | known                     | known                           |
| $\sigma$                                    | known                     | unknown                         |
| sem or $\sigma_M$                           | $\frac{\sigma}{\sqrt{N}}$ | $\frac{\hat{\sigma}}{\sqrt{N}}$ |
| Probability distribution                    | standard normal           | $t$                             |
| DF                                          | none                      | $N-1$                           |
| Tails                                       | One or two                | One or two                      |
| Critical value $(\alpha = .05, two-tailed)$ | 1.96                      | Depends on DF                   |

------------------------------------------------------------------------

### **Assumptions of the one-sample *t*-test**

**Normality.** We assume the sampling distribution of the mean is normally distributed. Under what two conditions can we be assured that this is true?

**Independence.** Observations in the dataset are not associated with one another. Put another way, collecting a score from Participant A doesn't tell me anything about what Participant B will say. How can we be safe in this assumption?

------------------------------------------------------------------------

### A brief example - REVIEW

Using the same Census at School data, we find that New York students who participated in a memory game ( $N = 224$ ) completed the game in an average time of 44.2 seconds ( $s = 15.3$ ). We know that the average US student completed the game in 45.04 seconds. How do our students compare? <br>

<br>

**Hypotheses**

$H_0: \mu = 45.05$

$H_1: \mu \neq 45.05$

------------------------------------------------------------------------

::: columns
::: {.column width="50%"}
$$\mu = 45.05$$

$$N = 227$$

$$ \bar{X} = 44.2 $$

$$ s = 15.3 $$

$$
\sigma = Unknown
$$
:::

::: {.column width="50%"}
```{r}
t.test(x = school$Score_in_memory_game, 
       mu = 45.05,
       alternative = "two.sided")
```
:::
:::

------------------------------------------------------------------------

```{r}
lsr::oneSampleTTest(x = school$Score_in_memory_game,
                    mu = 45.05, one.sided = FALSE)
```

------------------------------------------------------------------------

## Writing Up a t-test

> "A one-sample t-test was conducted to determine if the mean \[variable name\] differed from a hypothesized population mean of \[population mean\]. The sample mean was M = \[sample mean\], which was significantly \[greater than/less than/different from\] the hypothesized population mean, t(df) = \[t-value\], p = \[p-value\]."

A one-sample t-test was conducted to determine if the mean score in a memory game for NY students differed from the US population mean. The sample mean was $M = 44.164$ (SD = 15.32, CI = \[42.15, 46.18\]), which was not significantly different from the population mean, $t(223) = -0.87$, $p = 0.388$.

------------------------------------------------------------------------

Single sample t-tests are not used super often in practice

You will mainly see them when interpreting effect sizes of coefficients in your model

```{r}
#Load sleep data: https://vincentarelbundock.github.io/Rdatasets/datasets.html
sleep <- read_csv("https://raw.githubusercontent.com/dharaden/dharaden.github.io/main/data/sleepstudy.csv")
model = lm(Reaction ~ Days, data = sleep)
summary(model)
```

------------------------------------------------------------------------

## Types of t-Tests

Single Samples t-test

Independent Samples t-test

Paired Samples t-test

------------------------------------------------------------------------

## Types of t-Tests

~~Single Samples t-test~~

Independent Samples t-test

::: columns
::: {.column width="50%"}
-   Random Sampling

-   Independent observations
:::

::: {.column width="50%"}
-   Approximately normal distributions

-   Homogeneity of variances
:::
:::

Paired Samples t-test

-   Approximately normal distributions

-   Homogeneity of variances

------------------------------------------------------------------------

## Dataset

Moving forward for today, we will use this dataset

::: nonincremental
-   100 students from New York

-   100 students from New Mexico
:::

```{r}

state_school <- import("https://raw.githubusercontent.com/dharaden/dharaden.github.io/main/data/NM-NY_CAS.csv")

```

## Normality Assumption

1.  **Check for Normality**: Visualizing data (histograms), Q-Q plots, and statistical tests (Shapiro-Wilk, Anderson-Darling) to assess normality.

2.  **Remedies for Violations**: data transformation or non-parametric alternatives when data is not normally distributed.

------------------------------------------------------------------------

### Normality Assumption - Visualizing

Visualizing Data

```{r}
hist(as.numeric(state_school$Sleep_Hours_Schoolnight))

```

------------------------------------------------------------------------

Let's make it pretty

```{r}
state_school %>% 
  ggplot(aes(Sleep_Hours_Schoolnight,
             fill = Region)) +
  geom_bar(position = "dodge")

```

------------------------------------------------------------------------

### Normality Assumption --- Q-Q Plot

A Q-Q plot is a graphical method for assessing whether a dataset follows a normal distribution. It compares the quantiles of your data to the quantiles of a theoretical normal distribution. If your data follows a normal distribution, the points in the Q-Q plot should form a straight line.

```{r}
qqPlot(state_school$Sleep_Hours_Schoolnight)
```

------------------------------------------------------------------------

### Normality Assumption --- Shapiro-Wilk Test

Examines the Null Hypothesis that the data are normally distributed

```{r}
shapiro.test(state_school$Sleep_Hours_Schoolnight)
```

------------------------------------------------------------------------

### Normality Assumption

-   Strict adherence to normality assumptions is not always necessary,

    -   Larger samples bring in Central Limit Theorem

-   However, assessing normality is still a valuable step in understanding distributions and potential impacts on your analyses

------------------------------------------------------------------------

### Failure of Normality Assumptions

What can we do if our data violate these normality assumptions?

-   Logarithmic Transformations

-   Square Root Transformations

-   Non-parametric tests

    -   **Mann-Whitney U Test (Wilcoxon Rank-Sum Test):** Used for comparing two independent groups

    <!-- -->

    -   **Wilcoxon Signed-Rank Test:** Used for comparing two paired or matched groups

------------------------------------------------------------------------

## Homogeneity of Variance

1.  **Check for Equality of Variances**: Levene's test to assess if variances are equal between groups

2.  **Remedies for Violations**: Welch's t-test for unequal variances.

------------------------------------------------------------------------

### Levene's Test

This test is used to examine if the variance is equal across groups. The Null Hypothesis is that the variances are equal

```{r}
# Perform Levene's test for equality of variances
leveneTest(Sleep_Hours_Schoolnight ~ Region, 
           data = state_school)
```

::: {style="font-size: 30px"}
Like other tests of significance, Levene's test gets more powerful as sample size increases. So unless your two variances are exactly equal to each other (and if they are, you don't need to do a test), your test will be "significant" with a large enough sample. Part of the analysis has to be an eyeball test -- is this "significant" because they are truly different, or because I have many subjects.
:::

# Independent Samples t-test

[Chapter 13.3 in Learning Stats with R](https://learningstatisticswithr.com/book/ttest.html#studentttest)

Two different types: Student's & Welch's

-   Start with Student's t-test which assumes equal variances between the groups

$$
t = \frac{\bar{X_1} - \bar{X_2}}{SE(\bar{X_1} - \bar{X_2})}
$$

------------------------------------------------------------------------

## Student's t-test

$$
H_0 : \mu_1 = \mu_2  \ \  H_1 : \mu_1 \neq \mu_2
$$

![](/images/student_H.png){fig-align="center"}

------------------------------------------------------------------------

### Student's t-test: Calculate SE

Are able to use a pooled variance estimate

Both variances/standard deviations are assumed to be equal

Therefore:

$$
SE(\bar{X_1} - \bar{X_2}) = \hat{\sigma} \sqrt{\frac{1}{N_1} + \frac{1}{N_2}}
$$

We are calculating the **Standard Error of the Difference between means**

Degrees of Freedom: Total N - 2

------------------------------------------------------------------------

### Student's t-test: In R

Using the `independentSamplesTest()` within the `lsr` library (make sure it is installed) we can run the test *very* easily

Formula is outcome \~ group

```{r}
independentSamplesTTest(
  formula = Sleep_Hours_Schoolnight ~ Region, 
  data = state_school, 
  var.equal = TRUE #default is FALSE
)

```

------------------------------------------------------------------------

### Student's t-test: In R (classic)

Let's then try it out using the traditional `t.test()` function. It doesn't look as nice, but you will likely encounter it when searching for help

```{r}
t.test(
  formula = Sleep_Hours_Schoolnight ~ Region, 
  data = state_school, 
  var.equal = TRUE
)
```

------------------------------------------------------------------------

### Student's t-test: Write-up

The mean amount of sleep in New Mexico for youth was 6.989 (SD = 1.379), while the mean in New York was 6.994 (SD = 1.512). A Student's independent samples t-test showed that there was not a significant mean difference (*t*(180)=-0.024, *p*=.981, $CI_{95}$=\[-0.43, 0.42\], *d*=.004). This suggests that there is no difference between youth in NM and NY on amount of sleep on school nights.

------------------------------------------------------------------------

```{r}
#https://indrajeetpatil.github.io/ggstatsplot/
ggstatsplot::ggbetweenstats(
  data  = state_school,
  x     = Region,
  y     = Sleep_Hours_Schoolnight,
  title = "Distribution of hours of sleep across Region"
)
```

------------------------------------------------------------------------

## Welch's t-test

$$
H_0 : \mu_1 = \mu_2  \ \  H_1 : \mu_1 \neq \mu_2
$$

![](/images/welch_H.png){fig-align="center"}

------------------------------------------------------------------------

### Welch's t-test: Calculate SE

Since the variances are not equal, we have to estimate the SE differently

$$ SE(\bar{X_1} - \bar{X_2}) = \sqrt{\frac{\hat{\sigma_1^2}}{N_1} + \frac{\hat{\sigma_2^2}}{N_2}} $$

Degrees of Freedom is also very different:

![](/images/welch_df.png){fig-align="center" width="380"}

------------------------------------------------------------------------

### Welch's t-test: In R

We use the same function as before, but specify that the variances are not equal

```{r}
independentSamplesTTest(
  formula = Sleep_Hours_Schoolnight ~ Region, 
  data = state_school, 
  var.equal = FALSE
)
```

------------------------------------------------------------------------

### Welch's t-test: In R (classic)

Let's then try it out using the traditional `t.test()` function...turns out it is pretty straightforward

```{r}
t.test(
  formula = Sleep_Hours_Schoolnight ~ Region, 
  data = state_school, 
  var.equal = FALSE
)
```

------------------------------------------------------------------------

## Cool Visualizations

The library [ggstatsplot](https://indrajeetpatil.github.io/ggstatsplot/) has some wonderful visualizations of various tests

```{r}
#| code-fold: true
ggstatsplot::ggbetweenstats(
  data  = state_school,
  x     = Region,
  y     = Sleep_Hours_Schoolnight,
  title = "Distribution of hours of sleep across Region"
)
```

# In-Class Lab

Take a look at the data in `state_school` and identify another Independent Samples t-test that you can perform

::: nonincremental
-   Be sure to select (1) a grouping variable and (2) a continuous variable to look at differences between the groups
:::

Conduct the following:

::: nonincremental
1.  Check for Normality of the variable
2.  Check for Homogeneity of Variances
3.  Perform the correct t-test
:::

Report the results

**Knit the document**

# Paired Samples t-Test in R

------------------------------------------------------------------------

# Common Mistakes and Pitfalls

-   **Misinterpreting p-Values**

-   **Violations of Assumptions**

-   **Sample Size Considerations**

-   **Multiple Testing Issues**

------------------------------------------------------------------------

## Next time...
